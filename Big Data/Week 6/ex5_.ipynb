{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5: Spark APIs [100 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Accumulators [10 points]\n",
    "[10 points].The title of this Q&A is wrong. It’s really about global variables (aka accumulators). The question shows code that is incorrect. \n",
    "val data = Array(1,2,3,4,5)\n",
    "var counter = 0\n",
    "var rdd = sc.parallelize(data)\n",
    "\n",
    "// Wrong: Don't do this!!\n",
    "rdd.foreach(x => counter += x)\n",
    "\n",
    "println(\"Counter value: \" + counter)\n",
    "Write a corrected version of the code and demonstrate its intended operation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.apache.spark.util.LongAccumulator\n",
    "\n",
    "object SparkAccumulatorExample {\n",
    "  def main(args: Array[String]): Unit = {\n",
    "    val conf = new SparkConf().setAppName(\"AccumulatorExample\").setMaster(\"local[*]\")\n",
    "    val sc = new SparkContext(conf)\n",
    "\n",
    "    val data = Array(1, 2, 3, 4, 5)\n",
    "    val rdd = sc.parallelize(data)\n",
    "\n",
    "    // Correct approach using Accumulator\n",
    "    val counter: LongAccumulator = sc.longAccumulator(\"Counter Accumulator\")\n",
    "\n",
    "    rdd.foreach(x => counter.add(x))\n",
    "\n",
    "    println(\"Counter value: \" + counter.value)\n",
    "    \n",
    "    sc.stop()\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output: 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Airline Traffic [45 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. [15 points] Describe in words and in code (where applicable) the steps you took to set up the environment for gathering the statistical data in the below questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. [6 points] Which US Airline Has the Least Delays? Report by full names, (e.g., Delta Airlines, not DL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rows with valid Carrier codes: 1225398\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "    # Step 2: Define documented column names\n",
    "documented_columns = [\n",
    "        'Carrier', 'FlightNumber',\n",
    "        'Undocumented_1', 'Undocumented_2',  # Placeholder for extra columns\n",
    "        'OperatingCarrier', 'OperatingFlightNumber',\n",
    "        'DepartureAirport', 'ArrivalAirport', 'FlightDate', 'DayOfWeek',\n",
    "        'ScheduledDepartureTime_OAG', 'ScheduledDepartureTime_CRS',\n",
    "        'ActualDepartureTime', 'ScheduledArrivalTime_OAG', 'ScheduledArrivalTime_CRS',\n",
    "        'ActualArrivalTime', 'Diff_ScheduledDepartureTimes', 'Diff_ScheduledArrivalTimes',\n",
    "        'ScheduledElapsedMinutes', 'DepartureDelayMinutes', 'ArrivalDelayMinutes',\n",
    "        'Diff_ElapsedMinutes', 'WheelsOffTime', 'WheelsOnTime', 'AircraftTailNumber',\n",
    "        'CancellationCode', 'MinutesLate_DelayCodeE', 'MinutesLate_DelayCodeF',\n",
    "        'MinutesLate_DelayCodeG', 'MinutesLate_DelayCodeH', 'MinutesLate_DelayCodeI'\n",
    "    ]\n",
    "\n",
    "    # Step 3: Determine the total number of columns in the data\n",
    "with open('ontime.td.202406.asc', 'r') as f:\n",
    "        first_line = f.readline()\n",
    "        total_columns = len(first_line.split('|'))\n",
    "\n",
    "    # Step 4: Generate column names\n",
    "if total_columns > len(documented_columns):\n",
    "        extra_columns = total_columns - len(documented_columns)\n",
    "        column_names = documented_columns + [f'ExtraColumn_{i}' for i in range(1, extra_columns + 1)]\n",
    "else:\n",
    "        column_names = documented_columns[:total_columns]\n",
    "\n",
    "    # Step 5: Specify data types for all columns as strings\n",
    "dtype_spec = {col: str for col in column_names}\n",
    "\n",
    "    # Step 6: Load the datasets for June 2024 and July 2024\n",
    "june_data = pd.read_csv('ontime.td.202406.asc', delimiter='|', header=None, names=column_names, dtype=dtype_spec, low_memory=False)\n",
    "july_data = pd.read_csv('ontime.td.202407.asc', delimiter='|', header=None, names=column_names, dtype=dtype_spec, low_memory=False)\n",
    "\n",
    "combined_data = pd.concat([june_data, july_data], ignore_index=True)\n",
    "\n",
    "    # Step 7: Remove the 4 undocumented columns and extra ones\n",
    "columns_to_keep = [\n",
    "        'Carrier', 'FlightNumber', 'OperatingCarrier', 'OperatingFlightNumber',\n",
    "        'DepartureAirport', 'ArrivalAirport', 'FlightDate', 'DayOfWeek',\n",
    "        'ScheduledDepartureTime_OAG', 'ScheduledDepartureTime_CRS',\n",
    "        'ActualDepartureTime', 'ScheduledArrivalTime_OAG', 'ScheduledArrivalTime_CRS',\n",
    "        'ActualArrivalTime', 'Diff_ScheduledDepartureTimes', 'Diff_ScheduledArrivalTimes',\n",
    "        'ScheduledElapsedMinutes', 'DepartureDelayMinutes', 'ArrivalDelayMinutes',\n",
    "        'Diff_ElapsedMinutes', 'WheelsOffTime', 'WheelsOnTime', 'AircraftTailNumber',\n",
    "        'CancellationCode', 'MinutesLate_DelayCodeE', 'MinutesLate_DelayCodeF',\n",
    "        'MinutesLate_DelayCodeG', 'MinutesLate_DelayCodeH', 'MinutesLate_DelayCodeI'\n",
    "    ]\n",
    "combined_data = combined_data[columns_to_keep]\n",
    "\n",
    "    # Step 8: Clean Carrier column\n",
    "combined_data['Carrier'] = combined_data['Carrier'].str.upper().str.strip()\n",
    "\n",
    "    # Step 9: Filter valid Carrier codes (two uppercase letters)\n",
    "valid_carrier_pattern = re.compile(r'^[A-Z]{2}$')\n",
    "carrier_filter = combined_data['Carrier'].str.match(valid_carrier_pattern, na=False)\n",
    "print(f\"\\nRows with valid Carrier codes: {carrier_filter.sum()}\")\n",
    "combined_data = combined_data[carrier_filter]\n",
    "\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code shows how I parsed and cleaned the raw data to be able to answer the questions.\n",
    "I had to assign labels to the columns. I had to only account for 2 extra columns between B and C because doing\n",
    "4 like the instructions said messed up the Arrival and Departure Airport columns for the following questions.\n",
    "I decided to declare all the variables as strings and then convert to int as necessary. I used this method because\n",
    "I was getting a lot of improper data type errors and couldn't figure them out otherwise. This was likely due to the\n",
    "column labels being offset by two at first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carrier\n",
      "Southwest Airlines    125.858359\n",
      "American Airlines     129.479622\n",
      "Delta Airlines        131.965433\n",
      "Hawaiian Airlines     140.665946\n",
      "United Airlines       141.952446\n",
      "Name: DepartureDelayMinutes, dtype: float64\n",
      "\n",
      "The airline with the least delays is Southwest Airlines with an average delay of 125.86 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Step 10: Filter valid DepartureDelayMinutes\n",
    "    # Convert column to numeric, forcing errors to NaN\n",
    "combined_data['DepartureDelayMinutes'] = pd.to_numeric(combined_data['DepartureDelayMinutes'], errors='coerce')\n",
    "\n",
    "    # Drop rows where conversion resulted in NaN\n",
    "combined_data = combined_data.dropna(subset=['DepartureDelayMinutes'])\n",
    "\n",
    "    # Convert from float to int (since NaN values are removed)\n",
    "combined_data['DepartureDelayMinutes'] = combined_data['DepartureDelayMinutes'].astype(int)\n",
    "\n",
    "relevant_data = combined_data.dropna(subset=['DepartureDelayMinutes'])\n",
    "\n",
    "    # Step 12: Analyze the data\n",
    "if relevant_data.empty:\n",
    "        print(\"\\nThe cleaned dataset is empty. No valid rows found.\")\n",
    "else:\n",
    "        # Group by Carrier and calculate the mean delay (including all flights)\n",
    "    average_delays = relevant_data.groupby('Carrier')['DepartureDelayMinutes'].mean()\n",
    "\n",
    "# Sort by average delay\n",
    "    sorted_delays = average_delays.sort_values()\n",
    "\n",
    "    airline_mapping = {\n",
    "                'DL': 'Delta Airlines',\n",
    "                'AA': 'American Airlines',\n",
    "                'UA': 'United Airlines',\n",
    "                'WN': 'Southwest Airlines',\n",
    "                'AS': 'Alaska Airlines',\n",
    "                'B6': 'JetBlue Airways',\n",
    "                'NK': 'Spirit Airlines',\n",
    "                'F9': 'Frontier Airlines',\n",
    "                'HA': 'Hawaiian Airlines',\n",
    "                'G4': 'Allegiant Air',\n",
    "                'YX': 'Midwest Airlines',\n",
    "                'OO': 'SkyWest Airlines',\n",
    "                'MQ': 'Envoy Air',\n",
    "                'OH': 'PSA Airlines',\n",
    "                'YV': 'Mesa Airlines',\n",
    "                'QX': 'Horizon Air',\n",
    "                'EV': 'ExpressJet Airlines'\n",
    "            }\n",
    "\n",
    "            # Convert airline codes to full names\n",
    "    sorted_delays.index = sorted_delays.index.map(airline_mapping)\n",
    "\n",
    "            # Report the airline with the least delays\n",
    "    least_delay_airline = sorted_delays.idxmin()\n",
    "    least_delay_value = sorted_delays.min()\n",
    "    print(sorted_delays.head(5))\n",
    "\n",
    "    print(f\"\\nThe airline with the least delays is {least_delay_airline} with an average delay of {least_delay_value:.2f} minutes.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code is how I calculated the airlines with the least average delays. \n",
    "I converted the departure delays to int to be able to calculate this. \n",
    "The top 5 least delayed airlines are shown above. SouthWest Airlines ended\n",
    "up being the airline with the least average delays. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. [6 points] What Departure Time of Day Is Best to Avoid Flight Delays, segmented into 5 time blocks [night (10 pm - 6 am), morning (6 am to 10 am), mid-day (10 am to 2 pm), afternoon (2 pm - 6 pm), evening (6 pm - 10 pm)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeBlock\n",
      "Night        132.867186\n",
      "Mid-Day      135.265775\n",
      "Afternoon    135.455740\n",
      "Evening      137.914275\n",
      "Morning      175.985673\n",
      "Name: DepartureDelayMinutes, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def categorize_time(hour):\n",
    "    if 22 <= hour or hour < 6:\n",
    "        return 'Night'\n",
    "    elif 6 <= hour < 10:\n",
    "        return 'Morning'\n",
    "    elif 10 <= hour < 14:\n",
    "        return 'Mid-Day'\n",
    "    elif 14 <= hour < 18:\n",
    "        return 'Afternoon'\n",
    "    else:\n",
    "        return 'Evening'\n",
    "\n",
    "## Find the best time of day to avoid delays\n",
    "combined_data['DepartureHour'] = combined_data['ActualDepartureTime'].str[:2]\n",
    "combined_data['DepartureHour'] = pd.to_numeric(combined_data['DepartureHour'], errors='coerce')\n",
    "combined_data['TimeBlock'] = combined_data['DepartureHour'].apply(categorize_time)\n",
    "timeblock_delays = combined_data.groupby('TimeBlock')['DepartureDelayMinutes'].mean().sort_values()\n",
    "print(timeblock_delays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for calculating the time block with the least delays is above. Night was the best \n",
    "departure in terms of avoiding delays. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. [5 points] Which Airports Have The Most Flight Delays? Report by full name, (e.g., “Newark Liberty International,” not “EWR,” when the airport code EWR is provided)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pago Pago International                      334.461538\n",
      "Aeropuerto Internacional Rafael Hernández    254.245902\n",
      "San Juan - Luis Muñoz Marín International    247.113487\n",
      "Guam International                           242.926230\n",
      "Cyril E. King Airport                        242.833724\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "## Find the airports with the most delays\n",
    "airport_mapping = {\n",
    "    'ATL': 'Atlanta - Hartsfield Jackson',\n",
    "    'BWI': \"Baltimore/Wash. Int'l Thurgood Marshall\",\n",
    "    'BOS': 'Boston - Logan International',\n",
    "    'CLT': 'Charlotte - Douglas',\n",
    "    'MDW': 'Chicago - Midway',\n",
    "    'ORD': \"Chicago - O'Hare\",\n",
    "    'CVG': 'Cincinnati Greater Cincinnati',\n",
    "    'DFW': 'Dallas-Fort Worth International',\n",
    "    'DEN': 'Denver - International',\n",
    "    'DTW': 'Detroit - Metro Wayne County',\n",
    "    'FLL': 'Fort Lauderdale Hollywood International',\n",
    "    'IAH': 'Houston - George Bush International',\n",
    "    'LAS': 'Las Vegas - McCarran International',\n",
    "    'LAX': 'Los Angeles International',\n",
    "    'MIA': 'Miami International',\n",
    "    'MSP': 'Minneapolis-St. Paul International',\n",
    "    'EWR': 'Newark Liberty International',\n",
    "    'JFK': 'New York - JFK International',\n",
    "    'LGA': 'New York - LaGuardia',\n",
    "    'MCO': 'Orlando International',\n",
    "    'OAK': 'Oakland International',\n",
    "    'PHL': 'Philadelphia International',\n",
    "    'PHX': 'Phoenix - Sky Harbor International',\n",
    "    'PDX': 'Portland International',\n",
    "    'SLC': 'Salt Lake City International',\n",
    "    'STL': 'St. Louis Lambert International',\n",
    "    'SAN': 'San Diego Intl. Lindbergh Field',\n",
    "    'SFO': 'San Francisco International',\n",
    "    'SEA': 'Seattle-Tacoma International',\n",
    "    'TPA': 'Tampa International',\n",
    "    'DCA': 'Washington - Reagan National',\n",
    "    'IAD': 'Washington - Dulles International',\n",
    "    'PPG': 'Pago Pago International',\n",
    "    'GUM': 'Guam International',\n",
    "    'HNL': 'Honolulu International',\n",
    "    'OGG': 'Kahului Airport',\n",
    "    'KOA': 'Kona International',\n",
    "    'LIH': 'Lihue Airport',\n",
    "    'ITO': 'Hilo International',\n",
    "    'BQN': 'Aeropuerto Internacional Rafael Hernández',\n",
    "    'SJU': 'San Juan - Luis Muñoz Marín International',\n",
    "    'STT': 'Cyril E. King Airport'\n",
    "}\n",
    "\n",
    "    # Find the airports with the most delays\n",
    "combined_data['ArrivalDelayMinutes'] = pd.to_numeric(combined_data['ArrivalDelayMinutes'], errors='coerce')\n",
    "combined_data['DepartureDelayMinutes'] = pd.to_numeric(combined_data['DepartureDelayMinutes'], errors='coerce')\n",
    "\n",
    "arrival_delay = combined_data.groupby('ArrivalAirport')['ArrivalDelayMinutes'].mean().sort_values(ascending=False)\n",
    "departure_delay = combined_data.groupby('DepartureAirport')['DepartureDelayMinutes'].mean().sort_values(ascending=False)\n",
    "total_delay = arrival_delay + departure_delay\n",
    "\n",
    "total_delay.index = total_delay.index.to_series().replace(airport_mapping)\n",
    "total_delay = total_delay.sort_values(ascending=False)\n",
    "print(total_delay.head(5))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above shows the airports with the most delays. Pago Pago International in Puerto Rico was the airport with the most delays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [5 points] What Are the Top 5 Busiest Airports in the US. Report by full name, (e.g., “Newark Liberty International,” not “EWR”)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Busiest Airports in the US (by total arrivals and departures):\n",
      "Atlanta - Hartsfield Jackson       114182\n",
      "Dallas-Fort Worth International    110252\n",
      "Chicago - O'Hare                   109931\n",
      "Denver - International             101826\n",
      "Charlotte - Douglas                 86198\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count total arrivals and departures\n",
    "arrivals_count = combined_data['ArrivalAirport'].value_counts()\n",
    "departures_count = combined_data['DepartureAirport'].value_counts()\n",
    "\n",
    "# Sum arrivals and departures for each airport\n",
    "total_flights = arrivals_count.add(departures_count, fill_value=0)\n",
    "\n",
    "# Map airport codes to full names\n",
    "total_flights.index = total_flights.index.to_series().replace(airport_mapping)\n",
    "\n",
    "# Sort in descending order and get the top 5\n",
    "top_5_busiest_airports = total_flights.sort_values(ascending=False).head(5)\n",
    "\n",
    "# Display result\n",
    "print(\"Top 5 Busiest Airports in the US (by total arrivals and departures):\")\n",
    "print(top_5_busiest_airports)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above output shows the busiest airports in the US by total arrivals and departures. Atlanta was the busiest overall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ShortStoryJam [45 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. [3 points] To seed the effort, the text of about 22 short stories by Edgar Allan Poe, he of the “quoth the raven” fame, are available in my github repository. Clean the text and remove stopwords, as specified in a previous assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ways god nature providence ways models frame commensurate vastness profundity unsearchableness works depth greater democritus joseph glanville reached summit loftiest crag minutes man exhausted speak long ago length guided route youngest sons years happened event happened mortal man man survived hours deadly terror endured broken body soul suppose man single day change hairs jetty black white weaken limbs unstring nerves tremble exertion frightened shadow scarcely cliff giddy cliff edge carelessly thrown rest weightier portion body hung falling tenure elbow extreme slippery edge cliff arose sheer unobstructed precipice black shining rock sixteen feet crags beneath tempted half dozen yards brink truth deeply excited perilous position companion fell length ground clung shrubs dared glance upward sky struggled vain divest idea foundations mountain danger fury winds long reason sufficient courage sit distance fancies guide brought view scene event mentioned story spot eye continued particularizing manner distinguished close norwegian coast eighth degree latitude great province nordland dreary district lofoden mountain sit helseggen cloudy raise higher hold grass feel giddy belt vapor beneath sea looked dizzily beheld wide expanse ocean waters wore inky hue bring mind nubian geographer account mare tenebrarum panorama deplorably desolate human imagination conceive left eye reach lay outstretched ramparts lines horridly black beetling cliff character gloom forcibly illustrated surf reared high white ghastly crest howling shrieking forever opposite promontory apex distance miles sea visible small bleak island properly position discernible wilderness surge enveloped miles nearer land arose smaller size hideously craggy barren encompassed intervals cluster dark rocks appearance ocean space distant island shore unusual time strong gale blowing landward brig remote offing lay double reefed trysail constantly plunged hull sight regular swell short quick angry cross dashing water direction teeth wind foam vicinity rocks island distance resumed man called norwegians vurrgh midway moskoe mile northward ambaaren yonder islesen hotholm keildhelm suarven buckholm farther moskoe vurrgh otterholm flimen sandflesen stockholm true names places thought understand hear change water minutes helseggen ascended interior lofoden caught glimpse sea burst summit man spoke aware loud gradually increasing sound moaning vast herd buffaloes american prairie moment perceived seamen term chopping character ocean beneath rapidly changing current set eastward gazed current acquired monstrous velocity moment speed headlong impetuosity minutes sea vurrgh lashed ungovernable fury moskoe coast main uproar held sway vast bed waters seamed scarred conflicting channels burst suddenly phrensied convulsion heaving boiling hissing gyrating gigantic innumerable vortices whirling plunging eastward rapidity water assumes precipitous descents minutes scene radical alteration general surface grew smooth whirlpools disappeared prodigious streaks foam apparent streaks length spreading great distance entering combination gyratory motion subsided vortices form germ vast suddenly suddenly assumed distinct definite existence circle mile diameter edge whirl represented broad belt gleaming spray particle slipped mouth terrific funnel interior eye fathom smooth shining jet black wall water inclined horizon angle degrees speeding dizzily round round swaying sweltering motion sending winds appalling voice half shriek half roar mighty cataract niagara lifts agony heaven mountain trembled base rock rocked threw face clung scant herbage excess nervous agitation length man great whirlpool maelstr termed norwegians moskoe str island moskoe midway ordinary accounts vortex prepared jonas ramus circumstantial impart faintest conception magnificence horror scene wild bewildering sense confounds beholder point view writer question surveyed time summit helseggen storm passages description quoted details exceedingly feeble conveying impression spectacle lofoden moskoe depth water thirty fathoms ver vurrgh depth decreases afford convenient passage vessel risk splitting rocks calmest weather flood stream runs country lofoden moskoe boisterous rapidity roar impetuous ebb sea scarce equalled loudest dreadful cataracts noise heard leagues vortices pits extent depth ship attraction inevitably absorbed carried beat pieces rocks water relaxes fragments thrown intervals tranquility turn ebb flood calm weather quarter hour violence gradually returning stream boisterous fury heightened storm dangerous norway mile boats yachts ships carried guarding reach likewise frequently whales stream overpowered violence impossible howlings bellowings fruitless struggles disengage bear attempting swim lofoden moskoe caught stream borne roared terribly heard shore large stocks firs pine trees absorbed current rise broken torn degree bristles grew plainly consist craggy rocks whirled fro stream regulated flux reflux sea constantly high low water hours year early morning sexagesima sunday raged noise impetuosity stones houses coast fell ground regard depth water ascertained vicinity vortex fathoms reference portions channel close shore moskoe lofoden depth centre moskoe str immeasurably greater proof fact sidelong glance abyss whirl highest crag helseggen pinnacle howling phlegethon smiling simplicity honest jonas ramus records matter difficult belief anecdotes whales bears appeared fact evident thing largest ship existence coming influence deadly attraction resist feather hurricane disappear bodily attempts account phenomenon remember plausible perusal wore unsatisfactory aspect idea generally received smaller vortices ferroe islands collision waves rising falling flux reflux ridge rocks shelves confines water precipitates cataract higher flood rises deeper fall natural result whirlpool vortex prodigious suction lesser experiments encyclop dia britannica kircher imagine centre channel maelstr abyss penetrating globe issuing remote gulf bothnia decidedly named instance opinion idle gazed imagination assented mentioning guide surprised hear view universally entertained subject norwegians notion confessed inability comprehend agreed conclusive paper altogether unintelligible absurd amid thunder abyss good whirl man creep round crag lee deaden roar water story convince moskoe str desired proceeded brothers owned schooner rigged smack seventy tons burthen habit fishing islands moskoe vurrgh violent eddies sea good fishing proper opportunities courage attempt lofoden coastmen regular business islands usual grounds great lower southward fish hours risk places preferred choice spots rocks yield finest variety greater abundance single day timid craft scrape week fact matter desperate speculation risk life standing labor courage answering capital smack cove miles higher coast practice fine weather advantage minutes slack push main channel moskoe str pool drop anchorage otterholm sandflesen eddies violent remain time slack water weighed set expedition steady wind coming felt fail return seldom mis calculation point years forced stay night anchor account dead calm rare thing remain grounds week starving death gale blew shortly arrival channel boisterous thought occasion driven sea spite whirlpools threw round round violently length fouled anchor dragged drifted innumerable cross currents day morrow drove lee flimen good luck brought twentieth difficulties encountered grounds bad spot good weather shift gauntlet moskoe str accident times heart mouth happened minute slack wind strong thought starting current rendered smack unmanageable eldest brother son eighteen years stout boys great assistance times sweeps afterward fishing risk heart young danger horrible danger truth days years occurred tenth day july day people forget blew terrible hurricane heavens morning late afternoon gentle steady breeze south west sun shone brightly oldest seaman foreseen follow brothers crossed islands clock loaded smack fine fish remarked plenty day watch weighed started worst str slack water knew set fresh wind starboard quarter time spanked great rate dreaming danger slightest reason apprehend aback breeze helseggen unusual happened began feel uneasy knowing boat wind headway eddies point proposing return anchorage astern horizon covered singular copper colored cloud rose amazing velocity breeze headed fell dead becalmed drifting direction state things long time minute storm sky overcast driving spray suddenly dark smack hurricane blew folly attempt describing oldest seaman norway experienced thing sails cleverly puff masts board sawed mainmast youngest brother lashed safety boat lightest feather thing sat water complete flush deck small hatch bow hatch custom batten cross str precaution chopping seas circumstance foundered lay buried moments elder brother escaped destruction opportunity ascertaining foresail threw flat deck feet narrow gunwale bow hands grasping ring bolt foot fore mast mere instinct prompted undoubtedly thing flurried moments completely deluged time held breath clung bolt stand longer raised knees keeping hold hands head clear presently boat shake dog coming water rid measure seas stupor collect senses felt grasp arm elder brother heart leaped joy overboard moment joy turned horror mouth close ear screamed word moskoe str feelings moment shook head foot violent fit ague knew meant word knew wished understand wind drove bound whirl str save perceive crossing str channel long whirl calmest weather wait watch carefully slack driving pool hurricane thought slack hope moment cursed great fool dream hope knew doomed times gun ship time fury tempest spent feel scudded events seas wind lay flat frothing absolute mountains singular change heavens direction black pitch overhead burst circular rift clear sky clear deep bright blue blazed moon lustre knew wear lit thing greatest distinctness god scene light attempts speak brother manner understand din increased hear single word screamed voice ear presently shook head pale death held fingers listen meant hideous thought flashed dragged watch fob glanced face moonlight burst tears flung ocean clock time slack whirl str fury boat built properly trimmed deep laden waves strong gale large slip beneath appears strange landsman called riding sea phrase ridden swells cleverly presently gigantic sea happened counter bore rose sky believed wave rise high sweep slide plunge feel sick dizzy falling lofty mountain dream thrown quick glance glance sufficient exact position instant moskoe str whirlpool quarter mile dead ahead day moskoe str whirl race expect recognised place involuntarily closed eyes horror lids clenched spasm minutes afterward suddenly felt waves subside enveloped foam boat sharp half turn larboard shot direction thunderbolt moment roaring noise water completely drowned kind shrill shriek sound imagine waste pipes steam vessels letting steam belt surf surrounds whirl thought moment plunge abyss indistinctly account amazing velocity wore borne boat sink water skim air bubble surface surge starboard whirl larboard arose ocean left stood huge writhing wall horizon strange jaws gulf felt composed approaching mind hope rid great deal terror unmanned suppose despair strung nerves boasting truth began reflect magnificent thing die manner foolish paltry consideration individual life view wonderful manifestation god power blushed shame idea crossed mind possessed keenest curiosity whirl positively felt explore depths sacrifice principal grief companions shore mysteries doubt singular fancies occupy man mind extremity thought revolutions boat pool rendered light headed circumstance tended restore possession cessation wind reach situation belt surf considerably lower general bed ocean towered high black mountainous ridge sea heavy gale form idea confusion mind occasioned wind spray blind deafen strangle power action reflection great measure rid annoyances death condemned felons prison allowed petty indulgences forbidden doom uncertain circuit belt impossible careered round round hour flying floating gradually middle surge nearer nearer horrible edge time ring bolt brother stern holding small water cask securely lashed coop counter thing deck swept overboard gale approached brink pit hold ring agony terror endeavored force hands large afford secure grasp felt deeper grief attempt knew madman raving maniac sheer fright care contest point knew difference held bolt astern cask great difficulty smack flew round steadily keel swaying fro immense sweeps swelters whirl scarcely secured position wild lurch starboard rushed headlong abyss muttered hurried prayer god thought felt sickening sweep descent instinctively tightened hold barrel closed eyes seconds dared open expected instant destruction wondered death struggles water moment moment elapsed lived sense falling ceased motion vessel belt foam exception lay courage looked scene forget sensations awe horror admiration gazed boat appeared hanging magic midway interior surface funnel vast circumference prodigious depth perfectly smooth sides mistaken ebony bewildering rapidity spun gleaming ghastly radiance shot rays moon circular rift amid clouds streamed flood golden glory black walls inmost recesses abyss confused observe accurately general burst terrific grandeur beheld recovered gaze fell instinctively downward direction unobstructed view manner smack hung inclined surface pool keel deck lay plane parallel water sloped angle degrees lying beam ends observing scarcely difficulty maintaining hold footing situation dead level suppose speed revolved rays moon search profound gulf distinctly account thick mist enveloped hung magnificent rainbow narrow tottering bridge mussulmen pathway time eternity mist spray doubt occasioned clashing great walls funnel met yell heavens mist dare attempt slide abyss belt foam carried great distance slope farther descent proportionate round round swept uniform movement dizzying swings jerks yards complete circuit whirl progress downward revolution slow perceptible wide waste liquid ebony borne perceived boat object embrace whirl visible fragments vessels large masses building timber trunks trees smaller articles pieces house furniture broken boxes barrels staves unnatural curiosity place original terrors appeared grow drew nearer nearer dreadful doom began watch strange numerous things floated company delirious sought amusement speculating relative velocities descents foam fir tree time thing takes awful plunge disappears disappointed wreck dutch merchant ship overtook length making guesses nature deceived fact fact invariable miscalculation set train reflection limbs tremble heart beat heavily terror dawn exciting hope hope arose partly memory partly observation called mind great variety buoyant matter strewed coast lofoden absorbed thrown moskoe str greater number articles shattered extraordinary chafed roughened appearance stuck splinters distinctly recollected disfigured account difference supposing roughened fragments completely absorbed entered whirl late period tide reason descended slowly entering reach turn flood ebb case conceived instance whirled level ocean undergoing fate drawn early absorbed rapidly observations general rule larger bodies rapid descent masses equal extent spherical shape superiority speed descent sphere masses equal size cylindrical shape cylinder absorbed slowly escape conversations subject school master district learned cylinder sphere explained forgotten explanation observed fact natural consequence forms floating fragments happened cylinder swimming vortex offered resistance suction drawn greater difficulty equally bulky body form startling circumstance great enforcing observations rendering anxious turn account revolution passed barrel yard mast vessel things level opened eyes wonders whirlpool high moved original station longer hesitated resolved lash securely water cask held cut loose counter throw water attracted brother attention signs pointed floating barrels power understand thought length comprehended design case shook head despairingly refused station ring bolt impossible reach emergency admitted delay bitter struggle resigned fate fastened cask lashings secured counter precipitated sea moment hesitation result precisely hoped tale escape possession mode escape effected anticipate farther bring story conclusion hour thereabout quitting smack descended vast distance beneath wild gyrations rapid succession bearing loved brother plunged headlong forever chaos foam barrel attached sunk farther half distance gulf spot leaped overboard great change place character whirlpool slope sides vast funnel momently steep gyrations whirl grew gradually violent degrees froth rainbow disappeared gulf slowly uprise sky clear winds moon setting radiantly west surface ocean view shores lofoden spot pool moskoe str hour slack sea heaved mountainous waves effects hurricane borne violently channel str minutes hurried coast grounds fishermen boat picked exhausted fatigue danger removed speechless memory horror drew board mates daily companions knew traveller spirit land hair raven black day white expression countenance changed told story scarcely expect faith merry fishermen lofoden\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import re\n",
    "import string\n",
    "import requests\n",
    "import sys\n",
    "\n",
    "def load_stopwords():\n",
    "    url = \"https://gist.githubusercontent.com/rg089/35e00abf8941d72d419224cfd5b5925d/raw/12d899b70156fd0041fa9778d657330b024b959c/stopwords.txt\"\n",
    "    stopwords_list = requests.get(url).content\n",
    "    return set(stopwords_list.decode().splitlines())\n",
    "\n",
    "stopwords = load_stopwords()\n",
    "\n",
    "def process_text(text, is_recursive=False):\n",
    "    if not is_recursive:\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'\\[.*?\\]', '', text)\n",
    "        text = re.sub(r'[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "        text = re.sub(r'\\d+', ' ', text)\n",
    "        text = ' '.join(remove_stopwords(text))\n",
    "        return process_text(text, is_recursive=True)\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    words_list = re.sub(r'[^a-zA-Z0-9]', \" \", words.lower()).split()\n",
    "    return [word for word in words_list if word not in stopwords]\n",
    "\n",
    "def process_file(filename):\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            processed_text = process_text(text)\n",
    "            return processed_text\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{filename}' not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "paragraph = process_file('A_DESCENT_INTO_THE_MAELSTROM.txt')\n",
    "print(paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used the above code from my assignment 5 to clean the 22 short stories. It was modified to accept filenames instead of just text.\n",
    "The output is from \"A Descent Into the Maelstorm\" as an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. [8 points] Use NLTK to decompose the first story (A_DESCENT_INTO…) into sentences & sentences into tokens. Here is the code for doing that, after you set the variable paragraph to hold the text of the story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Library/Frameworks/Python.fr\n",
      "[nltk_data]     amework/Versions/3.12/lib/python3.12/site-packages/...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to /Library\n",
      "[nltk_data]     /Frameworks/Python.framework/Versions/3.12/lib/python3\n",
      "[nltk_data]     .12/site-packages/...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt', download_dir='/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/')\n",
    "nltk.download('averaged_perceptron_tagger', download_dir='/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download()\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "sent_text = nltk.sent_tokenize(paragraph) # this gives us a list of sentences\n",
    "# now loop over each sentence and tokenize it separately\n",
    "all_tagged = [nltk.pos_tag(nltk.word_tokenize(sent)) for sent in sent_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was having issues with the given code, so I changed the download to \"nltk.download()\". I got this from a stack overflow post. The whole python code is pasted below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2918685338.py, line 47)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 47\u001b[0;36m\u001b[0m\n\u001b[0;31m    all_tagged = [nltk.pos_tag(nltk.word_tokenize(sent)) for sent in sent_text]                                                                                                                                   22,52         All\u001b[0m\n\u001b[0m                                                                                                                                                                                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import re\n",
    "import string\n",
    "import requests\n",
    "import sys\n",
    "import nltk\n",
    "\n",
    "def load_stopwords():\n",
    "    url = \"https://gist.githubusercontent.com/rg089/35e00abf8941d72d419224cfd5b5925d/raw/12d899b70156fd0041fa9778d657330b024b959c/stopwords.txt\"\n",
    "    stopwords_list = requests.get(url).content\n",
    "    return set(stopwords_list.decode().splitlines())\n",
    "\n",
    "stopwords = load_stopwords()\n",
    "\n",
    "def process_text(text, is_recursive=False):\n",
    "    if not is_recursive:\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'\\[.*?\\]', '', text)\n",
    "        text = re.sub(r'[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "        text = re.sub(r'\\d+', ' ', text)\n",
    "        text = ' '.join(remove_stopwords(text))\n",
    "        return process_text(text, is_recursive=True)\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    words_list = re.sub(r'[^a-zA-Z0-9]', \" \", words.lower()).split()\n",
    "    return [word for word in words_list if word not in stopwords]\n",
    "\n",
    "def process_file(filename):\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            processed_text = process_text(text)\n",
    "            return processed_text\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{filename}' not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "paragraph = process_file('A_DESCENT_INTO_THE_MAELSTROM')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "sent_text = nltk.sent_tokenize(paragraph) # this gives us a list of sentences\n",
    "# now loop over each sentence and tokenize it separately\n",
    "all_tagged = [nltk.pos_tag(nltk.word_tokenize(sent)) for sent in sent_text]                                                                                                                                   22,52         All"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. [11 points] Tag all remaining words in the story as parts of speech using the Penn POS Tags. This SO answer shows how to obtain the POS tag values. Create and print a dictionary with the Penn POS Tags as keys and a list of words as the values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THe code and explanation for this part is below question 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  4. [11 points] In this framework, each row will represent a story. The columns will be as follows:\n",
    "The text of the story,\n",
    "Two-letter prefixes of each tag, for example NN, VB, RB, JJ etc.and the words belonging to that tag in the story. \n",
    "Show your code and the tag columns, at least for the one story.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for this and part 3 combined is below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import re\n",
    "import string\n",
    "import requests\n",
    "import sys\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import os\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')  # Required for sentence tokenization\n",
    "nltk.download('averaged_perceptron_tagger')  # Required for POS tagging\n",
    "\n",
    "def load_stopwords():\n",
    "    url = \"https://gist.githubusercontent.com/rg089/35e00abf8941d72d419224cfd5b5925d/raw/12d899b70156fd0041fa9778d657330b024b959c/stopwords.txt\"\n",
    "    stopwords_list = requests.get(url).content\n",
    "    return set(stopwords_list.decode().splitlines())\n",
    "\n",
    "stopwords = load_stopwords()\n",
    "\n",
    "def process_text(text, is_recursive=False):\n",
    "    if not is_recursive:\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'\\[.*?\\]', '', text)\n",
    "        text = re.sub(r'[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "        text = re.sub(r'\\d+', ' ', text)\n",
    "        text = ' '.join(remove_stopwords(text))\n",
    "        return process_text(text, is_recursive=True)\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    words_list = re.sub(r'[^a-zA-Z0-9]', \" \", words.lower()).split()\n",
    "    return [word for word in words_list if word not in stopwords]\n",
    "\n",
    "def process_file(filename):\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "            text = file.read()\n",
    "            processed_text = process_text(text)\n",
    "            return processed_text\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{filename}' not found.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing the file: {e}\")\n",
    "        return None\n",
    "\n",
    "# Process the file\n",
    "filename = 'A_DESCENT_INTO_THE_MAELSTROM.txt'\n",
    "paragraph = process_file(filename)\n",
    "\n",
    "if paragraph is not None:\n",
    "\n",
    "    # Tokenize sentences and perform POS tagging\n",
    "    sent_text = sent_tokenize(paragraph)  # Sentence tokenization\n",
    "    all_tagged = [nltk.pos_tag(word_tokenize(sent)) for sent in sent_text]  # POS tagging\n",
    "\n",
    "    # Flatten the list of tagged words\n",
    "    flat_tagged_words = [word_tag for sentence in all_tagged for word_tag in sentence]\n",
    "\n",
    "    # Create a dictionary with POS tags as keys and lists of words as values\n",
    "    pos_dict = {}\n",
    "    for word, tag in flat_tagged_words:\n",
    "        if tag not in pos_dict:\n",
    "            pos_dict[tag] = []\n",
    "        pos_dict[tag].append(word)\n",
    "\n",
    "    # Print the dictionary\n",
    "    print(\"\\nPOS Tags Dictionary:\")\n",
    "    for tag, words in pos_dict.items():\n",
    "        print(f\"{tag}: {words[:10]}...\")  # Print only the first 10 words for brevity\n",
    "else:\n",
    "    print(\"No text to process due to file error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tag Output for the first 10 of each part of speech:\n",
    "\n",
    "    POS Tags Dictionary:\n",
    "    NNS: ['ways', 'ways', 'models', 'minutes', 'sons', 'years', 'hours', 'limbs', 'nerves', 'feet']...\n",
    "    VBP: ['god', 'frame', 'terror', 'brink', 'mountain', 'guide', 'sea', 'land', 'remote', 'swell']...\n",
    "    JJ: ['nature', 'commensurate', 'unsearchableness', 'speak', 'mortal', 'suppose', 'single', 'jetty', 'black', 'white']...\n",
    "    NN: ['providence', 'vastness', 'profundity', 'democritus', 'joseph', 'glanville', 'summit', 'crag', 'man', 'length']...\n",
    "    VBZ: ['works', 'hairs', 'assumes', 'passages', 'decreases', 'runs', 'leagues', 'whales', 'rocks', 'precipitates']...\n",
    "    RB: ['depth', 'long', 'ago', 'deadly', 'scarcely', 'carelessly', 'beneath', 'deeply', 'length', 'upward']...\n",
    "    JJR: ['greater', 'weightier', 'higher', 'smaller', 'yonder', 'greater', 'smaller', 'higher', 'deeper', 'lower']...\n",
    "    VBD: ['reached', 'exhausted', 'guided', 'happened', 'happened', 'survived', 'frightened', 'hung', 'arose', 'tempted']...\n",
    "    JJS: ['loftiest', 'youngest', 'divest', 'crest', 'faintest', 'loudest', 'highest', 'honest', 'largest', 'finest']...\n",
    "    VBN: ['endured', 'beheld', 'reared', 'called', 'ascended', 'set', 'acquired', 'lashed', 'phrensied', 'assumed']...\n",
    "    IN: ['broken', 'teeth', 'otterholm', 'ver', 'abyss', 'amid', 'drove', 'overcast', 'thrown', 'wind']...\n",
    "    RBR: ['shadow', 'farther', 'matter', 'feather', 'higher', 'longer', 'listen', 'explore', 'farther', 'limbs']...\n",
    "    VBG: ['falling', 'shining', 'particularizing', 'beetling', 'howling', 'shrieking', 'blowing', 'offing', 'dashing', 'increasing']...\n",
    "    FW: ['elbow', 'kircher']...\n",
    "    VB: ['raise', 'timid', 'morrow', 'watch', 'deck', 'elder', 'shake', 'slack', 'keel', 'hold']..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. [12 points] The conjecture of many linguists is that the number of different parts of speech per thousand words, (nouns, verbs, adjectives, adverbs, …). is pretty much the same for all stories in a given language. In this case, with all stories in English, and all from the same author, we expect it to be true. Is the conjecture consistent with your findings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I calculated the frequencies per 1000 for each part of speech for the first 3 short stories:\n",
    "\n",
    "Domain of Arnheim\n",
    "\n",
    "    POS frequencies per 1000 words for THE_DOMAIN_OF_ARNHEIM:\n",
    "        CC: 34.24\n",
    "        CD: 6.21\n",
    "        DT: 122.73\n",
    "        EX: 2.88\n",
    "        FW: 0.30\n",
    "        IN: 140.76\n",
    "        JJ: 80.61\n",
    "        JJR: 3.33\n",
    "        JJS: 3.33\n",
    "        MD: 9.85\n",
    "        NN: 170.76\n",
    "        NNP: 15.15\n",
    "        NNS: 39.24\n",
    "        PDT: 1.36\n",
    "        PRP: 26.82\n",
    "        PRP$: 17.42\n",
    "        RB: 45.45\n",
    "        RBR: 2.12\n",
    "        RBS: 2.27\n",
    "        RP: 1.52\n",
    "        TO: 20.61\n",
    "        VB: 25.30\n",
    "        VBD: 29.24\n",
    "        VBG: 13.64\n",
    "        VBN: 27.73\n",
    "        VBP: 10.15\n",
    "        VBZ: 22.27\n",
    "        WDT: 10.45\n",
    "        WP: 4.39\n",
    "        WP$: 0.91\n",
    "        WRB: 1.36"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bernice:\n",
    "\n",
    "        Processing file: BERENICE\n",
    "    POS frequencies per 1000 words for BERENICE:\n",
    "    CC: 47.74\n",
    "    CD: 1.92\n",
    "    DT: 102.61\n",
    "    EX: 1.65\n",
    "    IN: 134.43\n",
    "    JJ: 81.76\n",
    "    JJR: 0.82\n",
    "    JJS: 3.29\n",
    "    MD: 5.76\n",
    "    NN: 152.26\n",
    "    NNP: 20.85\n",
    "    NNS: 36.21\n",
    "    PDT: 0.82\n",
    "    PRP: 48.83\n",
    "    PRP$: 29.36\n",
    "    RB: 49.93\n",
    "    RBR: 3.29\n",
    "    RBS: 2.47\n",
    "    RP: 1.10\n",
    "    TO: 13.44\n",
    "    UH: 1.37\n",
    "    VB: 16.74\n",
    "    VBD: 49.11\n",
    "    VBG: 14.54\n",
    "    VBN: 21.40\n",
    "    VBP: 12.07\n",
    "    VBZ: 7.13\n",
    "    WDT: 5.76\n",
    "    WP: 1.37\n",
    "    WP$: 0.27\n",
    "    WRB: 1.92"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cask of Amontillado:\n",
    "\n",
    "        Processing file: THE_CASK_OF_AMONTILLADO\n",
    "    POS frequencies per 1000 words for THE_CASK_OF_AMONTILLADO:\n",
    "    CC: 35.45\n",
    "    CD: 4.49\n",
    "    DT: 106.76\n",
    "    EX: 2.56\n",
    "    FW: 0.23\n",
    "    IN: 130.49\n",
    "    JJ: 73.03\n",
    "    JJR: 2.98\n",
    "    JJS: 2.26\n",
    "    MD: 9.13\n",
    "    NN: 151.85\n",
    "    NNP: 18.77\n",
    "    NNPS: 0.16\n",
    "    NNS: 34.70\n",
    "    PDT: 1.21\n",
    "    PRP: 51.86\n",
    "    PRP$: 24.31\n",
    "    RB: 51.74\n",
    "    RBR: 2.38\n",
    "    RBS: 1.42\n",
    "    RP: 1.76\n",
    "    TO: 17.94\n",
    "    UH: 0.55\n",
    "    VB: 23.87\n",
    "    VBD: 52.29\n",
    "    VBG: 12.57\n",
    "    VBN: 24.72\n",
    "    VBP: 11.21\n",
    "    VBZ: 10.18\n",
    "    WDT: 8.21\n",
    "    WP: 3.84\n",
    "    WP$: 0.71\n",
    "    WRB: 3.02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I calculated the frequencies of Nouns, Verbs and Adjectives from the above output for each of the 3 stories.\n",
    "\n",
    "#### Nouns (NN, NNP, NNS):\n",
    "        THE_DOMAIN_OF_ARNHEIM: 225.15\n",
    "\n",
    "        BERENICE: 209.32\n",
    "\n",
    "        THE_CASK_OF_AMONTILLADO: 205.32\n",
    "\n",
    "#### Verbs (VB, VBD, VBG, VBN, VBP, VBZ):\n",
    "        THE_DOMAIN_OF_ARNHEIM: 128.33\n",
    "\n",
    "        BERENICE: 120.99\n",
    "\n",
    "        THE_CASK_OF_AMONTILLADO: 134.84\n",
    "\n",
    "#### Adjectives (JJ, JJR, JJS):\n",
    "        THE_DOMAIN_OF_ARNHEIM: 87.27\n",
    "\n",
    "        BERENICE: 85.87\n",
    "\n",
    "        THE_CASK_OF_AMONTILLADO: 78.27\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above calculations, the data from these 3 short stories does support the conjecure from linguists. All 3 parts of speech had very similar frequencies per 1000 words, as can be seen above. The code I used is pasted below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "def calculate_pos_frequencies_per_thousand(file_path):\n",
    "    # Load and tokenize the text\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "    tokens = word_tokenize(text)\n",
    "    total_words = len(tokens)\n",
    "\n",
    "    # Tag tokens with POS\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "\n",
    "    # Count POS frequencies\n",
    "    pos_counts = defaultdict(int)\n",
    "    for word, pos in tagged_tokens:\n",
    "        pos_counts[pos] += 1\n",
    "\n",
    "    # Normalize frequencies per 1000 words\n",
    "    pos_frequencies_per_thousand = {pos: (count / total_words) * 1000 for pos, count in pos_counts.items()}\n",
    "\n",
    "    return pos_frequencies_per_thousand\n",
    "\n",
    "def main():\n",
    "    # Get the current working directory (where the script and text files are located)\n",
    "    directory = \".\"\n",
    "\n",
    "\n",
    "    # Loop through each file in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        # Skip Python scripts and process only non-Python files\n",
    "        if  os.path.isfile(os.path.join(directory, filename)):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            print(f\"Processing file: {filename}\")\n",
    "\n",
    "            # Calculate POS frequencies per 1000 words\n",
    "            pos_frequencies = calculate_pos_frequencies_per_thousand(file_path)\n",
    "\n",
    "            # Print the results\n",
    "            print(f\"POS frequencies per 1000 words for {filename}:\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
