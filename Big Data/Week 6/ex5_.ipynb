{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5: Spark APIs [100 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Accumulators [10 points]\n",
    "[10 points].The title of this Q&A is wrong. It’s really about global variables (aka accumulators). The question shows code that is incorrect. \n",
    "val data = Array(1,2,3,4,5)\n",
    "var counter = 0\n",
    "var rdd = sc.parallelize(data)\n",
    "\n",
    "// Wrong: Don't do this!!\n",
    "rdd.foreach(x => counter += x)\n",
    "\n",
    "println(\"Counter value: \" + counter)\n",
    "Write a corrected version of the code and demonstrate its intended operation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code I ran in the spark-shell and the resulting output is below: \n",
    "Note: I edited the format slightly to fully fit when converting to pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "\n",
    "    scala> // Create an RDD\n",
    "\n",
    "    scala> val data = Array(1, 2, 3, 4, 5)\n",
    "    data: Array[Int] = Array(1, 2, 3, 4, 5)\n",
    "\n",
    "    scala> val rdd = sc.parallelize(data)\n",
    "    rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[2] at parallelize at <console>:26\n",
    "\n",
    "    scala> \n",
    "\n",
    "    scala> // Use LongAccumulator (correct way)\n",
    "\n",
    "    scala> val counter = sc.longAccumulator(\"Counter Accumulator\") // Pass a string as the name\n",
    "    counter: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 0, name: \n",
    "    Some(Counter Accumulator), value: 0)\n",
    "\n",
    "    scala> \n",
    "\n",
    "    scala> // Perform operation using accumulator\n",
    "\n",
    "    scala> rdd.foreach(x => counter.add(x)) // Add values to the accumulator\n",
    "                                                                                    \n",
    "    scala> \n",
    "\n",
    "    scala> // Print the accumulator value\n",
    "\n",
    "    scala> println(\"Counter value: \" + counter.value)\n",
    "    Counter value: 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Airline Traffic [45 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. [15 points] Describe in words and in code (where applicable) the steps you took to set up the environment for gathering the statistical data in the below questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. [6 points] Which US Airline Has the Least Delays? Report by full names, (e.g., Delta Airlines, not DL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rows with valid Carrier codes: 1225398\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "    # Define documented column names\n",
    "documented_columns = [\n",
    "        'Carrier', 'FlightNumber',\n",
    "        'Undocumented_1', 'Undocumented_2', \n",
    "         'Undocumented_3',\n",
    "          'Undocumented_4', # Placeholder for extra columns\n",
    "          'DepartureAirport', 'ArrivalAirport',\n",
    "          'FlightDate', 'DayOfWeek',\n",
    "        'ScheduledDepartureTime_OAG', 'ScheduledDepartureTime_CRS',\n",
    "        'ActualDepartureTime', 'ScheduledArrivalTime_OAG', 'ScheduledArrivalTime_CRS',\n",
    "        'ActualArrivalTime', 'Diff_ScheduledDepartureTimes', 'Diff_ScheduledArrivalTimes',\n",
    "        'ScheduledElapsedMinutes', 'DepartureDelayMinutes', 'ArrivalDelayMinutes',\n",
    "        'Diff_ElapsedMinutes', 'WheelsOffTime', 'WheelsOnTime', 'AircraftTailNumber',\n",
    "        'CancellationCode', 'MinutesLate_DelayCodeE', 'MinutesLate_DelayCodeF',\n",
    "        'MinutesLate_DelayCodeG', 'MinutesLate_DelayCodeH', 'MinutesLate_DelayCodeI'\n",
    "    ]\n",
    "\n",
    "    # Determine the total number of columns in the data\n",
    "with open('ontime.td.202406.asc', 'r') as f:\n",
    "        first_line = f.readline()\n",
    "        total_columns = len(first_line.split('|'))\n",
    "\n",
    "    # Generate column names\n",
    "if total_columns > len(documented_columns):\n",
    "        extra_columns = total_columns - len(documented_columns)\n",
    "        column_names = documented_columns + [f'ExtraColumn_{i}' for i in range(1, extra_columns + 1)]\n",
    "else:\n",
    "        column_names = documented_columns[:total_columns]\n",
    "\n",
    "    # Specify data types for all columns as strings\n",
    "dtype_spec = {col: str for col in column_names}\n",
    "\n",
    "    # Step 6: Load the datasets for June 2024 and July 2024\n",
    "june_data = pd.read_csv('ontime.td.202406.asc', delimiter='|', header=None, names=column_names, dtype=dtype_spec, low_memory=False)\n",
    "july_data = pd.read_csv('ontime.td.202407.asc', delimiter='|', header=None, names=column_names, dtype=dtype_spec, low_memory=False)\n",
    "\n",
    "combined_data = pd.concat([june_data, july_data], ignore_index=True)\n",
    "\n",
    "    # Remove the 2 undocumented columns and extra ones\n",
    "columns_to_keep = [\n",
    "        'Carrier', 'FlightNumber',\n",
    "        'DepartureAirport', 'ArrivalAirport', 'FlightDate', 'DayOfWeek',\n",
    "        'ScheduledDepartureTime_OAG', 'ScheduledDepartureTime_CRS',\n",
    "        'ActualDepartureTime', 'ScheduledArrivalTime_OAG', 'ScheduledArrivalTime_CRS',\n",
    "        'ActualArrivalTime', 'Diff_ScheduledDepartureTimes', 'Diff_ScheduledArrivalTimes',\n",
    "        'ScheduledElapsedMinutes', 'DepartureDelayMinutes', 'ArrivalDelayMinutes',\n",
    "        'Diff_ElapsedMinutes', 'WheelsOffTime', 'WheelsOnTime', 'AircraftTailNumber',\n",
    "        'CancellationCode', 'MinutesLate_DelayCodeE', 'MinutesLate_DelayCodeF',\n",
    "        'MinutesLate_DelayCodeG', 'MinutesLate_DelayCodeH', 'MinutesLate_DelayCodeI'\n",
    "    ]\n",
    "combined_data = combined_data[columns_to_keep]\n",
    "\n",
    "    # Clean Carrier column\n",
    "combined_data['Carrier'] = combined_data['Carrier'].str.upper().str.strip()\n",
    "\n",
    "    # Filter valid Carrier codes (two uppercase letters)\n",
    "valid_carrier_pattern = re.compile(r'^[A-Z]{2}$')\n",
    "carrier_filter = combined_data['Carrier'].str.match(valid_carrier_pattern, na=False)\n",
    "print(f\"\\nRows with valid Carrier codes: {carrier_filter.sum()}\")\n",
    "combined_data = combined_data[carrier_filter]\n",
    "\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code shows how I parsed and cleaned the raw data to be able to answer the questions.\n",
    "I had to assign labels to the columns. I had to account for the 4 extra columns between B and C\n",
    "like the instructions said.\n",
    "I decided to declare all the variables as strings and then convert to int as necessary. I used this method because\n",
    "I was getting a lot of improper data type errors and couldn't figure them out otherwise. This was likely due to the\n",
    "column labels being offset by two at first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carrier\n",
      "Southwest Airlines    125.858359\n",
      "American Airlines     129.479622\n",
      "Delta Airlines        131.965433\n",
      "Hawaiian Airlines     140.665946\n",
      "United Airlines       141.952446\n",
      "Name: DepartureDelayMinutes, dtype: float64\n",
      "\n",
      "The airline with the least delays is Southwest Airlines with an average delay of 125.86 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Filter valid DepartureDelayMinutes\n",
    "    # Convert column to numeric, forcing errors to NaN\n",
    "combined_data['DepartureDelayMinutes'] = pd.to_numeric(combined_data['DepartureDelayMinutes'], errors='coerce')\n",
    "\n",
    "    # Drop rows where conversion resulted in NaN\n",
    "combined_data = combined_data.dropna(subset=['DepartureDelayMinutes'])\n",
    "\n",
    "    # Convert from float to int \n",
    "combined_data['DepartureDelayMinutes'] = combined_data['DepartureDelayMinutes'].astype(int)\n",
    "\n",
    "relevant_data = combined_data.dropna(subset=['DepartureDelayMinutes'])\n",
    "\n",
    "    # Check the dataset isn't empty (big problem at first) and calculate the mean delay\n",
    "if relevant_data.empty:\n",
    "        print(\"\\nThe cleaned dataset is empty. No valid rows found.\")\n",
    "else:\n",
    "        # Group by Carrier and calculate the mean delay (including all flights)\n",
    "    average_delays = relevant_data.groupby('Carrier')['DepartureDelayMinutes'].mean()\n",
    "\n",
    "# Sort by average delay\n",
    "    sorted_delays = average_delays.sort_values()\n",
    "\n",
    "    airline_mapping = {\n",
    "                'DL': 'Delta Airlines',\n",
    "                'AA': 'American Airlines',\n",
    "                'UA': 'United Airlines',\n",
    "                'WN': 'Southwest Airlines',\n",
    "                'AS': 'Alaska Airlines',\n",
    "                'B6': 'JetBlue Airways',\n",
    "                'NK': 'Spirit Airlines',\n",
    "                'F9': 'Frontier Airlines',\n",
    "                'HA': 'Hawaiian Airlines',\n",
    "                'G4': 'Allegiant Air',\n",
    "                'YX': 'Midwest Airlines',\n",
    "                'OO': 'SkyWest Airlines',\n",
    "                'MQ': 'Envoy Air',\n",
    "                'OH': 'PSA Airlines',\n",
    "                'YV': 'Mesa Airlines',\n",
    "                'QX': 'Horizon Air',\n",
    "                'EV': 'ExpressJet Airlines'\n",
    "            }\n",
    "\n",
    "            # Convert airline codes to full names\n",
    "    sorted_delays.index = sorted_delays.index.map(airline_mapping)\n",
    "\n",
    "            # Report the airline with the least delays\n",
    "    least_delay_airline = sorted_delays.idxmin()\n",
    "    least_delay_value = sorted_delays.min()\n",
    "    print(sorted_delays.head(5))\n",
    "\n",
    "    print(f\"\\nThe airline with the least delays is {least_delay_airline} with an average delay of {least_delay_value:.2f} minutes.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code is how I calculated the airlines with the least average delays. \n",
    "I converted the departure delays to int to be able to calculate this. \n",
    "The top 5 least delayed airlines are shown above. SouthWest Airlines ended\n",
    "up being the airline with the least average delays. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. [6 points] What Departure Time of Day Is Best to Avoid Flight Delays, segmented into 5 time blocks [night (10 pm - 6 am), morning (6 am to 10 am), mid-day (10 am to 2 pm), afternoon (2 pm - 6 pm), evening (6 pm - 10 pm)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeBlock\n",
      "Night        132.867186\n",
      "Mid-Day      135.265775\n",
      "Afternoon    135.455740\n",
      "Evening      137.914275\n",
      "Morning      175.985673\n",
      "Name: DepartureDelayMinutes, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def categorize_time(hour):\n",
    "    if 22 <= hour or hour < 6:\n",
    "        return 'Night'\n",
    "    elif 6 <= hour < 10:\n",
    "        return 'Morning'\n",
    "    elif 10 <= hour < 14:\n",
    "        return 'Mid-Day'\n",
    "    elif 14 <= hour < 18:\n",
    "        return 'Afternoon'\n",
    "    else:\n",
    "        return 'Evening'\n",
    "\n",
    "## Find the best time of day to avoid delays\n",
    "combined_data['DepartureHour'] = combined_data['ActualDepartureTime'].str[:2]\n",
    "combined_data['DepartureHour'] = pd.to_numeric(combined_data['DepartureHour'], errors='coerce')\n",
    "combined_data['TimeBlock'] = combined_data['DepartureHour'].apply(categorize_time)\n",
    "timeblock_delays = combined_data.groupby('TimeBlock')['DepartureDelayMinutes'].mean().sort_values()\n",
    "print(timeblock_delays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for calculating the time block with the least delays is above. Night was the best \n",
    "departure in terms of avoiding delays. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. [5 points] Which Airports Have The Most Flight Delays? Report by full name, (e.g., “Newark Liberty International,” not “EWR,” when the airport code EWR is provided)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pago Pago International                      334.461538\n",
      "Aeropuerto Internacional Rafael Hernández    254.245902\n",
      "San Juan - Luis Muñoz Marín International    247.113487\n",
      "Guam International                           242.926230\n",
      "Cyril E. King Airport                        242.833724\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "## Find the airports with the most delays\n",
    "airport_mapping = {\n",
    "    'ATL': 'Atlanta - Hartsfield Jackson',\n",
    "    'BWI': \"Baltimore/Wash. Int'l Thurgood Marshall\",\n",
    "    'BOS': 'Boston - Logan International',\n",
    "    'CLT': 'Charlotte - Douglas',\n",
    "    'MDW': 'Chicago - Midway',\n",
    "    'ORD': \"Chicago - O'Hare\",\n",
    "    'CVG': 'Cincinnati Greater Cincinnati',\n",
    "    'DFW': 'Dallas-Fort Worth International',\n",
    "    'DEN': 'Denver - International',\n",
    "    'DTW': 'Detroit - Metro Wayne County',\n",
    "    'FLL': 'Fort Lauderdale Hollywood International',\n",
    "    'IAH': 'Houston - George Bush International',\n",
    "    'LAS': 'Las Vegas - McCarran International',\n",
    "    'LAX': 'Los Angeles International',\n",
    "    'MIA': 'Miami International',\n",
    "    'MSP': 'Minneapolis-St. Paul International',\n",
    "    'EWR': 'Newark Liberty International',\n",
    "    'JFK': 'New York - JFK International',\n",
    "    'LGA': 'New York - LaGuardia',\n",
    "    'MCO': 'Orlando International',\n",
    "    'OAK': 'Oakland International',\n",
    "    'PHL': 'Philadelphia International',\n",
    "    'PHX': 'Phoenix - Sky Harbor International',\n",
    "    'PDX': 'Portland International',\n",
    "    'SLC': 'Salt Lake City International',\n",
    "    'STL': 'St. Louis Lambert International',\n",
    "    'SAN': 'San Diego Intl. Lindbergh Field',\n",
    "    'SFO': 'San Francisco International',\n",
    "    'SEA': 'Seattle-Tacoma International',\n",
    "    'TPA': 'Tampa International',\n",
    "    'DCA': 'Washington - Reagan National',\n",
    "    'IAD': 'Washington - Dulles International',\n",
    "    'PPG': 'Pago Pago International',\n",
    "    'GUM': 'Guam International',\n",
    "    'HNL': 'Honolulu International',\n",
    "    'OGG': 'Kahului Airport',\n",
    "    'KOA': 'Kona International',\n",
    "    'LIH': 'Lihue Airport',\n",
    "    'ITO': 'Hilo International',\n",
    "    'BQN': 'Aeropuerto Internacional Rafael Hernández',\n",
    "    'SJU': 'San Juan - Luis Muñoz Marín International',\n",
    "    'STT': 'Cyril E. King Airport'\n",
    "}\n",
    "\n",
    "    # Find the airports with the most delays\n",
    "combined_data['ArrivalDelayMinutes'] = pd.to_numeric(combined_data['ArrivalDelayMinutes'], errors='coerce')\n",
    "combined_data['DepartureDelayMinutes'] = pd.to_numeric(combined_data['DepartureDelayMinutes'], errors='coerce')\n",
    "\n",
    "arrival_delay = combined_data.groupby('ArrivalAirport')['ArrivalDelayMinutes'].mean().sort_values(ascending=False)\n",
    "departure_delay = combined_data.groupby('DepartureAirport')['DepartureDelayMinutes'].mean().sort_values(ascending=False)\n",
    "total_delay = arrival_delay + departure_delay\n",
    "\n",
    "total_delay.index = total_delay.index.to_series().replace(airport_mapping)\n",
    "total_delay = total_delay.sort_values(ascending=False)\n",
    "print(total_delay.head(5))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above shows the airports with the most delays. Pago Pago International in Puerto Rico was the airport with the most delays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [5 points] What Are the Top 5 Busiest Airports in the US. Report by full name, (e.g., “Newark Liberty International,” not “EWR”)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Busiest Airports in the US (by total arrivals and departures):\n",
      "Atlanta - Hartsfield Jackson       114182\n",
      "Dallas-Fort Worth International    110252\n",
      "Chicago - O'Hare                   109931\n",
      "Denver - International             101826\n",
      "Charlotte - Douglas                 86198\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count total arrivals and departures\n",
    "arrivals_count = combined_data['ArrivalAirport'].value_counts()\n",
    "departures_count = combined_data['DepartureAirport'].value_counts()\n",
    "\n",
    "# Sum arrivals and departures for each airport\n",
    "total_flights = arrivals_count.add(departures_count, fill_value=0)\n",
    "\n",
    "# Map airport codes to full names\n",
    "total_flights.index = total_flights.index.to_series().replace(airport_mapping)\n",
    "\n",
    "# Sort in descending order and get the top 5\n",
    "top_5_busiest_airports = total_flights.sort_values(ascending=False).head(5)\n",
    "\n",
    "# Display result\n",
    "print(\"Top 5 Busiest Airports in the US (by total arrivals and departures):\")\n",
    "print(top_5_busiest_airports)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above output shows the busiest airports in the US by total arrivals and departures. Atlanta was the busiest overall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ShortStoryJam [45 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. [3 points] To seed the effort, the text of about 22 short stories by Edgar Allan Poe, he of the “quoth the raven” fame, are available in my github repository. Clean the text and remove stopwords, as specified in a previous assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import re\n",
    "import string\n",
    "import requests\n",
    "import sys\n",
    "\n",
    "def load_stopwords():\n",
    "    url = \"https://gist.githubusercontent.com/rg089/35e00abf8941d72d419224cfd5b5925d/raw/12d899b70156fd0041fa9778d657330b024b959c/stopwords.txt\"\n",
    "    stopwords_list = requests.get(url).content\n",
    "    return set(stopwords_list.decode().splitlines())\n",
    "\n",
    "stopwords = load_stopwords()\n",
    "\n",
    "def process_text(text, is_recursive=False):\n",
    "    if not is_recursive:\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'\\[.*?\\]', '', text)\n",
    "        text = re.sub(r'[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "        text = re.sub(r'\\d+', ' ', text)\n",
    "        text = ' '.join(remove_stopwords(text))\n",
    "        return process_text(text, is_recursive=True)\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    words_list = re.sub(r'[^a-zA-Z0-9]', \" \", words.lower()).split()\n",
    "    return [word for word in words_list if word not in stopwords]\n",
    "\n",
    "def process_file(filename):\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            processed_text = process_text(text)\n",
    "            return processed_text\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{filename}' not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "paragraph = process_file('A_DESCENT_INTO_THE_MAELSTROM')\n",
    "print(paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used the above code from my assignment 5 to clean the 22 short stories. It was modified to accept filenames instead of just text.\n",
    "The output is from \"A Descent Into the Maelstorm\" as an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. [8 points] Use NLTK to decompose the first story (A_DESCENT_INTO…) into sentences & sentences into tokens. Here is the code for doing that, after you set the variable paragraph to hold the text of the story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Library/Frameworks/Python.fr\n",
      "[nltk_data]     amework/Versions/3.12/lib/python3.12/site-packages/...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to /Library\n",
      "[nltk_data]     /Frameworks/Python.framework/Versions/3.12/lib/python3\n",
      "[nltk_data]     .12/site-packages/...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt', download_dir='/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/')\n",
    "nltk.download('averaged_perceptron_tagger', download_dir='/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download()\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "sent_text = nltk.sent_tokenize(paragraph) # this gives us a list of sentences\n",
    "# now loop over each sentence and tokenize it separately\n",
    "all_tagged = [nltk.pos_tag(nltk.word_tokenize(sent)) for sent in sent_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was having issues with the given code, so I changed the download to \"nltk.download()\". I got this from a stack overflow post. The whole python code is pasted below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import re\n",
    "import string\n",
    "import requests\n",
    "import sys\n",
    "import nltk\n",
    "\n",
    "def load_stopwords():\n",
    "    url = \"https://gist.githubusercontent.com/rg089/35e00abf8941d72d419224cfd5b5925d/raw/12d899b70156fd0041fa9778d657330b024b959c/stopwords.txt\"\n",
    "    stopwords_list = requests.get(url).content\n",
    "    return set(stopwords_list.decode().splitlines())\n",
    "\n",
    "stopwords = load_stopwords()\n",
    "\n",
    "def process_text(text, is_recursive=False):\n",
    "    if not is_recursive:\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'\\[.*?\\]', '', text)\n",
    "        text = re.sub(r'[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "        text = re.sub(r'\\d+', ' ', text)\n",
    "        text = ' '.join(remove_stopwords(text))\n",
    "        return process_text(text, is_recursive=True)\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    words_list = re.sub(r'[^a-zA-Z0-9]', \" \", words.lower()).split()\n",
    "    return [word for word in words_list if word not in stopwords]\n",
    "\n",
    "def process_file(filename):\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            processed_text = process_text(text)\n",
    "            return processed_text\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{filename}' not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "paragraph = process_file('A_DESCENT_INTO_THE_MAELSTROM')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "sent_text = nltk.sent_tokenize(paragraph) # this gives us a list of sentences\n",
    "# now loop over each sentence and tokenize it separately\n",
    "all_tagged = [nltk.pos_tag(nltk.word_tokenize(sent)) for sent in sent_text]                                                                                                                                   22,52         All"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. [11 points] Tag all remaining words in the story as parts of speech using the Penn POS Tags. This SO answer shows how to obtain the POS tag values. Create and print a dictionary with the Penn POS Tags as keys and a list of words as the values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THe code and explanation for this part is below question 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  4. [11 points] In this framework, each row will represent a story. The columns will be as follows:\n",
    "The text of the story,\n",
    "Two-letter prefixes of each tag, for example NN, VB, RB, JJ etc.and the words belonging to that tag in the story. \n",
    "Show your code and the tag columns, at least for the one story.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for this and part 3 combined is below. I used \"A Descent inta the Maelstorm\" as an example again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import re\n",
    "import string\n",
    "import requests\n",
    "import sys\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import os\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')  # Required for sentence tokenization\n",
    "nltk.download('averaged_perceptron_tagger')  # Required for POS tagging\n",
    "\n",
    "def load_stopwords():\n",
    "    url = \"https://gist.githubusercontent.com/rg089/35e00abf8941d72d419224cfd5b5925d/raw/12d899b70156fd0041fa9778d657330b024b959c/stopwords.txt\"\n",
    "    stopwords_list = requests.get(url).content\n",
    "    return set(stopwords_list.decode().splitlines())\n",
    "\n",
    "stopwords = load_stopwords()\n",
    "\n",
    "def process_text(text, is_recursive=False):\n",
    "    if not is_recursive:\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'\\[.*?\\]', '', text)\n",
    "        text = re.sub(r'[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "        text = re.sub(r'\\d+', ' ', text)\n",
    "        text = ' '.join(remove_stopwords(text))\n",
    "        return process_text(text, is_recursive=True)\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    words_list = re.sub(r'[^a-zA-Z0-9]', \" \", words.lower()).split()\n",
    "    return [word for word in words_list if word not in stopwords]\n",
    "\n",
    "def process_file(filename):\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "            text = file.read()\n",
    "            processed_text = process_text(text)\n",
    "            return processed_text\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{filename}' not found.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing the file: {e}\")\n",
    "        return None\n",
    "\n",
    "# Process the file\n",
    "filename = 'A_DESCENT_INTO_THE_MAELSTROM.txt'\n",
    "paragraph = process_file(filename)\n",
    "\n",
    "if paragraph is not None:\n",
    "\n",
    "    # Tokenize sentences and perform POS tagging\n",
    "    sent_text = sent_tokenize(paragraph)  # Sentence tokenization\n",
    "    all_tagged = [nltk.pos_tag(word_tokenize(sent)) for sent in sent_text]  # POS tagging\n",
    "\n",
    "    # Flatten the list of tagged words\n",
    "    flat_tagged_words = [word_tag for sentence in all_tagged for word_tag in sentence]\n",
    "\n",
    "    # Create a dictionary with POS tags as keys and lists of words as values\n",
    "    pos_dict = {}\n",
    "    for word, tag in flat_tagged_words:\n",
    "        if tag not in pos_dict:\n",
    "            pos_dict[tag] = []\n",
    "        pos_dict[tag].append(word)\n",
    "\n",
    "    # Print the dictionary\n",
    "    print(\"\\nPOS Tags Dictionary:\")\n",
    "    for tag, words in pos_dict.items():\n",
    "        print(f\"{tag}: {words[:10]}...\")  # Print only the first 10 words for brevity\n",
    "else:\n",
    "    print(\"No text to process due to file error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tag Output for the first 10 of each part of speech:\n",
    "\n",
    "    POS Tags Dictionary:\n",
    "    NNS: ['ways', 'ways', 'models', 'minutes', 'sons', 'years', 'hours', 'limbs', 'nerves', 'feet']...\n",
    "    VBP: ['god', 'frame', 'terror', 'brink', 'mountain', 'guide', 'sea', 'land', 'remote', 'swell']...\n",
    "    JJ: ['nature', 'commensurate', 'unsearchableness', 'speak', 'mortal', 'suppose', 'single', 'jetty', 'black', 'white']...\n",
    "    NN: ['providence', 'vastness', 'profundity', 'democritus', 'joseph', 'glanville', 'summit', 'crag', 'man', 'length']...\n",
    "    VBZ: ['works', 'hairs', 'assumes', 'passages', 'decreases', 'runs', 'leagues', 'whales', 'rocks', 'precipitates']...\n",
    "    RB: ['depth', 'long', 'ago', 'deadly', 'scarcely', 'carelessly', 'beneath', 'deeply', 'length', 'upward']...\n",
    "    JJR: ['greater', 'weightier', 'higher', 'smaller', 'yonder', 'greater', 'smaller', 'higher', 'deeper', 'lower']...\n",
    "    VBD: ['reached', 'exhausted', 'guided', 'happened', 'happened', 'survived', 'frightened', 'hung', 'arose', 'tempted']...\n",
    "    JJS: ['loftiest', 'youngest', 'divest', 'crest', 'faintest', 'loudest', 'highest', 'honest', 'largest', 'finest']...\n",
    "    VBN: ['endured', 'beheld', 'reared', 'called', 'ascended', 'set', 'acquired', 'lashed', 'phrensied', 'assumed']...\n",
    "    IN: ['broken', 'teeth', 'otterholm', 'ver', 'abyss', 'amid', 'drove', 'overcast', 'thrown', 'wind']...\n",
    "    RBR: ['shadow', 'farther', 'matter', 'feather', 'higher', 'longer', 'listen', 'explore', 'farther', 'limbs']...\n",
    "    VBG: ['falling', 'shining', 'particularizing', 'beetling', 'howling', 'shrieking', 'blowing', 'offing', 'dashing', 'increasing']...\n",
    "    FW: ['elbow', 'kircher']...\n",
    "    VB: ['raise', 'timid', 'morrow', 'watch', 'deck', 'elder', 'shake', 'slack', 'keel', 'hold']..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. [12 points] The conjecture of many linguists is that the number of different parts of speech per thousand words, (nouns, verbs, adjectives, adverbs, …). is pretty much the same for all stories in a given language. In this case, with all stories in English, and all from the same author, we expect it to be true. Is the conjecture consistent with your findings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I calculated the frequencies per 1000 for each part of speech for the first 3 short stories My code returns it for every story, but I analyzed just the frequencies of the first 3 for simplicity:\n",
    "\n",
    "Domain of Arnheim\n",
    "\n",
    "    POS frequencies per 1000 words for THE_DOMAIN_OF_ARNHEIM:\n",
    "        CC: 34.24\n",
    "        CD: 6.21\n",
    "        DT: 122.73\n",
    "        EX: 2.88\n",
    "        FW: 0.30\n",
    "        IN: 140.76\n",
    "        JJ: 80.61\n",
    "        JJR: 3.33\n",
    "        JJS: 3.33\n",
    "        MD: 9.85\n",
    "        NN: 170.76\n",
    "        NNP: 15.15\n",
    "        NNS: 39.24\n",
    "        PDT: 1.36\n",
    "        PRP: 26.82\n",
    "        PRP$: 17.42\n",
    "        RB: 45.45\n",
    "        RBR: 2.12\n",
    "        RBS: 2.27\n",
    "        RP: 1.52\n",
    "        TO: 20.61\n",
    "        VB: 25.30\n",
    "        VBD: 29.24\n",
    "        VBG: 13.64\n",
    "        VBN: 27.73\n",
    "        VBP: 10.15\n",
    "        VBZ: 22.27\n",
    "        WDT: 10.45\n",
    "        WP: 4.39\n",
    "        WP$: 0.91\n",
    "        WRB: 1.36"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bernice:\n",
    "\n",
    "        Processing file: BERENICE\n",
    "    POS frequencies per 1000 words for BERENICE:\n",
    "    CC: 47.74\n",
    "    CD: 1.92\n",
    "    DT: 102.61\n",
    "    EX: 1.65\n",
    "    IN: 134.43\n",
    "    JJ: 81.76\n",
    "    JJR: 0.82\n",
    "    JJS: 3.29\n",
    "    MD: 5.76\n",
    "    NN: 152.26\n",
    "    NNP: 20.85\n",
    "    NNS: 36.21\n",
    "    PDT: 0.82\n",
    "    PRP: 48.83\n",
    "    PRP$: 29.36\n",
    "    RB: 49.93\n",
    "    RBR: 3.29\n",
    "    RBS: 2.47\n",
    "    RP: 1.10\n",
    "    TO: 13.44\n",
    "    UH: 1.37\n",
    "    VB: 16.74\n",
    "    VBD: 49.11\n",
    "    VBG: 14.54\n",
    "    VBN: 21.40\n",
    "    VBP: 12.07\n",
    "    VBZ: 7.13\n",
    "    WDT: 5.76\n",
    "    WP: 1.37\n",
    "    WP$: 0.27\n",
    "    WRB: 1.92"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cask of Amontillado:\n",
    "\n",
    "        Processing file: THE_CASK_OF_AMONTILLADO\n",
    "    POS frequencies per 1000 words for THE_CASK_OF_AMONTILLADO:\n",
    "    CC: 35.45\n",
    "    CD: 4.49\n",
    "    DT: 106.76\n",
    "    EX: 2.56\n",
    "    FW: 0.23\n",
    "    IN: 130.49\n",
    "    JJ: 73.03\n",
    "    JJR: 2.98\n",
    "    JJS: 2.26\n",
    "    MD: 9.13\n",
    "    NN: 151.85\n",
    "    NNP: 18.77\n",
    "    NNPS: 0.16\n",
    "    NNS: 34.70\n",
    "    PDT: 1.21\n",
    "    PRP: 51.86\n",
    "    PRP$: 24.31\n",
    "    RB: 51.74\n",
    "    RBR: 2.38\n",
    "    RBS: 1.42\n",
    "    RP: 1.76\n",
    "    TO: 17.94\n",
    "    UH: 0.55\n",
    "    VB: 23.87\n",
    "    VBD: 52.29\n",
    "    VBG: 12.57\n",
    "    VBN: 24.72\n",
    "    VBP: 11.21\n",
    "    VBZ: 10.18\n",
    "    WDT: 8.21\n",
    "    WP: 3.84\n",
    "    WP$: 0.71\n",
    "    WRB: 3.02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I calculated the frequencies of Nouns, Verbs and Adjectives from the above output for each of the 3 stories.\n",
    "\n",
    "#### Nouns (NN, NNP, NNS):\n",
    "        THE_DOMAIN_OF_ARNHEIM: 225.15\n",
    "\n",
    "        BERENICE: 209.32\n",
    "\n",
    "        THE_CASK_OF_AMONTILLADO: 205.32\n",
    "\n",
    "#### Verbs (VB, VBD, VBG, VBN, VBP, VBZ):\n",
    "        THE_DOMAIN_OF_ARNHEIM: 128.33\n",
    "\n",
    "        BERENICE: 120.99\n",
    "\n",
    "        THE_CASK_OF_AMONTILLADO: 134.84\n",
    "\n",
    "#### Adjectives (JJ, JJR, JJS):\n",
    "        THE_DOMAIN_OF_ARNHEIM: 87.27\n",
    "\n",
    "        BERENICE: 85.87\n",
    "\n",
    "        THE_CASK_OF_AMONTILLADO: 78.27\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above calculations, the data from these 3 short stories does support the conjecure from linguists. All 3 parts of speech had very similar frequencies per 1000 words, as can be seen above. The code I used is pasted below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "def calculate_pos_frequencies_per_thousand(file_path):\n",
    "    # Load and tokenize the text\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "    tokens = word_tokenize(text)\n",
    "    total_words = len(tokens)\n",
    "\n",
    "    # Tag tokens with POS\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "\n",
    "    # Count POS frequencies\n",
    "    pos_counts = defaultdict(int)\n",
    "    for word, pos in tagged_tokens:\n",
    "        pos_counts[pos] += 1\n",
    "\n",
    "    # Normalize frequencies per 1000 words\n",
    "    pos_frequencies_per_thousand = {pos: (count / total_words) * 1000 for pos, count in pos_counts.items()}\n",
    "\n",
    "    return pos_frequencies_per_thousand\n",
    "\n",
    "def main():\n",
    "    # Get the current working directory (where the script and text files are located)\n",
    "    directory = \".\"\n",
    "\n",
    "\n",
    "    # Loop through each file in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        # Skip Python scripts and process only non-Python files\n",
    "        if  os.path.isfile(os.path.join(directory, filename)):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            print(f\"Processing file: {filename}\")\n",
    "\n",
    "            # Calculate POS frequencies per 1000 words\n",
    "            pos_frequencies = calculate_pos_frequencies_per_thousand(file_path)\n",
    "\n",
    "            # Print the results\n",
    "            print(f\"POS frequencies per 1000 words for {filename}:\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
