{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6: Data Streams [100 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Querying Users [30 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code/psuedo code outline for the process. Since there are only 6000 messages, we will process all of them. We first receive all of them first and store them in a default dict. Then we classify them based on the frequency. The result should be a printed dictionary whose keys are the frequency: 1,2,etc. The values at each key should be the query itself, therefore categorizing them based on the number of times that query has been seen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the slide referenced in the question, we will sample 1/10th of the stream as a subset. The code creates 10 hash buckets based on the user and the timestamp. This is so that each user is partitioned fully into a single bucket. Therefore, we can answer definitively if a specific user had repeated queries in a given day because all that user's queries will be in the same bucket. This will avoid storing partial data for the users and giving incorrect information about repeated queries. This could also be scaled up by using 100 buckets insteadf (since 10 buckets is 1/10th of the data as the sampling constraint found in the context slide). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hashlib import md5\n",
    "from collections import defaultdict\n",
    "\n",
    "def hash_bucket(user, query, timestamp, num_buckets=10):\n",
    "    # Optional: convert timestamp to day granularity\n",
    "    day = timestamp.split(\"T\")[0]  # '2025-04-05T13:55:00' -> '2025-04-05'\n",
    "    key = f\"{user}:{day}\"  # sample by user per day\n",
    "    bucket = int(md5(key.encode()).hexdigest(), 16) % num_buckets\n",
    "    return bucket\n",
    "\n",
    "# Storage for 1/10th of stream\n",
    "stored_queries = defaultdict(lambda: defaultdict(int))  # user -> (query -> count)\n",
    "\n",
    "# Simulated stream\n",
    "for ev in dispatcher.launch():\n",
    "    user = ev[\"user\"]\n",
    "    query = ev[\"msg\"]\n",
    "    timestamp = ev[\"timestamp\"]\n",
    "\n",
    "    if hash_bucket(user, query, timestamp) == 0:  # Keep only bucket 0\n",
    "        stored_queries[(user, timestamp.split(\"T\")[0])][query] += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get information about the number of queries for a specific user, something along the lines of the following code would be run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (user, day), queries in stored_queries.items():\n",
    "    for query, count in queries.items():\n",
    "        print(f\"{user} ran query '{query}' {count} time(s) on {day}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bloom Filter [40 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [10 pts] Create a Bloom Filter, approximately 1000-2000 bits in size, for detecting bad words (i.e., AFINN of -4 or -5). It should be designed to run in Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import hashlib\n",
    "\n",
    "class BloomFilter:\n",
    "    def __init__(self, size=2048, num_hashes=3):\n",
    "        self.size = size\n",
    "        self.num_hashes = num_hashes\n",
    "        self.bit_vector = [0] * size\n",
    "\n",
    "    def _hashes(self, word):\n",
    "        hashes = []\n",
    "        for i in range(self.num_hashes):\n",
    "            hash_digest = hashlib.md5(f\"{word}_{i}\".encode()).hexdigest()\n",
    "            index = int(hash_digest, 16) % self.size\n",
    "            hashes.append(index)\n",
    "        return hashes\n",
    "\n",
    "    def add(self, word):\n",
    "        for idx in self._hashes(word):\n",
    "            self.bit_vector[idx] = 1\n",
    "\n",
    "    def check(self, word):\n",
    "        return all(self.bit_vector[idx] for idx in self._hashes(word))\n",
    "\n",
    "    def to_base64(self):\n",
    "        # Pack bits into bytes\n",
    "        bitstring = ''.join(map(str, self.bit_vector))\n",
    "        byte_array = bytearray(int(bitstring[i:i+8], 2) for i in range(0, len(bitstring), 8))\n",
    "        return base64.b64encode(byte_array).decode()\n",
    "\n",
    "    # === Step 1: Load bad words ===\n",
    "with open('bad_words.txt') as f:  # You said you have this list now\n",
    "    bad_words = [line.strip().lower() for line in f if line.strip()]\n",
    "\n",
    "# === Step 2: Build Bloom Filter ===\n",
    "bloom = BloomFilter()\n",
    "for word in bad_words:\n",
    "    bloom.add(word)\n",
    "\n",
    "# === Step 3: Save Base64 bit vector to HDFS or local file ===\n",
    "bitstring = bloom.to_base64()\n",
    "with open('bloomfilter_base64.txt', 'w') as f:\n",
    "    f.write(bitstring)\n",
    "\n",
    "print(\"Bloom filter created and written to bloomfilter_base64.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [10 pts] The bit vector should be placed in HDFS  as a Base64-encoded text file and loaded into Spark from HDFS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ran the program and then the following command to put it in HDFS:\n",
    "\n",
    "        hdfs dfs -put bloomfilter_base64.txt /user/brian_farrell/bloomfilter_base64.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [15 pts] Integrate the Bloom Filter into Spark such that every arriving sentence is examined and passed along if none of the words in the sentence are bad words. Sentences that do contain bad words should be suppressed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work in Progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "import base64\n",
    "\n",
    "# Initialize Spark\n",
    "sc = SparkContext(appName=\"DrunkSpeechFilter\")\n",
    "ssc = StreamingContext(sc, 1)\n",
    "\n",
    "# Load Bloom filter from HDFS\n",
    "b64_string = sc.textFile(\"hdfs:///user/yourname/bloomfilter.b64\").collect()[0]\n",
    "bit_array = list(bin(int.from_bytes(base64.b64decode(b64_string), 'big'))[2:].zfill(2048))\n",
    "bit_vector = list(map(int, bit_array))\n",
    "\n",
    "# Reconstruct BloomFilter logic\n",
    "import hashlib\n",
    "\n",
    "def get_hashes(word, size=2048, num_hashes=3):\n",
    "    return [int(hashlib.md5(f\"{word}_{i}\".encode()).hexdigest(), 16) % size for i in range(num_hashes)]\n",
    "\n",
    "def is_bad(word):\n",
    "    return all(bit_vector[idx] for idx in get_hashes(word.lower()))\n",
    "\n",
    "def contains_bad_words(sentence):\n",
    "    return any(is_bad(word) for word in sentence.split())\n",
    "\n",
    "# Simulated DStream (you could replace with socketTextStream for real input)\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "\n",
    "# Filter clean sentences\n",
    "clean_sentences = lines.filter(lambda sentence: not contains_bad_words(sentence))\n",
    "\n",
    "# Output\n",
    "clean_sentences.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Counting Unique Users [30 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify read_stdin.py to implement the HyperLogLog algorithm. Increase the number of senders and decrease the (μ,σ) of the delay between queries until the receiver can no longer keep up! Draw a graph of the estimated number of users as a function of elapsed time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The modified click-feeder.py program is below that significantly increases the users used to cause the std-in to not be able to keep up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Configuration: High number of users, low delay\n",
    "NUM_USERS = 10000               # Total users to simulate\n",
    "NUM_QUERIES = 100              # Unique queries each user might send\n",
    "TOTAL_EVENTS = 10000000         # Total events to emit (set high for \"infinite\" effect)\n",
    "MEAN_DELAY = 0.01              # Mean delay between events (seconds)\n",
    "STD_DEV_DELAY = 0.003          # Standard deviation of delay\n",
    "\n",
    "# Create list of user names and queries\n",
    "user_ids = [f\"user{str(i).zfill(5)}\" for i in range(NUM_USERS)]\n",
    "query_templates = [f\"query_{i}\" for i in range(NUM_QUERIES)]\n",
    "\n",
    "# Emit events\n",
    "for _ in range(TOTAL_EVENTS):\n",
    "    user = random.choice(user_ids)\n",
    "    query = random.choice(query_templates)\n",
    "    timestamp = int(time.time())\n",
    "\n",
    "    # Output format: <user>\\t<query>\\t<timestamp>\n",
    "    print(f\"{user}\\t{query}\\t{timestamp}\", flush=True)\n",
    "\n",
    "    # Control the delay between events\n",
    "    delay = max(0, random.gauss(MEAN_DELAY, STD_DEV_DELAY))\n",
    "    time.sleep(delay)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The updated read_stdin.py using hyperloglog is below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import hyperloglog\n",
    "\n",
    "# Initialize HyperLogLog with error rate ~1% (~1KB memory usage)\n",
    "hll = hyperloglog.HyperLogLog(0.01)\n",
    "\n",
    "# Track time\n",
    "start_time = time.time()\n",
    "interval = 1  # seconds between measurements\n",
    "next_tick = start_time + interval\n",
    "\n",
    "# Optional: for logging (to create a graph later if desired)\n",
    "log = []\n",
    "\n",
    "print(\"Elapsed(s)\\tEstimated Unique Users\", flush=True)\n",
    "\n",
    "try:\n",
    "    for line in sys.stdin:\n",
    "        try:\n",
    "            user, query, timestamp = line.strip().split('\\t')\n",
    "        except ValueError:\n",
    "            continue  # skip malformed lines\n",
    "\n",
    "        hll.add(user)\n",
    "\n",
    "        now = time.time()\n",
    "        if now >= next_tick:\n",
    "            elapsed = int(now - start_time)\n",
    "            est_count = len(hll)\n",
    "            print(f\"{elapsed}\\t{est_count}\", flush=True)\n",
    "            log.append((elapsed, est_count))\n",
    "            next_tick += interval\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Streaming stopped by user.\")\n",
    "    sys.exit(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Command: python3 click-feeder.py | python3 read_stdin.py > log.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ran the above command for 60 seconds and stored the output in log.txt:\n",
    "\n",
    "            Elapsed(s)\tEstimated Unique Users\n",
    "            1\t87\n",
    "            2\t169\n",
    "            3\t244\n",
    "            4\t320\n",
    "            5\t388\n",
    "            6\t451\n",
    "            7\t514\n",
    "            8\t579\n",
    "            9\t637\n",
    "            10\t687\n",
    "            11\t739\n",
    "            12\t796\n",
    "            13\t848\n",
    "            14\t896\n",
    "            15\t934\n",
    "            16\t977\n",
    "            17\t1016\n",
    "            18\t1054\n",
    "            19\t1096\n",
    "            20\t1131\n",
    "            21\t1177\n",
    "            22\t1212\n",
    "            23\t1239\n",
    "            24\t1257\n",
    "            25\t1293\n",
    "            26\t1320\n",
    "            27\t1352\n",
    "            28\t1379\n",
    "            29\t1398\n",
    "            30\t1429\n",
    "            31\t1454\n",
    "            32\t1470\n",
    "            33\t1494\n",
    "            34\t1514\n",
    "            35\t1534\n",
    "            36\t1551\n",
    "            37\t1564\n",
    "            38\t1589\n",
    "            39\t1608\n",
    "            40\t1627\n",
    "            41\t1637\n",
    "            42\t1654\n",
    "            43\t1666\n",
    "            44\t1678\n",
    "            45\t1688\n",
    "            46\t1704\n",
    "            47\t1715\n",
    "            48\t1721\n",
    "            49\t1732\n",
    "            50\t1748\n",
    "            51\t1759\n",
    "            52\t1771\n",
    "            53\t1778\n",
    "            54\t1787\n",
    "            55\t1794\n",
    "            56\t1802\n",
    "            57\t1806\n",
    "            58\t1809\n",
    "            59\t1815\n",
    "            60\t1820\n",
    "            Streaming stopped by user.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used the following code below to graph the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Updated data from the second run\n",
    "elapsed = list(range(1, 61))\n",
    "estimated_users = [\n",
    "    87, 169, 244, 320, 388, 451, 514, 579, 637, 687,\n",
    "    739, 796, 848, 896, 934, 977, 1016, 1054, 1096, 1131,\n",
    "    1177, 1212, 1239, 1257, 1293, 1320, 1352, 1379, 1398, 1429,\n",
    "    1454, 1470, 1494, 1514, 1534, 1551, 1564, 1589, 1608, 1627,\n",
    "    1637, 1654, 1666, 1678, 1688, 1704, 1715, 1721, 1732, 1748,\n",
    "    1759, 1771, 1778, 1787, 1794, 1802, 1806, 1809, 1815, 1820\n",
    "]\n",
    "\n",
    "# Plotting the estimated unique users over time\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(elapsed, estimated_users, marker='o', color='blue', linestyle='-')\n",
    "plt.title('Estimated Unique Users Over Time')\n",
    "plt.xlabel('Elapsed Time (seconds)')\n",
    "plt.ylabel('Estimated Unique Users')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![title](plot2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the above graph and output, the read_stdin starts to struggle keeping up with the click-feeder.py around 50 seconds. Prior to the 50 second mark, it is increasing linearly, but then plateaus. I used 10,000 simulated users to ensure it would not reach the correct number. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
