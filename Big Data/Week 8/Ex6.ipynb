{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6: Data Streams [100 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Querying Users [30 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code/psuedo code outline for the process. Since there are only 6000 messages, we will process all of them. We first receive all of them first and store them in a default dict. Then we classify them based on the frequency. The result should be a printed dictionary whose keys are the frequency: 1,2,etc. The values at each key should be the query itself, therefore categorizing them based on the number of times that query has been seen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the slide referenced in the question, we will sample 1/10th of the stream as a subset. The code creates 10 hash buckets based on the user and the timestamp. This is so that each user is partitioned fully into a single bucket. Therefore, we can answer definitively if a specific user had repeated queries in a given day because all that user's queries will be in the same bucket. This will avoid storing partial data for the users and giving incorrect information about repeated queries. This could also be scaled up by using 100 buckets insteadf (since 10 buckets is 1/10th of the data as the sampling constraint found in the context slide). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hashlib import md5\n",
    "from collections import defaultdict\n",
    "\n",
    "def hash_bucket(user, query, timestamp, num_buckets=10):\n",
    "    # Optional: convert timestamp to day granularity\n",
    "    day = timestamp.split(\"T\")[0]  # '2025-04-05T13:55:00' -> '2025-04-05'\n",
    "    key = f\"{user}:{day}\"  # sample by user per day\n",
    "    bucket = int(md5(key.encode()).hexdigest(), 16) % num_buckets\n",
    "    return bucket\n",
    "\n",
    "# Storage for 1/10th of stream\n",
    "stored_queries = defaultdict(lambda: defaultdict(int))  # user -> (query -> count)\n",
    "\n",
    "# Simulated stream\n",
    "for ev in dispatcher.launch():\n",
    "    user = ev[\"user\"]\n",
    "    query = ev[\"msg\"]\n",
    "    timestamp = ev[\"timestamp\"]\n",
    "\n",
    "    if hash_bucket(user, query, timestamp) == 0:  # Keep only bucket 0\n",
    "        stored_queries[(user, timestamp.split(\"T\")[0])][query] += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get information about the number of queries for a specific user, something along the lines of the following code would be run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (user, day), queries in stored_queries.items():\n",
    "    for query, count in queries.items():\n",
    "        print(f\"{user} ran query '{query}' {count} time(s) on {day}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bloom Filter [40 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [10 pts] Create a Bloom Filter, approximately 1000-2000 bits in size, for detecting bad words (i.e., AFINN of -4 or -5). It should be designed to run in Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import hashlib\n",
    "\n",
    "class BloomFilter:\n",
    "    def __init__(self, size=2048, num_hashes=3):\n",
    "        self.size = size\n",
    "        self.num_hashes = num_hashes\n",
    "        self.bit_vector = [0] * size\n",
    "\n",
    "    def _hashes(self, word):\n",
    "        hashes = []\n",
    "        for i in range(self.num_hashes):\n",
    "            hash_digest = hashlib.md5(f\"{word}_{i}\".encode()).hexdigest()\n",
    "            index = int(hash_digest, 16) % self.size\n",
    "            hashes.append(index)\n",
    "        return hashes\n",
    "\n",
    "    def add(self, word):\n",
    "        for idx in self._hashes(word):\n",
    "            self.bit_vector[idx] = 1\n",
    "\n",
    "    def check(self, word):\n",
    "        return all(self.bit_vector[idx] for idx in self._hashes(word))\n",
    "\n",
    "    def to_base64(self):\n",
    "        # Pack bits into bytes\n",
    "        bitstring = ''.join(map(str, self.bit_vector))\n",
    "        byte_array = bytearray(int(bitstring[i:i+8], 2) for i in range(0, len(bitstring), 8))\n",
    "        return base64.b64encode(byte_array).decode()\n",
    "\n",
    "    # === Step 1: Load bad words ===\n",
    "with open('bad_words.txt') as f:  # You said you have this list now\n",
    "    bad_words = [line.strip().lower() for line in f if line.strip()]\n",
    "\n",
    "# === Step 2: Build Bloom Filter ===\n",
    "bloom = BloomFilter()\n",
    "for word in bad_words:\n",
    "    bloom.add(word)\n",
    "\n",
    "# === Step 3: Save Base64 bit vector to HDFS or local file ===\n",
    "bitstring = bloom.to_base64()\n",
    "with open('bloomfilter_base64.txt', 'w') as f:\n",
    "    f.write(bitstring)\n",
    "\n",
    "print(\"Bloom filter created and written to bloomfilter_base64.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [10 pts] The bit vector should be placed in HDFS  as a Base64-encoded text file and loaded into Spark from HDFS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ran the program and then the following command to put it in HDFS:\n",
    "\n",
    "        hdfs dfs -put bloomfilter_base64.txt /user/brian_farrell/bloomfilter_base64.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [15 pts] Integrate the Bloom Filter into Spark such that every arriving sentence is examined and passed along if none of the words in the sentence are bad words. Sentences that do contain bad words should be suppressed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work in Progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "import base64\n",
    "\n",
    "# Initialize Spark\n",
    "sc = SparkContext(appName=\"DrunkSpeechFilter\")\n",
    "ssc = StreamingContext(sc, 1)\n",
    "\n",
    "# Load Bloom filter from HDFS\n",
    "b64_string = sc.textFile(\"hdfs:///user/yourname/bloomfilter.b64\").collect()[0]\n",
    "bit_array = list(bin(int.from_bytes(base64.b64decode(b64_string), 'big'))[2:].zfill(2048))\n",
    "bit_vector = list(map(int, bit_array))\n",
    "\n",
    "# Reconstruct BloomFilter logic\n",
    "import hashlib\n",
    "\n",
    "def get_hashes(word, size=2048, num_hashes=3):\n",
    "    return [int(hashlib.md5(f\"{word}_{i}\".encode()).hexdigest(), 16) % size for i in range(num_hashes)]\n",
    "\n",
    "def is_bad(word):\n",
    "    return all(bit_vector[idx] for idx in get_hashes(word.lower()))\n",
    "\n",
    "def contains_bad_words(sentence):\n",
    "    return any(is_bad(word) for word in sentence.split())\n",
    "\n",
    "# Simulated DStream (you could replace with socketTextStream for real input)\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "\n",
    "# Filter clean sentences\n",
    "clean_sentences = lines.filter(lambda sentence: not contains_bad_words(sentence))\n",
    "\n",
    "# Output\n",
    "clean_sentences.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Quiz6Q2Listener.py\"\"\"\n",
    "\n",
    "import sys, time\n",
    "\n",
    "import pyspark\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import BooleanType\n",
    "\n",
    "import math\n",
    "import mmh3\n",
    "from bitarray import bitarray\n",
    "import base64\n",
    "\n",
    "class BloomFilterAbridged:\n",
    "    \"\"\"An abridged version of the BloomFilter class I previously built.\n",
    "       Can reconstruct the bit array from a Base64-encoded bit string.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_items, false_positive_prob):\n",
    "\n",
    "        # Create a Bloom Filter approximately 1000-2000 bits in size\n",
    "        self.size = 1500\n",
    "\n",
    "        # Calculate optimal number of hash functions to use\n",
    "        self.hash_count = self.det_optimal_hash_count(self.size, num_items)\n",
    "\n",
    "        # Initialize bit array of all zeroes\n",
    "        self.bit_array = bitarray(self.size)\n",
    "        self.bit_array.setall(0)\n",
    "\n",
    "        self.false_positive_prob = false_positive_prob  # false positive probability\n",
    "\n",
    "    @classmethod\n",
    "    def from_base64(cls, base64_str, num_items, false_positive_prob):\n",
    "        \"\"\"\n",
    "        Create a bloom filter with a bit array that is encoded in a Base64 bit\n",
    "        string.\n",
    "\n",
    "        Parameters:\n",
    "            base64_str : str\n",
    "                The Base64-encoded bit string that the bit vector is encoded as.\n",
    "            num_items : int\n",
    "                Number of items expected to be stored in bloom filter\n",
    "            false_positive_prob : float\n",
    "                False Positive probability in decimal\n",
    "\n",
    "        Returns:\n",
    "            BloomFilter object with loaded bit array\n",
    "        \"\"\"\n",
    "\n",
    "        # Create empty bloom filter\n",
    "        bloom = cls(num_items, false_positive_prob)\n",
    "\n",
    "        # Decode Base64 to bytes\n",
    "        byte_array = base64.b64decode(base64_str.encode('utf-8'))\n",
    "\n",
    "        # Convert bytes to bitarray\n",
    "        bloom.bit_array = bitarray()\n",
    "        bloom.bit_array.frombytes(byte_array)\n",
    "        bloom.bit_array = bloom.bit_array[:1500]\n",
    "\n",
    "        # Return the filter\n",
    "        return bloom\n",
    "\n",
    "    def check(self, item):\n",
    "        \"\"\"Check for existence of an item in filter\n",
    "        \"\"\"\n",
    "        for i in range(self.hash_count):\n",
    "\n",
    "            # Calculate the same hash positions as done in add()\n",
    "            digest = mmh3.hash(item, i) % self.size\n",
    "\n",
    "            # If any bit is not set, the item definitely doesn't exist\n",
    "            if self.bit_array[digest] == False:\n",
    "                return False\n",
    "\n",
    "        # If all bits were set, the item might exist\n",
    "        return True\n",
    "\n",
    "    def det_optimal_hash_count(self, m, n):\n",
    "        \"\"\"Calculates the optimal number of hash functions using k = (m/n) * lg(2)\n",
    "\n",
    "        This formula is from: https://en.wikipedia.org/wiki/Bloom_filter\n",
    "\n",
    "            m : int\n",
    "                size of bit array\n",
    "            n : int\n",
    "                number of items expected to be stored in filter\n",
    "        \"\"\"\n",
    "        k = (m/n) * math.log(2)\n",
    "        return int(k)\n",
    "\n",
    "    def get_bit_array(self):\n",
    "        return self.bit_array\n",
    "\n",
    "\n",
    "def setLogLevel(sc, level):\n",
    "    from pyspark.sql import SparkSession\n",
    "    spark = SparkSession(sc)\n",
    "    spark.sparkContext.setLogLevel(level)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    if len(sys.argv) != 3:\n",
    "        print(\"Usage: bad_word_listener.py <hostname> <port>\", file=sys.stderr)\n",
    "        sys.exit(-1)\n",
    "\n",
    "    print ('Argv', sys.argv)\n",
    "\n",
    "    host = sys.argv[1]\n",
    "    port = int(sys.argv[2])\n",
    "    print ('host', type(host), host, 'port', type(port), port)\n",
    "\n",
    "    sc_bak = SparkContext.getOrCreate()\n",
    "    sc_bak.stop()\n",
    "\n",
    "    time.sleep(15)\n",
    "    print ('Ready to work!')\n",
    "\n",
    "    ctx = pyspark.SparkContext(appName = \"Netcat Wordcount\", master=\"local[*]\")\n",
    "    print ('Context', ctx)\n",
    "\n",
    "    spark = SparkSession(ctx).builder.getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "    setLogLevel(sc, \"WARN\")\n",
    "\n",
    "    print ('Session:', spark)\n",
    "    print ('SparkContext', sc)\n",
    "\n",
    "    # sc = SparkContext(conf=conf)\n",
    "\n",
    "    # Create DataFrame representing the stream of input lines from connection to host:port\n",
    "    lines = spark\\\n",
    "        .readStream\\\n",
    "        .format('socket')\\\n",
    "        .option('host', host)\\\n",
    "        .option('port', port)\\\n",
    "        .load()\n",
    "\n",
    "    # Read the Base64-encoded bit string from the Base64-encoded text file and\n",
    "    # broadcast it.\n",
    "    file_df = spark.read.text('/user/brian_farrell/bloomfilter_base64.txt')\n",
    "    base64_str = file_df.first()['value']\n",
    "    broadcast_filter = spark.sparkContext.broadcast(base64_str)\n",
    "\n",
    "    def check_line_for_bad_words(line):\n",
    "        \"\"\"Check if any of the words in a line are bad.\n",
    "        \"\"\"\n",
    "        # Create Bloom filter with the bit array encoded in the broadcasted\n",
    "        # Base64 string.\n",
    "        bf = BloomFilterAbridged.from_base64(broadcast_filter.value,\n",
    "                                            num_items=63,\n",
    "                                            false_positive_prob=0.05)\n",
    "\n",
    "        # Split the line into words\n",
    "        words = line.split()\n",
    "\n",
    "        # Return True if all words are good (no bad words found)\n",
    "        return all(not bf.check(word) for word in words)\n",
    "\n",
    "    # Register the user defined function\n",
    "    is_clean_line = udf(check_line_for_bad_words, BooleanType())\n",
    "\n",
    "    # Filter lines to only keep those without bad words\n",
    "    clean_lines = lines.filter(is_clean_line(lines.value))\n",
    "\n",
    "    # Start running the query that prints the clean lines to the console\n",
    "    query = clean_lines\\\n",
    "        .writeStream\\\n",
    "        .outputMode('append')\\\n",
    "        .format('console')\\\n",
    "        .option(\"truncate\", False)\\\n",
    "        .option(\"numRows\", 100000)\\\n",
    "        .option(\"separator\", \"\")\\\n",
    "        .option(\"header\", False)\\\n",
    "        .start()\n",
    "\n",
    "    query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Demo](https://tufts.zoom.us/rec/share/PHLhSyKuWtj2wErp890cV_zZrr_phiRXFMEfZZcFedOjXkDBw3JCotma7a9h79aa.nn_9dtpICC560Wg_?startTime=1744123581000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Counting Unique Users [30 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify read_stdin.py to implement the HyperLogLog algorithm. Increase the number of senders and decrease the (μ,σ) of the delay between queries until the receiver can no longer keep up! Draw a graph of the estimated number of users as a function of elapsed time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The modified click-feeder.py program is below that significantly increases the users used to cause the std-in to not be able to keep up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Configuration: High number of users, low delay\n",
    "NUM_USERS = 10000               # Total users to simulate\n",
    "NUM_QUERIES = 100              # Unique queries each user might send\n",
    "TOTAL_EVENTS = 10000000         # Total events to emit (set high for \"infinite\" effect)\n",
    "MEAN_DELAY = 0.01              # Mean delay between events (seconds)\n",
    "STD_DEV_DELAY = 0.003          # Standard deviation of delay\n",
    "\n",
    "# Create list of user names and queries\n",
    "user_ids = [f\"user{str(i).zfill(5)}\" for i in range(NUM_USERS)]\n",
    "query_templates = [f\"query_{i}\" for i in range(NUM_QUERIES)]\n",
    "\n",
    "# Emit events\n",
    "for _ in range(TOTAL_EVENTS):\n",
    "    user = random.choice(user_ids)\n",
    "    query = random.choice(query_templates)\n",
    "    timestamp = int(time.time())\n",
    "\n",
    "    # Output format: <user>\\t<query>\\t<timestamp>\n",
    "    print(f\"{user}\\t{query}\\t{timestamp}\", flush=True)\n",
    "\n",
    "    # Control the delay between events\n",
    "    delay = max(0, random.gauss(MEAN_DELAY, STD_DEV_DELAY))\n",
    "    time.sleep(delay)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The updated read_stdin.py using hyperloglog is below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import hyperloglog\n",
    "\n",
    "# Initialize HyperLogLog with error rate ~1% (~1KB memory usage)\n",
    "hll = hyperloglog.HyperLogLog(0.01)\n",
    "\n",
    "# Track time\n",
    "start_time = time.time()\n",
    "interval = 1  # seconds between measurements\n",
    "next_tick = start_time + interval\n",
    "\n",
    "# Optional: for logging (to create a graph later if desired)\n",
    "log = []\n",
    "\n",
    "print(\"Elapsed(s)\\tEstimated Unique Users\", flush=True)\n",
    "\n",
    "try:\n",
    "    for line in sys.stdin:\n",
    "        try:\n",
    "            user, query, timestamp = line.strip().split('\\t')\n",
    "        except ValueError:\n",
    "            continue  # skip malformed lines\n",
    "\n",
    "        hll.add(user)\n",
    "\n",
    "        now = time.time()\n",
    "        if now >= next_tick:\n",
    "            elapsed = int(now - start_time)\n",
    "            est_count = len(hll)\n",
    "            print(f\"{elapsed}\\t{est_count}\", flush=True)\n",
    "            log.append((elapsed, est_count))\n",
    "            next_tick += interval\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Streaming stopped by user.\")\n",
    "    sys.exit(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Command: python3 click-feeder.py | python3 read_stdin.py > log.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ran the above command for 60 seconds and stored the output in log.txt:\n",
    "\n",
    "            Elapsed(s)\tEstimated Unique Users\n",
    "            1\t87\n",
    "            2\t169\n",
    "            3\t244\n",
    "            4\t320\n",
    "            5\t388\n",
    "            6\t451\n",
    "            7\t514\n",
    "            8\t579\n",
    "            9\t637\n",
    "            10\t687\n",
    "            11\t739\n",
    "            12\t796\n",
    "            13\t848\n",
    "            14\t896\n",
    "            15\t934\n",
    "            16\t977\n",
    "            17\t1016\n",
    "            18\t1054\n",
    "            19\t1096\n",
    "            20\t1131\n",
    "            21\t1177\n",
    "            22\t1212\n",
    "            23\t1239\n",
    "            24\t1257\n",
    "            25\t1293\n",
    "            26\t1320\n",
    "            27\t1352\n",
    "            28\t1379\n",
    "            29\t1398\n",
    "            30\t1429\n",
    "            31\t1454\n",
    "            32\t1470\n",
    "            33\t1494\n",
    "            34\t1514\n",
    "            35\t1534\n",
    "            36\t1551\n",
    "            37\t1564\n",
    "            38\t1589\n",
    "            39\t1608\n",
    "            40\t1627\n",
    "            41\t1637\n",
    "            42\t1654\n",
    "            43\t1666\n",
    "            44\t1678\n",
    "            45\t1688\n",
    "            46\t1704\n",
    "            47\t1715\n",
    "            48\t1721\n",
    "            49\t1732\n",
    "            50\t1748\n",
    "            51\t1759\n",
    "            52\t1771\n",
    "            53\t1778\n",
    "            54\t1787\n",
    "            55\t1794\n",
    "            56\t1802\n",
    "            57\t1806\n",
    "            58\t1809\n",
    "            59\t1815\n",
    "            60\t1820\n",
    "            Streaming stopped by user.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used the following code below to graph the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Updated data from the second run\n",
    "elapsed = list(range(1, 61))\n",
    "estimated_users = [\n",
    "    87, 169, 244, 320, 388, 451, 514, 579, 637, 687,\n",
    "    739, 796, 848, 896, 934, 977, 1016, 1054, 1096, 1131,\n",
    "    1177, 1212, 1239, 1257, 1293, 1320, 1352, 1379, 1398, 1429,\n",
    "    1454, 1470, 1494, 1514, 1534, 1551, 1564, 1589, 1608, 1627,\n",
    "    1637, 1654, 1666, 1678, 1688, 1704, 1715, 1721, 1732, 1748,\n",
    "    1759, 1771, 1778, 1787, 1794, 1802, 1806, 1809, 1815, 1820\n",
    "]\n",
    "\n",
    "# Plotting the estimated unique users over time\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(elapsed, estimated_users, marker='o', color='blue', linestyle='-')\n",
    "plt.title('Estimated Unique Users Over Time')\n",
    "plt.xlabel('Elapsed Time (seconds)')\n",
    "plt.ylabel('Estimated Unique Users')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![title](plot2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the above graph and output, the read_stdin starts to struggle keeping up with the click-feeder.py around 50 seconds. Prior to the 50 second mark, it is increasing linearly, but then plateaus. I used 10,000 simulated users to ensure it would not reach the correct number. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
