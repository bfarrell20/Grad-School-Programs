{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6: Data Streams [100 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Querying Users [30 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "query_counts = defaultdict(int)  # key = query string, value = count\n",
    "\n",
    "# Simulate receiving events from click-feeder.py\n",
    "for ev in dispatcher.launch():\n",
    "    query = ev['msg']  # assuming 'msg' holds the query string\n",
    "    query_counts[query] += 1\n",
    "\n",
    "    max_evs -= 1\n",
    "    if max_evs == 0:\n",
    "        break\n",
    "\n",
    "# Now classify each query by how many times it was seen\n",
    "frequency_buckets = defaultdict(list)  # key = frequency, value = list of queries\n",
    "\n",
    "for query, count in query_counts.items():\n",
    "    frequency_buckets[count].append(query)\n",
    "\n",
    "# Print summary\n",
    "for freq in sorted(frequency_buckets.keys()):\n",
    "    print(f\"Queries seen {freq} time(s): {len(frequency_buckets[freq])} queries\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strategy: The simplest and most accurate method here would be to process every event during the 12-minute window, since it's feasible (only 6000 events, and memory is not a constraint). We can then tally query frequencies using a dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bloom Filter [40 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [10 pts] Create a Bloom Filter, approximately 1000-2000 bits in size, for detecting bad words (i.e., AFINN of -4 or -5). It should be designed to run in Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import hashlib\n",
    "\n",
    "class BloomFilter:\n",
    "    def __init__(self, size=2048, num_hashes=3):\n",
    "        self.size = size\n",
    "        self.num_hashes = num_hashes\n",
    "        self.bit_vector = [0] * size\n",
    "\n",
    "    def _hashes(self, word):\n",
    "        hashes = []\n",
    "        for i in range(self.num_hashes):\n",
    "            hash_digest = hashlib.md5(f\"{word}_{i}\".encode()).hexdigest()\n",
    "            index = int(hash_digest, 16) % self.size\n",
    "            hashes.append(index)\n",
    "        return hashes\n",
    "\n",
    "    def add(self, word):\n",
    "        for idx in self._hashes(word):\n",
    "            self.bit_vector[idx] = 1\n",
    "\n",
    "    def check(self, word):\n",
    "        return all(self.bit_vector[idx] for idx in self._hashes(word))\n",
    "\n",
    "    def to_base64(self):\n",
    "        # Pack bits into bytes\n",
    "        bitstring = ''.join(map(str, self.bit_vector))\n",
    "        byte_array = bytearray(int(bitstring[i:i+8], 2) for i in range(0, len(bitstring), 8))\n",
    "        return base64.b64encode(byte_array).decode()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [10 pts] The bit vector should be placed in HDFS  as a Base64-encoded text file and loaded into Spark from HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_words = [...]  # List of ~65 words with AFINN scores -4 or -5\n",
    "\n",
    "bf = BloomFilter(size=2048, num_hashes=3)\n",
    "for word in bad_words:\n",
    "    bf.add(word.lower())\n",
    "\n",
    "with open(\"bloomfilter.b64\", \"w\") as f:\n",
    "    f.write(bf.to_base64())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ran the following command to put it in HDFS:\n",
    "\n",
    "        hdfs dfs -put bloomfilter.b64 /user/brianfarrell/bloomfilter.b64\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [15 pts] Integrate the Bloom Filter into Spark such that every arriving sentence is examined and passed along if none of the words in the sentence are bad words. Sentences that do contain bad words should be suppressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "import base64\n",
    "\n",
    "# Initialize Spark\n",
    "sc = SparkContext(appName=\"DrunkSpeechFilter\")\n",
    "ssc = StreamingContext(sc, 1)\n",
    "\n",
    "# Load Bloom filter from HDFS\n",
    "b64_string = sc.textFile(\"hdfs:///user/yourname/bloomfilter.b64\").collect()[0]\n",
    "bit_array = list(bin(int.from_bytes(base64.b64decode(b64_string), 'big'))[2:].zfill(2048))\n",
    "bit_vector = list(map(int, bit_array))\n",
    "\n",
    "# Reconstruct BloomFilter logic\n",
    "import hashlib\n",
    "\n",
    "def get_hashes(word, size=2048, num_hashes=3):\n",
    "    return [int(hashlib.md5(f\"{word}_{i}\".encode()).hexdigest(), 16) % size for i in range(num_hashes)]\n",
    "\n",
    "def is_bad(word):\n",
    "    return all(bit_vector[idx] for idx in get_hashes(word.lower()))\n",
    "\n",
    "def contains_bad_words(sentence):\n",
    "    return any(is_bad(word) for word in sentence.split())\n",
    "\n",
    "# Simulated DStream (you could replace with socketTextStream for real input)\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "\n",
    "# Filter clean sentences\n",
    "clean_sentences = lines.filter(lambda sentence: not contains_bad_words(sentence))\n",
    "\n",
    "# Output\n",
    "clean_sentences.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Counting Unique Users [30 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify read_stdin.py to implement the HyperLogLog algorithm. Increase the number of senders and decrease the (μ,σ) of the delay between queries until the receiver can no longer keep up! Draw a graph of the estimated number of users as a function of elapsed time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import hyperloglog\n",
    "\n",
    "# Initialize HyperLogLog with error rate ~1% (~1KB memory usage)\n",
    "hll = hyperloglog.HyperLogLog(0.01)\n",
    "\n",
    "# Track time\n",
    "start_time = time.time()\n",
    "interval = 1  # seconds between measurements\n",
    "next_tick = start_time + interval\n",
    "\n",
    "# Optional: for logging (to create a graph later if desired)\n",
    "log = []\n",
    "\n",
    "print(\"Elapsed(s)\\tEstimated Unique Users\", flush=True)\n",
    "\n",
    "try:\n",
    "    for line in sys.stdin:\n",
    "        try:\n",
    "            user, query, timestamp = line.strip().split('\\t')\n",
    "        except ValueError:\n",
    "            continue  # skip malformed lines\n",
    "\n",
    "        hll.add(user)\n",
    "\n",
    "        now = time.time()\n",
    "        if now >= next_tick:\n",
    "            elapsed = int(now - start_time)\n",
    "            est_count = len(hll)\n",
    "            print(f\"{elapsed}\\t{est_count}\", flush=True)\n",
    "            log.append((elapsed, est_count))\n",
    "            next_tick += interval\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Streaming stopped by user.\")\n",
    "    sys.exit(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Command: python3 click-feeder.py | python3 read_stdin.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
