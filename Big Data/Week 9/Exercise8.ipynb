{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b9ae9e1",
   "metadata": {},
   "source": [
    "# Exercise 8: Finding Similar Items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc318842",
   "metadata": {},
   "source": [
    "### 1. [10 points] Since the reviews are relatively short pieces of text, we shall utilize a shingling strategy outlined in section 3.2.4 Shingles Built from Words and Example 3.5 on the same page. Write a shingling program that takes a review text and returns a set of shingles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4c6cefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered reviews: 19110\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "MIN_REVIEWS_PER_ITEM = 25\n",
    "df = pd.read_csv('reviews.csv')\n",
    "\n",
    "# Drop rows with no review text\n",
    "df = df.dropna(subset=['Review Text'])\n",
    "\n",
    "# Filter Clothing IDs with at least MIN_REVIEWS_PER_ITEM reviews\n",
    "filtered_ids = df['Clothing ID'].value_counts()\n",
    "valid_ids = filtered_ids[filtered_ids >= MIN_REVIEWS_PER_ITEM].index\n",
    "df_filtered = df[df['Clothing ID'].isin(valid_ids)].copy()\n",
    "\n",
    "print(f\"Filtered reviews: {len(df_filtered)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf7a7ca",
   "metadata": {},
   "source": [
    "The below code takes in the reviews that meet the 25 word minimum criteria and returns 5-word shingles. An example is shown for how it works for one of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebff86c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"love this dress! it's sooo\", \"store, and i'm glad i\", 'the knee. would definitely be', 'happened to find it in', 'did bc i never would', \"a store, and i'm glad\", 'below the knee. would definitely', 'hits just a little below', 'be a true midi on', 'on me- hits just a', 'and am 5\\'8\". i love', 'a petite and am 5\\'8\".', 'the length on me- hits', 'knee. would definitely be a', 'i never would have ordered', 'to find it in a', \"in a store, and i'm\", 'am 5\\'8\". i love the', 'petite. i bought a petite', 'bc i never would have', 'true midi on someone who', 'i happened to find it', 'i bought a petite and', 'a true midi on someone', 'midi on someone who is', 'i love the length on', 'definitely be a true midi', \"it's sooo pretty. i happened\", \"and i'm glad i did\", 'bought a petite and am', 'love the length on me-', 'petite and am 5\\'8\". i', \"bc it's petite. i bought\", \"it online bc it's petite.\", \"online bc it's petite. i\", \"it's petite. i bought a\", \"i'm glad i did bc\", 'me- hits just a little', 'never would have ordered it', '5\\'8\". i love the length', 'have ordered it online bc', 'it in a store, and', \"dress! it's sooo pretty. i\", 'someone who is truly petite.', \"this dress! it's sooo pretty.\", 'length on me- hits just', 'pretty. i happened to find', 'would have ordered it online', \"ordered it online bc it's\", 'a little below the knee.', 'would definitely be a true', 'sooo pretty. i happened to', 'find it in a store,', 'on someone who is truly', 'glad i did bc i', 'little below the knee. would', 'i did bc i never', 'just a little below the'}\n"
     ]
    }
   ],
   "source": [
    "def shingle_review(review, k=5):\n",
    "    \"\"\"Returns a set of k-word shingles from a review text.\"\"\"\n",
    "    words = review.lower().split()\n",
    "    shingles = set()\n",
    "    for i in range(len(words) - k + 1):\n",
    "        shingle = ' '.join(words[i:i + k])\n",
    "        shingles.add(shingle)\n",
    "    return shingles\n",
    "\n",
    "# Example output of one review's 5-word shingles\n",
    "example_review = df_filtered['Review Text'].iloc[0]\n",
    "print(shingle_review(example_review))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dd9ca6",
   "metadata": {},
   "source": [
    "### 2. [15 points] Write a minhashing program to calculate minhash values for each review text, using a provided set of 200 hash functions, (each function expressed as a lambda). Time the creation of minhash values for processing all review texts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d983ecc",
   "metadata": {},
   "source": [
    "The below code minhashes values for each review text using s set of 200 hash function. A very large prime number was found that is larger than next_prime. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14fecd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import hashlib\n",
    "import time\n",
    "\n",
    "NUM_HASHES = 200\n",
    "MAX_SHINGLE_ID = 2**32 - 1\n",
    "next_prime = 4294967311  # a large prime > MAX_SHINGLE_ID\n",
    "\n",
    "# Generate hash functions\n",
    "hash_functions = []\n",
    "for _ in range(NUM_HASHES):\n",
    "    a = random.randint(1, next_prime - 1)\n",
    "    b = random.randint(0, next_prime - 1)\n",
    "    hash_functions.append(lambda x, a=a, b=b: (a * x + b) % next_prime)\n",
    "\n",
    "def hash_shingle(shingle):\n",
    "    return int(hashlib.md5(shingle.encode('utf-8')).hexdigest(), 16) % MAX_SHINGLE_ID\n",
    "\n",
    "def compute_minhash_signature(shingle_set, hash_funcs):\n",
    "    sig = []\n",
    "    for h in hash_funcs:\n",
    "        min_val = float('inf')\n",
    "        for shingle in shingle_set:\n",
    "            x = hash_shingle(shingle)\n",
    "            hx = h(x)\n",
    "            if hx < min_val:\n",
    "                min_val = hx\n",
    "        sig.append(min_val)\n",
    "    return sig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95303ffb",
   "metadata": {},
   "source": [
    "The code below is used to time the shingling of every review in the csv file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47ca3336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to compute minhash signatures: 193.40 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "signatures = []\n",
    "\n",
    "for review in df_filtered['Review Text']:\n",
    "    shingles = shingle_review(review)\n",
    "    sig = compute_minhash_signature(shingles, hash_functions)\n",
    "    signatures.append(sig)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken to compute minhash signatures: {end - start:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f228f5f",
   "metadata": {},
   "source": [
    "When being run on my laptop, the time taken to complete the signatureds was 607.66 seconds, or around 10 minutes and 13 seconds. When being run on my pc (the output above), the time taken was 193.40 seconds. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a302a427",
   "metadata": {},
   "source": [
    "### 3. [10 points] An alternative to calculating minhashes from 200 hash functions is to use one hash function and 199 cheap hash functions derived from it, as outlined in this SO post. Time the creation of minhash values for processing all review texts using this method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45ff3d9",
   "metadata": {},
   "source": [
    "The below code shows this alternative method that uses one hash function and 199 cheap hash functions, or XOR-transformed hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a3ed0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import hashlib\n",
    "import time\n",
    "\n",
    "NUM_HASHES = 200\n",
    "MAX_SHINGLE_ID = 2**32 - 1\n",
    "\n",
    "# Generate 199 random 32-bit integers for XOR masking\n",
    "xor_masks = np.random.randint(0, MAX_SHINGLE_ID, size=NUM_HASHES - 1, dtype=np.uint32)\n",
    "\n",
    "def fast_hash_200(shingle):\n",
    "    \"\"\"Generate 200 hash values using 1 good hash and 199 cheap XORs.\"\"\"\n",
    "    # Base hash: 32-bit from MD5\n",
    "    base_hash = int(hashlib.md5(shingle.encode()).hexdigest(), 16) & 0xFFFFFFFF\n",
    "    hashes = [base_hash]\n",
    "    for mask in xor_masks:\n",
    "        hashes.append(base_hash ^ mask)\n",
    "    return hashes\n",
    "\n",
    "def compute_minhash_signature_fast(shingle_set):\n",
    "    sig = [float('inf')] * NUM_HASHES\n",
    "    for shingle in shingle_set:\n",
    "        h_values = fast_hash_200(shingle)\n",
    "        for i in range(NUM_HASHES):\n",
    "            if h_values[i] < sig[i]:\n",
    "                sig[i] = h_values[i]\n",
    "    return sig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70dbb59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time using XOR-based minhashing: 25.49 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "signatures_fast = []\n",
    "\n",
    "for review in df_filtered['Review Text']:\n",
    "    shingles = shingle_review(review)\n",
    "    sig = compute_minhash_signature_fast(shingles)\n",
    "    signatures_fast.append(sig)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time using XOR-based minhashing: {end - start:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598333e3",
   "metadata": {},
   "source": [
    "When using my laptop, the code above runs the faster signature method and times it. This method took 497.42 seconds, or 8 minutes and 29 seconds. According to the above output, this method is faster than using 200 hashes. The time when being run on my pc (the above output), the time is 25.49 seconds. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9975a113",
   "metadata": {},
   "source": [
    "### 4. [10 points] Write a program to allocate the reviews among 216 buckets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31a3d61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_BUCKETS = 2**16  # 65536\n",
    "\n",
    "def signature_to_bucket(signature):\n",
    "    \"\"\"Hash a signature into one of 65,536 buckets.\"\"\"\n",
    "    sig_str = ''.join(map(str, signature))\n",
    "    hash_val = int(hashlib.md5(sig_str.encode()).hexdigest(), 16)\n",
    "    return hash_val % NUM_BUCKETS\n",
    "\n",
    "# Reassign all reviews to one of 65536 buckets\n",
    "buckets = [signature_to_bucket(sig) for sig in signatures_fast]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cae562",
   "metadata": {},
   "source": [
    "### 5. [10 points] How many non-empty buckets did you get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63f6cae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non-empty buckets (out of 65536): 16537\n"
     ]
    }
   ],
   "source": [
    "used_buckets = set(buckets)\n",
    "all_buckets = set(range(NUM_BUCKETS))\n",
    "\n",
    "empty_buckets = all_buckets - used_buckets\n",
    "non_empty = len(all_buckets - empty_buckets)\n",
    "num_empty_buckets = len(empty_buckets)\n",
    "\n",
    "print(f\"Number of non-empty buckets (out of 65536): {non_empty}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d020e8d4",
   "metadata": {},
   "source": [
    "According to the above output, there are 16,537 non-empty buckets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1561c4",
   "metadata": {},
   "source": [
    "### 6. [10 points] Find actual similar reviews: Compute the exact Jaccard similarity for all pairs of reviews and output the pairs of reviews that have a similarity at least 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3eac1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11 pairs with Jaccard similarity >= 0.5\n",
      "Review 1538 and Review 10295 -> Jaccard: 1.000\n",
      "Review 2936 and Review 11684 -> Jaccard: 0.611\n",
      "Review 3194 and Review 14506 -> Jaccard: 0.737\n",
      "Review 3923 and Review 15080 -> Jaccard: 0.648\n",
      "Review 5386 and Review 18642 -> Jaccard: 0.979\n"
     ]
    }
   ],
   "source": [
    "# Generate all shingle sets once\n",
    "shingle_sets = [shingle_review(review) for review in df_filtered['Review Text']]\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "similar_pairs_jaccard = []\n",
    "\n",
    "for i, j in combinations(range(len(shingle_sets)), 2):\n",
    "    s1, s2 = shingle_sets[i], shingle_sets[j]\n",
    "    intersection = len(s1.intersection(s2))\n",
    "    union = len(s1.union(s2))\n",
    "    if union == 0:\n",
    "        continue\n",
    "    similarity = intersection / union\n",
    "    if similarity >= 0.5:\n",
    "        similar_pairs_jaccard.append((i, j, similarity))\n",
    "\n",
    "print(f\"Found {len(similar_pairs_jaccard)} pairs with Jaccard similarity >= 0.5\")\n",
    "\n",
    "for i, j, sim in similar_pairs_jaccard[:5]:\n",
    "    print(f\"Review {i} and Review {j} -> Jaccard: {sim:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "837d399d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1538 and Review 10295 -> Jaccard: 1.000\n",
      "Review 2936 and Review 11684 -> Jaccard: 0.611\n",
      "Review 3194 and Review 14506 -> Jaccard: 0.737\n",
      "Review 3923 and Review 15080 -> Jaccard: 0.648\n",
      "Review 5386 and Review 18642 -> Jaccard: 0.979\n",
      "Review 7741 and Review 17843 -> Jaccard: 1.000\n",
      "Review 7835 and Review 11863 -> Jaccard: 0.647\n",
      "Review 8330 and Review 11757 -> Jaccard: 1.000\n",
      "Review 10200 and Review 17229 -> Jaccard: 0.577\n",
      "Review 11249 and Review 17839 -> Jaccard: 0.842\n",
      "Review 13923 and Review 17506 -> Jaccard: 1.000\n"
     ]
    }
   ],
   "source": [
    "for i, j, sim in similar_pairs_jaccard:\n",
    "    print(f\"Review {i} and Review {j} -> Jaccard: {sim:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3f258d",
   "metadata": {},
   "source": [
    "As you can see from the above output, there were 11 exact Jaccard similarities found with this method. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adee3285",
   "metadata": {},
   "source": [
    "### 7. [15 points] Find similar reviews using min-hashing: Compute signatures for all reviews using 200 hash functions. Output the pairs that have a similarity of at least 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0459ac19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 418 pairs with MinHash similarity >= 0.5\n"
     ]
    }
   ],
   "source": [
    "similar_pairs_minhash = []\n",
    "\n",
    "for i, j in combinations(range(len(signatures)), 2):\n",
    "    sig1, sig2 = signatures[i], signatures[j]\n",
    "    matches = sum(1 for a, b in zip(sig1, sig2) if a == b)\n",
    "    similarity = matches / NUM_HASHES\n",
    "    if similarity >= 0.5:\n",
    "        similar_pairs_minhash.append((i, j, similarity))\n",
    "\n",
    "print(f\"Found {len(similar_pairs_minhash)} pairs with MinHash similarity >= 0.5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e6aba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j, sim in similar_pairs_minhash:\n",
    "    print(f\"Review {i} and Review {j} -> MinHash similarity: {sim:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2698232",
   "metadata": {},
   "source": [
    "From the above output, there were 418 pairs with a MinHash similarity above 0.5. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f10b382",
   "metadata": {},
   "source": [
    "### 8. [10 points] Compare min-hashing to truth: Use your results from the previous two problems to determine the following\n",
    "        The count of true positives: actual similar reviews identified by min-hashing as similar.\n",
    "        The count of true negatives: actual dissimilar reviews identified by min-hashing as dissimilar.\n",
    "        The count of false negatives: actual similar reviews identified by min-hashing as dissimilar.\n",
    "        The count of false positives: actual dissimilar reviews identified by min-hashing as similar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12daae9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_similar = set((i, j) for i, j, _ in similar_pairs_jaccard)\n",
    "approx_similar = set((i, j) for i, j, _ in similar_pairs_minhash)\n",
    "\n",
    "# Ensure all pair indices are in the same order\n",
    "all_pairs = set(combinations(range(len(shingle_sets)), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5e1bec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 11\n",
      "False Positives: 407\n",
      "False Negatives: 0\n",
      "True Negatives: 182586077\n"
     ]
    }
   ],
   "source": [
    "TP = len(actual_similar & approx_similar)\n",
    "FP = len(approx_similar - actual_similar)\n",
    "FN = len(actual_similar - approx_similar)\n",
    "TN = len(all_pairs - (actual_similar | approx_similar))\n",
    "\n",
    "print(f\"True Positives: {TP}\")\n",
    "print(f\"False Positives: {FP}\")\n",
    "print(f\"False Negatives: {FN}\")\n",
    "print(f\"True Negatives: {TN}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c7ec97",
   "metadata": {},
   "source": [
    "As you can see from above, there are only the 11 true positives from the earlier actual similarites. There were 407 false positives and zero false negatives. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8e37c7",
   "metadata": {},
   "source": [
    "### 9. [5 points] Compare min-hashing to truth using the cheap hash functions outlined in question 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e89c99dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_pairs_minhash_fast = []\n",
    "\n",
    "for i, j in combinations(range(len(signatures_fast)), 2):\n",
    "    sig1, sig2 = signatures_fast[i], signatures_fast[j]\n",
    "    matches = sum(1 for a, b in zip(sig1, sig2) if a == b)\n",
    "    similarity = matches / NUM_HASHES\n",
    "    if similarity >= 0.5:\n",
    "        similar_pairs_minhash_fast.append((i, j, similarity))\n",
    "\n",
    "approx_similar_fast = set((i, j) for i, j, _ in similar_pairs_minhash_fast)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "58ba78cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast MinHashing:\n",
      "  True Positives: 11\n",
      "  False Positives: 410\n",
      "  False Negatives: 0\n",
      "  True Negatives: 182586074\n"
     ]
    }
   ],
   "source": [
    "TP_fast = len(actual_similar & approx_similar_fast)\n",
    "FP_fast = len(approx_similar_fast - actual_similar)\n",
    "FN_fast = len(actual_similar - approx_similar_fast)\n",
    "TN_fast = len(all_pairs - (actual_similar | approx_similar_fast))\n",
    "\n",
    "print(f\"Fast MinHashing:\")\n",
    "print(f\"  True Positives: {TP_fast}\")\n",
    "print(f\"  False Positives: {FP_fast}\")\n",
    "print(f\"  False Negatives: {FN_fast}\")\n",
    "print(f\"  True Negatives: {TN_fast}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3383e193",
   "metadata": {},
   "source": [
    "This method resulted in 3 more false positives with 410. However, this method is also much faster, so the drop in accuracy is likely worth the increase in performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c239b4",
   "metadata": {},
   "source": [
    "### 10. [5 points] Comment on the performance difference (TP, TN, FN, FP) between questions 8 and 9. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "50a71c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Comparison ===\n",
      "Standard MinHashing   -> TP: 11, FP: 407, FN: 0, TN: 182586077\n",
      "Cheap Hash Functions  -> TP: 11, FP: 410, FN: 0, TN: 182586074\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Comparison ===\")\n",
    "print(f\"Standard MinHashing   -> TP: {TP}, FP: {FP}, FN: {FN}, TN: {TN}\")\n",
    "print(f\"Cheap Hash Functions  -> TP: {TP_fast}, FP: {FP_fast}, FN: {FN_fast}, TN: {TN_fast}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf57bb89",
   "metadata": {},
   "source": [
    "The cheap method has 3 more false positives, with the same amount of false negatives. With the cheap method being much faster, it is worth the small drop in accuracy for the increase in speed. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
