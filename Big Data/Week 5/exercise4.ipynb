{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVvdEWZM3j4M"
      },
      "source": [
        "# Exercise 4: Hadoop for Fun and Profit (125 pts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQ6tbMPF3j4O"
      },
      "source": [
        "## 1. Functional Programming [20 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFkA6_eN3j4O"
      },
      "source": [
        "### 1. Create a function 十 that takes an arbitrary number of arguments, and adds them all. Also create a function 一 that subtracts the sum of arguments 2 through n from the first argument.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yoaW-m2d3j4O",
        "outputId": "ee093fdd-b0ef-4117-b19f-bc6c222be4cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "def 十(*args):\n",
        "    return sum(args)\n",
        "\n",
        "def 一(a, *args):\n",
        "    return a - sum(args)\n",
        "\n",
        "print(十(1, 2, 3))  # Output: 6\n",
        "print(一(5, 1, 2))  # Output: 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27vjDAcX3j4Q"
      },
      "source": [
        "### 2. Also create a function 乛 that performs right-associative subtraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrgMYFOd3j4Q",
        "outputId": "3898ee03-d0ea-4f03-925b-a6b86791efcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6\n",
            "3\n"
          ]
        }
      ],
      "source": [
        "def 乛(*args):\n",
        "    result = args[-1]  # Start from the last element\n",
        "    for num in reversed(args[:-1]):  # Iterate in reverse order, skipping the last element\n",
        "        result = num - result  # Right-associative subtraction\n",
        "    return result\n",
        "\n",
        "print(乛(5, 1, 2))      # Output: 6\n",
        "print(乛(5, 1, 2, 3))   # Output: 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkng71ZY3j4Q"
      },
      "source": [
        "### 3. Create a function ϵ that can flatten a tree."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1V3dH9ZC3j4R",
        "outputId": "96125291-f049-48c0-85ae-5a6b3c1da691"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
          ]
        }
      ],
      "source": [
        "def ϵ(tree):\n",
        "    result = []\n",
        "    for item in tree:\n",
        "        if isinstance(item, list):\n",
        "            result.extend(ϵ(item))  # Recursively flatten nested lists\n",
        "        else:\n",
        "            result.append(item)  # Append non-list elements directly\n",
        "    return result\n",
        "\n",
        "print(ϵ([1, [2, [3, 4], [5, 6], 7], 8, [9, 10]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIZsrP3x3j4R"
      },
      "source": [
        "#### 4. Create a function γ that takes a function and a sequence and groups the elements of the sequence based on the result of the given function. In the example below, len returns the length of a sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwyJ2sGy3j4R",
        "outputId": "e26513e6-8c8b-4424-ed6c-4528197f8162"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{2: ['hi', 'me'], 3: ['dog', 'bad'], 4: ['good']}\n"
          ]
        }
      ],
      "source": [
        "def γ(f, seq):\n",
        "    result = {}\n",
        "    for item in seq:\n",
        "        key = f(item)  # Apply function to get the key\n",
        "        if key not in result:\n",
        "            result[key] = []  # Initialize list if key doesn't exist\n",
        "        result[key].append(item)  # Append item to the corresponding list\n",
        "    return result\n",
        "\n",
        "print(γ(len, [\"hi\", \"dog\", \"me\", \"bad\", \"good\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34iGRJfQ3j4S"
      },
      "source": [
        "## 2. Confirming Hadoop Installation [20 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8Rubx2q3j4S"
      },
      "source": [
        "#### 1. [2 points] Acquire the cluster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFEwcPDg3j4S"
      },
      "source": [
        "#### 2. [2 points] Load the five books into the master, move the data into HDFS."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKwct_Xh3j4S"
      },
      "source": [
        "Output: Found 5 items\n",
        "\n",
        "    -rw-r--r--   1 brian_farrell hadoop     179903 2025-02-19 17:27 /user/brian_farrell/five-books/a_tangled_tale.txt\n",
        "    -rw-r--r--   1 brian_farrell hadoop     173379 2025-02-19 17:27 /user/brian_farrell/five-books/alice_in_wonderland.txt\n",
        "    rw-r--r--   1 brian_farrell hadoop     394246 2025-02-19 17:27 /user/brian_farrell/five-books/sylvie_and_bruno.txt\n",
        "    -rw-r--r--   1 brian_farrell hadoop     458755 2025-02-19 17:27 /user/brian_farrell/five-books/symbolic_logic.txt\n",
        "    -rw-r--r--   1 brian_farrell hadoop     135443 2025-02-19 17:27 /user/brian_farrell/five-books/the_game_of_logic.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeZBNBS43j4S"
      },
      "source": [
        "#### 3. [2 points] Without writing any code of your own, verify that you have a good installation of hadoop by running wordcount on five-books."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmDDLhze3j4S"
      },
      "source": [
        "Command:\n",
        "\n",
        "    hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar wordcount /user/brian_farrell/five-books /books-count-0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBwzqYND3j4T"
      },
      "source": [
        "Output:\n",
        "\n",
        "    total 320\n",
        "    drwxr-xr-x  2 brian_farrell brian_farrell   4096 Feb 22 19:17 .\n",
        "    drwxr-xr-x 12 brian_farrell brian_farrell   4096 Feb 22 19:17 ..\n",
        "    -rw-r--r--  1 brian_farrell brian_farrell      0 Feb 22 19:17 _SUCCESS\n",
        "    -rw-r--r--  1 brian_farrell brian_farrell 105799 Feb 22 19:17 part-r-00000\n",
        "    -rw-r--r--  1 brian_farrell brian_farrell 103061 Feb 22 19:17 part-r-00001\n",
        "    -rw-r--r--  1 brian_farrell brian_farrell 104969 Feb 22 19:17 part-r-00002"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEFEd_hJ3j4T"
      },
      "source": [
        "Sample:\n",
        "\n",
        "    whole,” 1\n",
        "    wholesome,      1\n",
        "    wholesome;      11\n",
        "    wholesomeness.  1\n",
        "    whom    24\n",
        "    why     26\n",
        "    why,”   1\n",
        "    why--why--”     1\n",
        "    why?    1\n",
        "    wide    14\n",
        "    wide.”  1\n",
        "    wider.  1\n",
        "    width,  2\n",
        "    wife.   2\n",
        "    wig,    2\n",
        "    wig.\"   1\n",
        "    wigs\".  2\n",
        "    wigs.   1\n",
        "    wild,   1\n",
        "    wild;   2\n",
        "    wildest 1\n",
        "    will    414\n",
        "    will.'  1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZXlSk_d3j4T"
      },
      "source": [
        "#### 4. Run wordcount using the provided mapper_noll.py and the default reducer aggregate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2B4mP6D3j4T"
      },
      "source": [
        "Command:\n",
        "\n",
        "    mapred streaming \\\n",
        "    -files /home/brian_farrell/big-data-repo/hadoop/mapper_noll.py \\\n",
        "    -mapper mapper_noll.py \\\n",
        "    -reducer aggregate \\\n",
        "    -input /user/brian_farrell/five-books/* \\\n",
        "    -output /five-books-count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hi0Yte4C3j4T"
      },
      "source": [
        "Output:\n",
        "\n",
        "    total 116\n",
        "    drwxr-xr-x 2 brian_farrell brian_farrell  4096 Feb 22 19:59 .\n",
        "    drwxr-xr-x 3 brian_farrell brian_farrell  4096 Feb 22 19:59 ..\n",
        "    -rw-r--r-- 1 brian_farrell brian_farrell     0 Feb 22 19:59 _SUCCESS\n",
        "    -rw-r--r-- 1 brian_farrell brian_farrell 34743 Feb 22 19:59 part-00000\n",
        "    -rw-r--r-- 1 brian_farrell brian_farrell 34964 Feb 22 19:59 part-00001\n",
        "    -rw-r--r-- 1 brian_farrell brian_farrell 33989 Feb 22 19:59 part-00002"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yBfJa0Q3j4T"
      },
      "source": [
        "#### 5. [2 points] Run wordcount using the provided mapper_noll.py and the provided reducer reducer_noll.py."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zh2gTuCG3j4T"
      },
      "source": [
        "Command:\n",
        "\n",
        "    mapred streaming \\\n",
        "    -files /home/brian_farrell/big-data-repo/hadoop/mapper_noll.py,/home/brian_farrell/big-data-repo/hadoop/reducer_noll.py \\\n",
        "    -mapper mapper_noll.py \\\n",
        "    -reducer reducer_noll.py \\\n",
        "    -input /user/brian_farrell/five-books/* \\\n",
        "    -output /user/brian_farrell/five-books-output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_imVUM23j4T"
      },
      "source": [
        "Output:\n",
        "\n",
        "    total 244\n",
        "    drwxr-xr-x 2 brian_farrell brian_farrell  4096 Feb 22 19:37 .\n",
        "    drwxr-xr-x 5 brian_farrell brian_farrell  4096 Feb 22 19:37 ..\n",
        "    -rw-r--r-- 1 brian_farrell brian_farrell     0 Feb 22 19:37 _SUCCESS\n",
        "    -rw-r--r-- 1 brian_farrell brian_farrell 79255 Feb 22 19:37 part-00000\n",
        "    -rw-r--r-- 1 brian_farrell brian_farrell 79515 Feb 22 19:37 part-00001\n",
        "    -rw-r--r-- 1 brian_farrell brian_farrell 77539 Feb 22 19:37 part-00002"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vEjBaUO3j4U"
      },
      "source": [
        "Sample:\n",
        "\n",
        "    LongValueSum:www        40\n",
        "    LongValueSum:wykehamicus        1\n",
        "    LongValueSum:xii        4\n",
        "    LongValueSum:xix        4\n",
        "    LongValueSum:xxi        2\n",
        "    LongValueSum:xxviii     1\n",
        "    LongValueSum:xxx        1\n",
        "    LongValueSum:y  1486\n",
        "    LongValueSum:yawning    4\n",
        "    LongValueSum:years      53\n",
        "    LongValueSum:yer        8\n",
        "    LongValueSum:yielding   1\n",
        "    LongValueSum:ym 79\n",
        "    LongValueSum:younger    8\n",
        "    LongValueSum:yours      16\n",
        "    LongValueSum:youth      16\n",
        "    LongValueSum:youthful   1\n",
        "    LongValueSum:ys 3\n",
        "    LongValueSum:zeal       1\n",
        "    LongValueSum:zeroes     2\n",
        "    LongValueSum:zig        1\n",
        "    LongValueSum:zigzag     1\n",
        "    LongValueSum:zigzags    1\n",
        "    LongValueSum:zimmermann 2\n",
        "    LongValueSum:zip        5\n",
        "    LongValueSum:zo 5\n",
        "LongValueSum:zodiac     3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnaEvSRv3j4U"
      },
      "source": [
        "#### 6. [4 points] Modify the provided reducer_noll.py such that the resulting words don’t all begin with “LongValueSum:”"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjSmFfDV3j4U"
      },
      "source": [
        "Command:\n",
        "\n",
        "    mapred streaming     -files /home/brian_farrell/big-data-repo/hadoop/mapper_noll.py     -mapper mapper_noll.py     -reducer aggregate     -input /user/brian_farrell/five-books/*     -output /five-books-count\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxe3gDul3j4U"
      },
      "source": [
        "Output:\n",
        "\n",
        "    total 116\n",
        "    drwxr-xr-x 2 brian_farrell brian_farrell  4096 Feb 22 20:12 .\n",
        "    drwxr-xr-x 8 brian_farrell brian_farrell  4096 Feb 22 20:12 ..\n",
        "    -rw-r--r-- 1 brian_farrell brian_farrell     0 Feb 22 20:12 _SUCCESS\n",
        "    -rw-r--r-- 1 brian_farrell brian_farrell 34743 Feb 22 20:12 part-00000\n",
        "    -rw-r--r-- 1 brian_farrell brian_farrell 34964 Feb 22 20:12 part-00001\n",
        "    -rw-r--r-- 1 brian_farrell brian_farrell 33989 Feb 22 20:12 part-00002\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmTxufTz3j4U"
      },
      "source": [
        "Sample:\n",
        "\n",
        "    wurf    1\n",
        "    www     40\n",
        "    wykehamicus     1\n",
        "    xii     4\n",
        "    xix     4\n",
        "    xxi     2\n",
        "    xxviii  1\n",
        "    xxx     1\n",
        "    y       1486\n",
        "    yawning 4\n",
        "    years   53\n",
        "    yer     8\n",
        "    yielding        1\n",
        "    ym      79\n",
        "    younger 8\n",
        "    yours   16\n",
        "    youth   16\n",
        "    youthful        1\n",
        "    ys      3\n",
        "    zeal    1\n",
        "    zeroes  2\n",
        "    zig     1\n",
        "    zigzag  1\n",
        "    zigzags 1\n",
        "    zimmermann      2\n",
        "    zip     5\n",
        "    zo      5\n",
        "    zodiac  3\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5A1osRry3j4U"
      },
      "source": [
        "#### 7. [3 points] How many unique words were found by Q2.3, Q2.4 & Q2.6, respectively?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-t-MWxP3j4U"
      },
      "source": [
        "    2.3: 320\n",
        "    2.4: 116\n",
        "    2.6: 412\n",
        "    2.5: 244 (wasn't asked for)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ry9mZom53j4V"
      },
      "source": [
        "#### 8. [3 points] What accounts for the difference between {Q2.3 & Q2.4}, {Q2.4 & Q2.7} and {Q2.7 & Q2.3}, respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4U8N1OO73j4V"
      },
      "source": [
        "2.3 and 2.4 (320 vs 116):\n",
        "\n",
        "    The discrepancy between the wordcounts is likely due to the the 2.4 mapper adding the LongSumValue prefix to the words and the 2.4 reducer it uses does not properly account for this. Another factor, is the word count for 2.3 includes words ending in different punctuation. Looking at the sample data I provided for 2.3, \"why\" and \"why,\" counted as seperate words. 2.4 accounts for this, resulting in less words being counted as unique than the default wordcount of 2.3. Some of these words may have also been discarded because of incorrect format with the LongValueSum not being properly handled.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMYdsf0m3j4V"
      },
      "source": [
        "2.4 and 2.7(Actually 2.6):\n",
        "\n",
        "    The reducer in 2.6 did not include the LongValueSum in the result, so there are more unique words compared to 2.4. This use of \"LongValueSum\" could have led to some format issues in the results, meaning less unique words for 2.4. For example, \"zodiac\" appears in 2.6, but not 2.4. This could be because it had to be scrapped because of how \"LongValueSum\" was added to the word.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngfJdP623j4V"
      },
      "source": [
        "2.3 and 2.7 (Actually 2.6):\n",
        "\n",
        "    2.3 does not handle the LongValueSum from the mapper_noll properly, leading to some words getting discarded that shouldn’t. The reducer for 2.6 fixes this issue, leading to more words being accurately counted.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgwmlDfR3j4V"
      },
      "source": [
        "## 3. Analyzing Server Logs [55 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYaeG1eT3j4V"
      },
      "source": [
        "#### 1. [6+9=15 points] What is the percentage of each request type (GET, PUT, POST, etc.)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tA3YNNxy3j4V"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "### mapper_3.1.py\n",
        "import sys\n",
        "\n",
        "# input comes from STDIN\n",
        "for line in sys.stdin:\n",
        "    line = line.strip() # remove leading and trailing whitespace\n",
        "    parts = line.split()\n",
        "    # Line with not enough parts (i.e., malformed line)\n",
        "    if len(parts) < 5:\n",
        "        continue\n",
        "    request_type = parts[5].replace('\"', '')\n",
        "    print(f\"{request_type}\\t1\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVBjUM9m3j4W"
      },
      "outputs": [],
      "source": [
        "##Reducer:\n",
        "\n",
        "#!/usr/bin/env python\n",
        "\"\"\"reducer_3.1.py\n",
        "Aggregates the counts of each request type. Outputs each request type along\n",
        "with its total count.\n",
        "\"\"\"\n",
        "import sys\n",
        "current_type = None\n",
        "current_count = 0\n",
        "request_counts = {}\n",
        "# input comes from STDIN\n",
        "for line in sys.stdin:\n",
        "    line = line.strip() # remove leading and trailing whitespace\n",
        "    request_type, count = line.split('\\t', 1)\n",
        "    count = int(count)\n",
        "\n",
        "    if current_type == request_type:\n",
        "        current_count += count\n",
        "    else:\n",
        "        if current_type:\n",
        "            request_counts[current_type] = current_count\n",
        "        current_type = request_type\n",
        "        current_count = count\n",
        "\n",
        "# Don't forget to add the last request type\n",
        "if current_type:\n",
        "    request_counts[current_type] = current_count\n",
        "\n",
        "for request_type, count in request_counts.items():\n",
        "    print(f\"{request_type}\\t{count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vw1DDs-53j4W"
      },
      "source": [
        "Command:\n",
        "\n",
        "    hadoop jar /usr/lib/hadoop/hadoop-streaming-3.3.6.jar \\\n",
        "        -files /home/brian_farrell/big-data-repo/hadoop/mapper_3.1.py,/home/brian_farrell/big-data-repo/hadoop/reducer_3.1.py \\\n",
        "        -mapper mapper_3.1.py \\\n",
        "        -reducer reducer_3.1.py \\\n",
        "        -input /user/brian_farrell/access.log \\\n",
        "        -output /user/brian_farrell/server_logs_q1_counts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "El32v_6W3j4W"
      },
      "source": [
        "Terminal Output:\n",
        "\n",
        "        2025-02-22 16:39:33,739 INFO mapreduce.Job: Running job: job_1740240464508_0001\n",
        "        2025-02-22 16:39:43,930 INFO mapreduce.Job: Job job_1740240464508_0001 running in uber mode : false\n",
        "        2025-02-22 16:39:43,932 INFO mapreduce.Job:  map 0% reduce 0%\n",
        "        2025-02-22 16:39:56,052 INFO mapreduce.Job:  map 33% reduce 0%\n",
        "        2025-02-22 16:40:05,131 INFO mapreduce.Job:  map 67% reduce 0%\n",
        "        2025-02-22 16:40:15,203 INFO mapreduce.Job:  map 100% reduce 0%\n",
        "        2025-02-22 16:40:25,262 INFO mapreduce.Job:  map 100% reduce 33%\n",
        "        2025-02-22 16:40:26,270 INFO mapreduce.Job:  map 100% reduce 67%\n",
        "        2025-02-22 16:40:27,274 INFO mapreduce.Job:  map 100% reduce 100%\n",
        "        2025-02-22 16:40:28,283 INFO mapreduce.Job: Job job_1740240464508_0001 completed successfully\n",
        "        2025-02-22 16:40:28,357 INFO mapreduce.Job: Counters: 55\n",
        "                File System Counters\n",
        "                        FILE: Number of bytes read=670863\n",
        "                        FILE: Number of bytes written=4835061\n",
        "                        FILE: Number of read operations=0\n",
        "                        FILE: Number of large read operations=0\n",
        "                        FILE: Number of write operations=0\n",
        "                        HDFS: Number of bytes read=16167683\n",
        "                        HDFS: Number of bytes written=30\n",
        "                        HDFS: Number of read operations=42\n",
        "                        HDFS: Number of large read operations=0\n",
        "                        HDFS: Number of write operations=9\n",
        "                        HDFS: Number of bytes read erasure-coded=0\n",
        "                Job Counters\n",
        "                        Killed map tasks=1\n",
        "                        Launched map tasks=9\n",
        "                        Launched reduce tasks=3\n",
        "                        Data-local map tasks=9\n",
        "                        Total time spent by all maps in occupied slots (ms)=270825824\n",
        "                        Total time spent by all reduces in occupied slots (ms)=77674840\n",
        "                        Total time spent by all map tasks (ms)=79984\n",
        "                        Total time spent by all reduce tasks (ms)=22940\n",
        "                        Total vcore-milliseconds taken by all map tasks=79984\n",
        "                        Total vcore-milliseconds taken by all reduce tasks=22940\n",
        "                        Total megabyte-milliseconds taken by all map tasks=270825824\n",
        "                        Total megabyte-milliseconds taken by all reduce tasks=77674840\n",
        "                Map-Reduce Framework\n",
        "                        Map input records=78252\n",
        "                        Map output records=78251\n",
        "                        Map output bytes=514343\n",
        "                        Map output materialized bytes=671007\n",
        "                        Input split bytes=954\n",
        "                        Combine input records=0\n",
        "                        Combine output records=0\n",
        "                        Reduce input groups=3\n",
        "                        Reduce shuffle bytes=671007\n",
        "                        Reduce input records=78251\n",
        "                        Reduce output records=3\n",
        "                        Spilled Records=156502\n",
        "                        Shuffled Maps =27\n",
        "                        Failed Shuffles=0\n",
        "                        Merged Map outputs=27\n",
        "                        GC time elapsed (ms)=716\n",
        "                        CPU time spent (ms)=12770\n",
        "                        Physical memory (bytes) snapshot=6737772544\n",
        "                        Virtual memory (bytes) snapshot=57909956608\n",
        "                        Total committed heap usage (bytes)=6956253184\n",
        "                        Peak Map Physical memory (bytes)=653885440\n",
        "                        Peak Map Virtual memory (bytes)=4839124992\n",
        "                        Peak Reduce Physical memory (bytes)=426160128\n",
        "                        Peak Reduce Virtual memory (bytes)=4827148288\n",
        "                Shuffle Errors\n",
        "                        BAD_ID=0\n",
        "                        CONNECTION=0\n",
        "                        IO_ERROR=0\n",
        "                        WRONG_LENGTH=0\n",
        "                        WRONG_MAP=0\n",
        "                        WRONG_REDUCE=0\n",
        "                File Input Format Counters\n",
        "                        Bytes Read=16166729\n",
        "                File Output Format Counters\n",
        "                        Bytes Written=30\n",
        "        2025-02-22 16:40:28,357 INFO streaming.StreamJob: Output directory: /user/brian_farrell/server_logs_q1_counts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8iYqpwJ3j4X"
      },
      "source": [
        "Command Results:\n",
        "\n",
        "    cat part-00000\n",
        "        GET: 33414\n",
        "        POST: 44584\n",
        "    cat part-00001\n",
        "        ...\n",
        "    cat part-00002   \n",
        "        HEAD: 253\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2PVfBB93j4X"
      },
      "source": [
        "Total = 33414 + 44584 + 253 = 78,251\n",
        "\n",
        "    GET request percentage: 33,414 / 78,251 * 100 = 42.7%\n",
        "    POST request percentage: 44,584 / 78,251 * 100 = 57.0%\n",
        "    HEAD request percentage: 253 / 78,251 * 100 = 0.3%\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMDLG67e3j4X"
      },
      "source": [
        "#### 2. [6+9=15 points] What percent of the responses fall into each of the following five types?\n",
        "        Informational responses (100–199)\n",
        "        Successful responses (200–299)\n",
        "        Redirection messages (300–399)\n",
        "        Client error responses (400–499)\n",
        "        Server error responses (500–599)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbowYn2w3j4X"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "### mapper_3.2.py\n",
        "import sys\n",
        "\n",
        "# input comes from STDIN\n",
        "for line in sys.stdin:\n",
        "    line = line.strip() # remove leading and trailing whitespace\n",
        "\n",
        "    parts = line.split()\n",
        "\n",
        "    # Line with not enough parts (i.e., malformed line)\n",
        "    if len(parts) < 8:\n",
        "        continue\n",
        "\n",
        "    # The response code is in the ninth field\n",
        "    status_code = int(parts[8])\n",
        "\n",
        "    if 100 <= status_code < 200:\n",
        "        category = \"Informational\"\n",
        "    elif 200 <= status_code < 300:\n",
        "        category = \"Successful\"\n",
        "    elif 300 <= status_code < 400:\n",
        "        category = \"Redirection\"\n",
        "    elif 400 <= status_code < 500:\n",
        "        category = \"Client Error\"\n",
        "    elif 500 <= status_code < 600:\n",
        "        category = \"Server Error\"\n",
        "    else:\n",
        "        category = None\n",
        "\n",
        "    if category:\n",
        "        print(f\"{category}\\t1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NaQrMipA3j4X"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"reducer_3.2.py\n",
        "\n",
        "Aggregates the counts of each category. Outputs each category along\n",
        "with its total count.\n",
        "\"\"\"\n",
        "\n",
        "import sys\n",
        "\n",
        "category_counts = {}\n",
        "\n",
        "# input comes from STDIN\n",
        "for line in sys.stdin:\n",
        "\n",
        "    line = line.strip() # remove leading and trailing whitespace\n",
        "\n",
        "    category, count = line.split('\\t', 1)\n",
        "    count = int(count)\n",
        "\n",
        "    # Accumulate the counts for each category\n",
        "    if category in category_counts:\n",
        "        category_counts[category] += count\n",
        "    else:\n",
        "        category_counts[category] = count\n",
        "\n",
        "for category, count in category_counts.items():\n",
        "    print(f\"{category}\\t{count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vbmtz6lL3j4Y"
      },
      "source": [
        "Command:\n",
        "\n",
        "    hadoop jar /usr/lib/hadoop/hadoop-streaming-3.3.6.jar \\\n",
        "        -files /home/brian_farrell/big-data-repo/hadoop/mapper_3.2.py,/home/brian_farrell/big-data-repo/hadoop/reducer_3.2.py \\\n",
        "        -mapper mapper_3.2.py \\\n",
        "        -reducer reducer_3.2.py \\\n",
        "        -input /user/brian_farrell/access.log \\\n",
        "        -output /user/brian_farrell/server_logs_q2_counts\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpJGw3HF3j4Y"
      },
      "source": [
        "Terminal Output:\n",
        "\n",
        "    packageJobJar: [] [/usr/lib/hadoop/hadoop-streaming-3.3.6.jar] /tmp/streamjob17027406577386007500.jar tmpDir=null\n",
        "    2025-02-22 16:48:37,244 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at hadoop-homework-m.local./10.142.0.5:8032\n",
        "    2025-02-22 16:48:37,390 INFO client.AHSProxy: Connecting to Application History server at hadoop-homework-m.local./10.142.0.5:10200\n",
        "    2025-02-22 16:48:37,786 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at hadoop-homework-m.local./10.142.0.5:8032\n",
        "    2025-02-22 16:48:37,786 INFO client.AHSProxy: Connecting to Application History server at hadoop-homework-m.local./10.142.0.5:10200\n",
        "    2025-02-22 16:48:37,934 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/brian_farrell/.staging/job_1740240464508_0002\n",
        "    2025-02-22 16:48:38,222 INFO mapred.FileInputFormat: Total input files to process : 1\n",
        "    2025-02-22 16:48:38,266 INFO mapreduce.JobSubmitter: number of splits:9\n",
        "    2025-02-22 16:48:38,466 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1740240464508_0002\n",
        "    2025-02-22 16:48:38,467 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
        "    2025-02-22 16:48:38,607 INFO conf.Configuration: resource-types.xml not found\n",
        "    2025-02-22 16:48:38,608 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
        "    2025-02-22 16:48:38,664 INFO impl.YarnClientImpl: Submitted application application_1740240464508_0002\n",
        "    2025-02-22 16:48:38,696 INFO mapreduce.Job: The url to track the job: http://hadoop-homework-m.local.:8088/proxy/application_1740240464508_0002/\n",
        "    2025-02-22 16:48:38,698 INFO mapreduce.Job: Running job: job_1740240464508_0002\n",
        "    2025-02-22 16:48:46,777 INFO mapreduce.Job: Job job_1740240464508_0002 running in uber mode : false\n",
        "    2025-02-22 16:48:46,777 INFO mapreduce.Job:  map 0% reduce 0%\n",
        "    2025-02-22 16:48:57,917 INFO mapreduce.Job:  map 33% reduce 0%\n",
        "    2025-02-22 16:49:04,980 INFO mapreduce.Job:  map 44% reduce 0%\n",
        "    2025-02-22 16:49:06,991 INFO mapreduce.Job:  map 67% reduce 0%\n",
        "    2025-02-22 16:49:14,036 INFO mapreduce.Job:  map 78% reduce 0%\n",
        "    2025-02-22 16:49:16,053 INFO mapreduce.Job:  map 100% reduce 0%\n",
        "    2025-02-22 16:49:25,109 INFO mapreduce.Job:  map 100% reduce 33%\n",
        "    2025-02-22 16:49:26,113 INFO mapreduce.Job:  map 100% reduce 67%\n",
        "    2025-02-22 16:49:27,116 INFO mapreduce.Job:  map 100% reduce 100%\n",
        "    2025-02-22 16:49:29,128 INFO mapreduce.Job: Job job_1740240464508_0002 completed successfully\n",
        "    2025-02-22 16:49:29,195 INFO mapreduce.Job: Counters: 55\n",
        "            File System Counters\n",
        "                    FILE: Number of bytes read=1185988\n",
        "                    FILE: Number of bytes written=5865323\n",
        "                    FILE: Number of read operations=0\n",
        "                    FILE: Number of large read operations=0\n",
        "                    FILE: Number of write operations=0\n",
        "                    HDFS: Number of bytes read=16167683\n",
        "                    HDFS: Number of bytes written=52\n",
        "                    HDFS: Number of read operations=42\n",
        "                    HDFS: Number of large read operations=0\n",
        "                    HDFS: Number of write operations=9\n",
        "                    HDFS: Number of bytes read erasure-coded=0\n",
        "            Job Counters\n",
        "                    Killed map tasks=1\n",
        "                    Launched map tasks=9\n",
        "                    Launched reduce tasks=3\n",
        "                    Data-local map tasks=9\n",
        "                    Total time spent by all maps in occupied slots (ms)=245959040\n",
        "                    Total time spent by all reduces in occupied slots (ms)=74563106\n",
        "                    Total time spent by all map tasks (ms)=72640\n",
        "                    Total time spent by all reduce tasks (ms)=22021\n",
        "                    Total vcore-milliseconds taken by all map tasks=72640\n",
        "                    Total vcore-milliseconds taken by all reduce tasks=22021\n",
        "                    Total megabyte-milliseconds taken by all map tasks=245959040\n",
        "                    Total megabyte-milliseconds taken by all reduce tasks=74563106\n",
        "            Map-Reduce Framework\n",
        "                    Map input records=78252\n",
        "                    Map output records=78251\n",
        "                    Map output bytes=1029468\n",
        "                    Map output materialized bytes=1186132\n",
        "                    Input split bytes=954\n",
        "                    Combine input records=0\n",
        "                    Combine output records=0\n",
        "                    Reduce input groups=3\n",
        "                    Reduce shuffle bytes=1186132\n",
        "                    Reduce input records=78251\n",
        "                    Reduce output records=3\n",
        "                    Spilled Records=156502\n",
        "                    Shuffled Maps =27\n",
        "                    Failed Shuffles=0\n",
        "                    Merged Map outputs=27\n",
        "                    GC time elapsed (ms)=629\n",
        "                    CPU time spent (ms)=12990\n",
        "                    Physical memory (bytes) snapshot=6788599808\n",
        "                    Virtual memory (bytes) snapshot=57918042112\n",
        "                    Total committed heap usage (bytes)=6963593216\n",
        "                    Peak Map Physical memory (bytes)=620580864\n",
        "                    Peak Map Virtual memory (bytes)=4829052928\n",
        "                    Peak Reduce Physical memory (bytes)=483934208\n",
        "                    Peak Reduce Virtual memory (bytes)=4839940096\n",
        "            Shuffle Errors\n",
        "                    BAD_ID=0\n",
        "                    CONNECTION=0\n",
        "                    IO_ERROR=0\n",
        "                    WRONG_LENGTH=0\n",
        "                    WRONG_MAP=0\n",
        "                    WRONG_REDUCE=0\n",
        "            File Input Format Counters\n",
        "                    Bytes Read=16166729\n",
        "            File Output Format Counters\n",
        "                    Bytes Written=52\n",
        "    2025-02-22 16:49:29,195 INFO streaming.StreamJob: Output directory: /user/brian_farrell/server_logs_q2_counts\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCUfvZLK3j4Y"
      },
      "source": [
        "Command Results:\n",
        "\n",
        "    cat part-00000:\n",
        "        Client Error    4638\n",
        "    cat part-00001\n",
        "        ...\n",
        "    cat part-00002\n",
        "        Redirection     2929\n",
        "        Successful      70684\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQ6kMwqt3j4Y"
      },
      "source": [
        "Calculations:\n",
        "\n",
        "    Total = 4,638 + 2,929 + 70,684 = 78,251\n",
        "    Client Error percentage: 4,638 / 78,251 * 100 = 5.93%\n",
        "    Redirection percentage: 2,929 / 78,251 * 100 = 3.74%\n",
        "    Successful percentage: 70,684 / 78,251 * 100 = 90.33%\n",
        "    Informational percentage: 0%\n",
        "    Server Error percentage: 0%\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZUQzCvR3j4Y"
      },
      "source": [
        "#### 3. [9+16=25 points] What 5 IP addresses generate the most client errors?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqGeluD73j4Z"
      },
      "outputs": [],
      "source": [
        "##Mapper:\n",
        "\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"mapper_3.3.py\n",
        "\n",
        "Reads each line of the log, extracts the IP address and the status code,\n",
        "and only emits the IP addresses for client errors (status codes 400–499).\n",
        "Outputs each IP address as the key followed by a count of 1.\n",
        "\"\"\"\n",
        "\n",
        "import sys\n",
        "\n",
        "# input comes from STDIN\n",
        "for line in sys.stdin:\n",
        "    line = line.strip() # remove leading and trailing whitespace\n",
        "\n",
        "    parts = line.split()\n",
        "\n",
        "    # Line with not enough parts (i.e., malformed line)\n",
        "    if len(parts) < 8:\n",
        "        continue\n",
        "\n",
        "    ip_address = parts[0]  # The IP address is the first part\n",
        "    status_code = int(parts[8])  # The status code is the ninth part\n",
        "\n",
        "    if 400 <= status_code < 500:\n",
        "        print(f\"{ip_address}\\t{1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZviswfB3j4Z"
      },
      "outputs": [],
      "source": [
        "##Reducer:\n",
        "\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"reducer_3.3.py\n",
        "\n",
        "Aggregates the counts of client errors for each IP address, sorts the IPs\n",
        "by the total number of client errors, and prints the top 5 IP addresses that\n",
        "generated the most client errors.\n",
        "\"\"\"\n",
        "\n",
        "import sys\n",
        "import collections\n",
        "\n",
        "ip_error_counts = collections.defaultdict(int)\n",
        "\n",
        "# input comes from STDIN\n",
        "for line in sys.stdin:\n",
        "    line = line.strip() # remove leading and trailing whitespace\n",
        "\n",
        "    ip_address, count = line.split('\\t', 1)\n",
        "    count = int(count)\n",
        "    ip_error_counts[ip_address] += count\n",
        "\n",
        "# Sort IPs by the number of client errors in descending order and take the top 5\n",
        "top_5_ips = sorted(ip_error_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n",
        "for ip, count in top_5_ips:\n",
        "    print(f\"{ip}\\t{count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bX8pZ_MP3j4Z"
      },
      "source": [
        "Command Run:\n",
        "\n",
        "    hadoop jar /usr/lib/hadoop/hadoop-streaming-3.3.6.jar \\\n",
        "        -files /home/brian_farrell/big-data-repo/hadoop/mapper_3.3.py,/home/brian_farrell/big-data-repo/hadoop/reducer_3.3.py \\\n",
        "        -mapper mapper_3.3.py \\\n",
        "        -reducer reducer_3.3.py \\\n",
        "        -input /user/brian_farrell/access.log \\\n",
        "        -output /user/brian_farrell/server_logs_q3_counts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MnXhzTy3j4Z"
      },
      "source": [
        "Terminal Output:\n",
        "\n",
        "    packageJobJar: [] [/usr/lib/hadoop/hadoop-streaming-3.3.6.jar] /tmp/streamjob8572591051469266581.jar tmpDir=null\n",
        "    2025-02-22 17:05:44,231 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at hadoop-homework-m.local./10.142.0.5:8032\n",
        "    2025-02-22 17:05:44,389 INFO client.AHSProxy: Connecting to Application History server at hadoop-homework-m.local./10.142.0.5:10200\n",
        "    2025-02-22 17:05:44,764 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at hadoop-homework-m.local./10.142.0.5:8032\n",
        "    2025-02-22 17:05:44,764 INFO client.AHSProxy: Connecting to Application History server at hadoop-homework-m.local./10.142.0.5:10200\n",
        "    2025-02-22 17:05:44,910 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/brian_farrell/.staging/job_1740240464508_0003\n",
        "    2025-02-22 17:05:45,624 INFO mapred.FileInputFormat: Total input files to process : 1\n",
        "    2025-02-22 17:05:46,069 INFO mapreduce.JobSubmitter: number of splits:9\n",
        "    2025-02-22 17:05:46,640 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1740240464508_0003\n",
        "    2025-02-22 17:05:46,640 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
        "    2025-02-22 17:05:46,767 INFO conf.Configuration: resource-types.xml not found\n",
        "    2025-02-22 17:05:46,767 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
        "    2025-02-22 17:05:46,820 INFO impl.YarnClientImpl: Submitted application application_1740240464508_0003\n",
        "    2025-02-22 17:05:46,886 INFO mapreduce.Job: The url to track the job: http://hadoop-homework-m.local.:8088/proxy/application_1740240464508_0003/\n",
        "    2025-02-22 17:05:46,887 INFO mapreduce.Job: Running job: job_1740240464508_0003\n",
        "    2025-02-22 17:05:54,976 INFO mapreduce.Job: Job job_1740240464508_0003 running in uber mode : false\n",
        "    2025-02-22 17:05:54,977 INFO mapreduce.Job:  map 0% reduce 0%\n",
        "    2025-02-22 17:06:05,096 INFO mapreduce.Job:  map 11% reduce 0%\n",
        "    2025-02-22 17:06:06,110 INFO mapreduce.Job:  map 33% reduce 0%\n",
        "    2025-02-22 17:06:15,180 INFO mapreduce.Job:  map 67% reduce 0%\n",
        "    2025-02-22 17:06:25,258 INFO mapreduce.Job:  map 100% reduce 0%\n",
        "    2025-02-22 17:06:35,310 INFO mapreduce.Job:  map 100% reduce 33%\n",
        "    2025-02-22 17:06:36,317 INFO mapreduce.Job:  map 100% reduce 67%\n",
        "    2025-02-22 17:06:37,321 INFO mapreduce.Job:  map 100% reduce 100%\n",
        "    2025-02-22 17:06:38,329 INFO mapreduce.Job: Job job_1740240464508_0003 completed successfully\n",
        "    2025-02-22 17:06:38,399 INFO mapreduce.Job: Counters: 55\n",
        "            File System Counters\n",
        "                    FILE: Number of bytes read=84024\n",
        "                    FILE: Number of bytes written=3661395\n",
        "                    FILE: Number of read operations=0\n",
        "                    FILE: Number of large read operations=0\n",
        "                    FILE: Number of write operations=0\n",
        "                    HDFS: Number of bytes read=16167683\n",
        "                    HDFS: Number of bytes written=259\n",
        "                    HDFS: Number of read operations=42\n",
        "                    HDFS: Number of large read operations=0\n",
        "                    HDFS: Number of write operations=9\n",
        "                    HDFS: Number of bytes read erasure-coded=0\n",
        "            Job Counters\n",
        "                    Killed map tasks=1\n",
        "                    Launched map tasks=9\n",
        "                    Launched reduce tasks=3\n",
        "                    Data-local map tasks=9\n",
        "                    Total time spent by all maps in occupied slots (ms)=264253598\n",
        "                    Total time spent by all reduces in occupied slots (ms)=73364462\n",
        "                    Total time spent by all map tasks (ms)=78043\n",
        "                    Total time spent by all reduce tasks (ms)=21667\n",
        "                    Total vcore-milliseconds taken by all map tasks=78043\n",
        "                    Total vcore-milliseconds taken by all reduce tasks=21667\n",
        "                    Total megabyte-milliseconds taken by all map tasks=264253598\n",
        "                    Total megabyte-milliseconds taken by all reduce tasks=73364462\n",
        "            Map-Reduce Framework\n",
        "                    Map input records=78252\n",
        "                    Map output records=4638\n",
        "                    Map output bytes=74730\n",
        "                    Map output materialized bytes=84168\n",
        "                    Input split bytes=954\n",
        "                    Combine input records=0\n",
        "                    Combine output records=0\n",
        "                    Reduce input groups=803\n",
        "                    Reduce shuffle bytes=84168\n",
        "                    Reduce input records=4638\n",
        "                    Reduce output records=15\n",
        "                    Spilled Records=9276\n",
        "                    Shuffled Maps =27\n",
        "                    Failed Shuffles=0\n",
        "                    Merged Map outputs=27\n",
        "                    GC time elapsed (ms)=639\n",
        "                    CPU time spent (ms)=11890\n",
        "                    Physical memory (bytes) snapshot=6683926528\n",
        "                    Virtual memory (bytes) snapshot=57905254400\n",
        "                    Total committed heap usage (bytes)=7199522816\n",
        "                    Peak Map Physical memory (bytes)=682102784\n",
        "                    Peak Map Virtual memory (bytes)=4830674944\n",
        "                    Peak Reduce Physical memory (bytes)=471760896\n",
        "                    Peak Reduce Virtual memory (bytes)=4839522304\n",
        "            Shuffle Errors\n",
        "                    BAD_ID=0\n",
        "                    CONNECTION=0\n",
        "                    IO_ERROR=0\n",
        "                    WRONG_LENGTH=0\n",
        "                    WRONG_MAP=0\n",
        "                    WRONG_REDUCE=0\n",
        "            File Input Format Counters\n",
        "                    Bytes Read=16166729\n",
        "            File Output Format Counters\n",
        "                    Bytes Written=259\n",
        "    2025-02-22 17:06:38,399 INFO streaming.StreamJob: Output directory: /user/brian_farrell/server_logs_q3_counts\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mhFpJn63j4Z"
      },
      "source": [
        "Output File Commands:\n",
        "\n",
        "    cat part-00000:\n",
        "        193.106.30.100  53\n",
        "        91.171.55.234   28\n",
        "        81.5.193.243    23\n",
        "        143.244.43.177  22\n",
        "        80.110.118.26   21\n",
        "\n",
        "    cat part-00001\n",
        "        173.255.176.5   2059\n",
        "        13.77.204.88    78\n",
        "        51.210.243.185  58\n",
        "        62.35.7.187     41\n",
        "        52.66.242.202   36\n",
        "\n",
        "    cat part-00002\n",
        "        212.9.160.24    126\n",
        "        213.171.211.253 37\n",
        "        91.134.208.24   19\n",
        "        103.73.71.85    18\n",
        "        139.162.191.178 18\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_41Iquh3j4a"
      },
      "source": [
        "Top 5 IP Addresses:\n",
        "\n",
        "1.\t 173.255.176.5   2059\n",
        "2.\t 212.9.160.24    126\n",
        "3.\t 13.77.204.88    78\n",
        "4.\t 51.210.243.185  58\n",
        "5.\t 193.106.30.100  53\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaJui7wl3j4a"
      },
      "source": [
        "## 4. Presidential Speeches [15 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuAHPF3Q3j4a"
      },
      "source": [
        "#### Compute the average valence of each president’s speeches according to this outline:\n",
        "\n",
        "##### 1. [7 points] In the mapper (which is given a sequence of lines of speeches as input):\n",
        "    a. Clean each line as suggested above,\n",
        "    b. Calculate the valence of each word in the line,\n",
        "    c. Emit a (tab-separated) key-value pair (president, word valence) for each word in the line.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mW8pNXYC3j4a"
      },
      "source": [
        "The mapper used for question 1 is below. The explanation and output with the mapper and reducer is shown in the answer to question 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0pgjjIr3j4a"
      },
      "outputs": [],
      "source": [
        "##Mapper:\n",
        "\n",
        "#!/usr/bin/env python\n",
        "\"\"\"mapper_pres.py\n",
        "\n",
        "Given a sequence of lines of speeches as input, cleans each line,\n",
        "calculates the valence of each word in the line, and emits a (tab-separated)\n",
        "key-value pair (president, word valence) for each word in the line.\n",
        "\"\"\"\n",
        "\n",
        "import requests\n",
        "import re\n",
        "import string\n",
        "import sys\n",
        "import string\n",
        "import os\n",
        "\n",
        "stopwords_list = requests.get(\"https://gist.githubusercontent.com/rg089/35e00abf8941d72d419224cfd5b5925d/raw/12d899b70156fd0041fa9778d657330b024b959c/stopwords.txt\").content\n",
        "stopwords = list(set(stopwords_list.decode().splitlines()))\n",
        "\n",
        "\n",
        "def remove_stopwords(words):\n",
        "    list_ = re.sub(r\"[^a-zA-Z0-9]\", \" \", words.lower()).split()\n",
        "    return [itm for itm in list_ if itm not in stopwords]\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub('\\[.*?\\]', '', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
        "    text = re.sub('[\\d\\n]', ' ', text)\n",
        "    return ' '.join(remove_stopwords(text))\n",
        "\n",
        "def build_valence_dict():\n",
        "    afinn_dict = {}\n",
        "\n",
        "    url = \"https://raw.githubusercontent.com/fnielsen/afinn/master/afinn/data/AFINN-en-165.txt\"\n",
        "    response = requests.get(url)\n",
        "    lines = response.text.splitlines()\n",
        "\n",
        "    for line in lines:\n",
        "        word, valence = line.split('\\t')\n",
        "        afinn_dict[word] = int(valence)\n",
        "\n",
        "    assert(len(afinn_dict) == 3382)\n",
        "\n",
        "    return afinn_dict\n",
        "\n",
        "def calc_valence(text):\n",
        "    \"\"\"Takes a cleaned line of any presidential speech, handles edge cases\n",
        "    (including when text is empty, when text is a string of non-printable\n",
        "    characters, and when text is a bytecode string), computes the valences\n",
        "    of each word in the line, and returns a list of these valences.\"\"\"\n",
        "\n",
        "    # If the text is an empty string or contains only whitespace characters.\n",
        "    if len(text) == 0 or text.strip() == '':\n",
        "        return []\n",
        "\n",
        "    # If text is a string of non-printable characters.\n",
        "    if all(char not in string.printable for char in text):\n",
        "        return []\n",
        "\n",
        "    # If text is a bytecode string.\n",
        "    if isinstance(text, bytes):\n",
        "        return []\n",
        "\n",
        "    words = text.split()\n",
        "    valences = []\n",
        "    for word in words:\n",
        "        if word in afinn_dict:\n",
        "            #valences.append((word, afinn_dict[word]))\n",
        "            valences.append(afinn_dict[word])\n",
        "\n",
        "    return valences\n",
        "\n",
        "def valence(text):\n",
        "    \"\"\"Takes a line of any presidential speech and returns its valence after\n",
        "    cleaning it.\n",
        "    \"\"\"\n",
        "    return calc_valence(clean_text(text))\n",
        "\n",
        "\n",
        "# Build the valence dict from https://raw.githubusercontent.com/fnielsen/afinn/master/afinn/data/AFINN-en-165.txt\n",
        "afinn_dict = build_valence_dict()\n",
        "\n",
        "# input comes from STDIN\n",
        "for line in sys.stdin:\n",
        "\n",
        "    # Get a list of valences for each word in the current line.\n",
        "    valences = valence(line)\n",
        "\n",
        "    # Get the filename currently being processed and parse it to get\n",
        "    # the current president's name.\n",
        "    try:\n",
        "        file_name = os.environ['mapreduce_map_input_file']\n",
        "    except KeyError:\n",
        "        file_name = os.environ['map_input_file']\n",
        "    file_name = file_name.split('/')[-1]\n",
        "    pres_name = file_name.split('_')[0]\n",
        "\n",
        "    for x in valences:\n",
        "        print(f\"{pres_name}\\t{x}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fBZQS4l3j4a"
      },
      "source": [
        "#### 2. [6 points] In the reducer (which is given all (president, word valence) key-value pairs with the same key, i.e.president):\n",
        "        a. Compute the average valence of all words spoken by the president,\n",
        "        b. Emit a (tab-separated) key-value pair (president, sentiment of president’s speeches).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4SQRYnA3j4b"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "\n",
        "###reducer_pres.py\n",
        "\n",
        "\n",
        "\n",
        "import sys\n",
        "\n",
        "current_president = None\n",
        "valence_list = []\n",
        "\n",
        "# input comes from STDIN\n",
        "for line in sys.stdin:\n",
        "\n",
        "    line = line.strip()  # remove extra whitespace\n",
        "\n",
        "    president, valence = line.split('\\t', 1)\n",
        "    valence = float(valence)\n",
        "\n",
        "    # If for first president\n",
        "    if current_president is None:\n",
        "        current_president = president\n",
        "\n",
        "    # Check if we have moved to a new president\n",
        "    if current_president != president:\n",
        "\n",
        "        # Output the average sentiment for the previous president\n",
        "        avg_valence = sum(valence_list) / len(valence_list)\n",
        "        print(f\"{current_president}\\t{sum(valence_list) / len(valence_list):.4f}\")\n",
        "\n",
        "        # Reset for  new president\n",
        "        current_president = president\n",
        "        valence_list = []\n",
        "\n",
        "    # Append the current valence to the list for the current president\n",
        "    valence_list.append(valence)\n",
        "\n",
        "# output the last president if needed\n",
        "if current_president:\n",
        "    avg_valence = sum(valence_list) / len(valence_list)\n",
        "    print(f\"{current_president}\\t{avg_valence:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngDWy3JF3j4b"
      },
      "source": [
        "Command Outputs:\n",
        "\n",
        "    cat part-00000\n",
        "        adams   0.5786\n",
        "        buchanan        0.2075\n",
        "        bush    0.4552\n",
        "        cleveland       0.4633\n",
        "        fillmore        0.5572\n",
        "        garfield        0.3992\n",
        "        lincoln -0.0624\n",
        "        madison 0.5097\n",
        "        mckinley        0.6250\n",
        "        obama   0.5076\n",
        "        reagan  0.4107\n",
        "        taft    0.6663\n",
        "        truman  0.4895\n",
        "        tyler   0.5092\n",
        "\n",
        "    cat part-00001\n",
        "        arthur  0.5791\n",
        "        carter  0.3769\n",
        "        coolidge        0.7562\n",
        "        fdroosevelt     0.2491\n",
        "        grant   0.5603\n",
        "        gwbush  0.2886\n",
        "        hoover  0.3796\n",
        "        johnson 0.2509\n",
        "        jqadams 0.8077\n",
        "        monroe  0.8293\n",
        "        pierce  0.4761\n",
        "        polk    0.3765\n",
        "        taylor  0.8567\n",
        "        washington      0.6852\n",
        "    \n",
        "    cat part-00002\n",
        "        bharrison       0.5404\n",
        "        clinton 0.3855\n",
        "        eisenhower      0.7023\n",
        "        ford    0.6131\n",
        "        harding 0.3848\n",
        "        harrison        0.5650\n",
        "        hayes   0.6773\n",
        "        jackson 0.4907\n",
        "        jefferson       0.5214\n",
        "        kennedy 0.4792\n",
        "        lbjohnson       0.4594\n",
        "        nixon   0.3671\n",
        "        roosevelt       0.3084\n",
        "        vanburen        0.4908\n",
        "        wilson  0.4972\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNGYt9Jx3j4b"
      },
      "source": [
        "## 5. Hadoop Errors [15 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l97EWa_F3j4b"
      },
      "source": [
        "#### 1. [7 points] Where (what server & location) did the divide-by-zero error messages show up and how many did you find?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTZ5oTo83j4b"
      },
      "source": [
        "    There were 14 errors that were thrown and displayed. The below output shows that all 15 of the jobs failed as well as an example of one of the 14 errors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFQlY7Wt3j4b"
      },
      "source": [
        "    Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
        "            at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
        "            at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
        "            at org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
        "            at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
        "            at org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
        "            at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
        "            at org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
        "            at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:178)\n",
        "            at java.base/java.security.AccessController.doPrivileged(Native Method)\n",
        "            at java.base/javax.security.auth.Subject.doAs(Subject.java:423)\n",
        "            at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)\n",
        "            at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:172)\n",
        "    \n",
        "    2025-02-20 17:55:16,823 INFO mapreduce.Job: Counters: 14\n",
        "            Job Counters\n",
        "                    Failed map tasks=15\n",
        "                    Killed map tasks=10\n",
        "                    Killed reduce tasks=3\n",
        "                    Launched map tasks=16\n",
        "                    Other local map tasks=11\n",
        "                    Data-local map tasks=5\n",
        "                    Total time spent by all maps in occupied slots (ms)=400265832\n",
        "                    Total time spent by all reduces in occupied slots (ms)=0\n",
        "                    Total time spent by all map tasks (ms)=118212\n",
        "                    Total vcore-milliseconds taken by all map tasks=118212\n",
        "                    Total megabyte-milliseconds taken by all map tasks=400265832\n",
        "            Map-Reduce Framework\n",
        "                    CPU time spent (ms)=0\n",
        "                    Physical memory (bytes) snapshot=0\n",
        "                    Virtual memory (bytes) snapshot=0\n",
        "    2025-02-20 17:55:16,823 ERROR streaming.StreamJob: Job not successful!\n",
        "    Streaming Command Failed!\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4S62xXq3j4c"
      },
      "source": [
        "#### 2.\t[8 points] How many such messages did you find? Is the count you found consistent with what you might expect from random.randint(0,99)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HG0G_qZ3j4c"
      },
      "source": [
        "    There were 14 total error messages. 15/15 of the jobs failed as well. The line x = 1 / random.randint(0,99) is run on every single word that is processed in the five books. The line should produce a division by zero around every 100 times it is run. Since there are thousands of lines in each book, I would expect each of the jobs to have many more than 100. Therefore, this is in line with what I would expect from the randint line."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}