{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xwKPkm7bkxJW",
        "outputId": "4a9199f0-c73a-41b9-bafe-9390e74701f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Fetching AAPL data for 2021-01...\n",
            "Successfully fetched 494 rows for AAPL in 2021-01.\n",
            "Fetching AAPL data for 2021-02...\n",
            "Successfully fetched 494 rows for AAPL in 2021-02.\n",
            "Fetching AAPL data for 2021-03...\n",
            "Successfully fetched 598 rows for AAPL in 2021-03.\n",
            "Fetching AAPL data for 2021-04...\n",
            "Successfully fetched 546 rows for AAPL in 2021-04.\n",
            "Fetching AAPL data for 2021-05...\n",
            "Successfully fetched 520 rows for AAPL in 2021-05.\n",
            "Fetching AAPL data for 2021-06...\n",
            "Successfully fetched 572 rows for AAPL in 2021-06.\n",
            "Fetching AAPL data for 2021-07...\n",
            "Successfully fetched 546 rows for AAPL in 2021-07.\n",
            "Fetching AAPL data for 2021-08...\n",
            "Successfully fetched 572 rows for AAPL in 2021-08.\n",
            "Fetching AAPL data for 2021-09...\n",
            "Successfully fetched 546 rows for AAPL in 2021-09.\n",
            "Fetching AAPL data for 2021-10...\n",
            "Successfully fetched 546 rows for AAPL in 2021-10.\n",
            "Fetching AAPL data for 2021-11...\n",
            "Successfully fetched 546 rows for AAPL in 2021-11.\n",
            "Fetching AAPL data for 2021-12...\n",
            "Successfully fetched 572 rows for AAPL in 2021-12.\n",
            "Fetching AAPL data for 2022-01...\n",
            "Successfully fetched 520 rows for AAPL in 2022-01.\n",
            "Fetching AAPL data for 2022-02...\n",
            "Successfully fetched 494 rows for AAPL in 2022-02.\n",
            "Fetching AAPL data for 2022-03...\n",
            "Successfully fetched 598 rows for AAPL in 2022-03.\n",
            "Fetching AAPL data for 2022-04...\n",
            "Successfully fetched 520 rows for AAPL in 2022-04.\n",
            "Fetching AAPL data for 2022-05...\n",
            "Successfully fetched 546 rows for AAPL in 2022-05.\n",
            "Fetching AAPL data for 2022-06...\n",
            "Successfully fetched 546 rows for AAPL in 2022-06.\n",
            "Fetching AAPL data for 2022-07...\n",
            "Successfully fetched 520 rows for AAPL in 2022-07.\n",
            "Fetching AAPL data for 2022-08...\n",
            "Successfully fetched 598 rows for AAPL in 2022-08.\n",
            "Fetching AAPL data for 2022-09...\n",
            "Successfully fetched 546 rows for AAPL in 2022-09.\n",
            "Fetching AAPL data for 2022-10...\n",
            "Successfully fetched 546 rows for AAPL in 2022-10.\n",
            "Fetching AAPL data for 2022-11...\n",
            "Successfully fetched 534 rows for AAPL in 2022-11.\n",
            "Fetching AAPL data for 2022-12...\n",
            "Successfully fetched 546 rows for AAPL in 2022-12.\n",
            "Fetching AAPL data for 2023-01...\n",
            "Successfully fetched 520 rows for AAPL in 2023-01.\n",
            "Fetching AAPL data for 2023-02...\n",
            "Successfully fetched 494 rows for AAPL in 2023-02.\n",
            "Fetching AAPL data for 2023-03...\n",
            "Successfully fetched 597 rows for AAPL in 2023-03.\n",
            "Fetching AAPL data for 2023-04...\n",
            "Successfully fetched 494 rows for AAPL in 2023-04.\n",
            "Fetching AAPL data for 2023-05...\n",
            "Successfully fetched 572 rows for AAPL in 2023-05.\n",
            "Fetching AAPL data for 2023-06...\n",
            "Successfully fetched 546 rows for AAPL in 2023-06.\n",
            "Fetching AAPL data for 2023-07...\n",
            "Successfully fetched 520 rows for AAPL in 2023-07.\n",
            "Fetching AAPL data for 2023-08...\n",
            "Successfully fetched 598 rows for AAPL in 2023-08.\n",
            "Fetching AAPL data for 2023-09...\n",
            "Successfully fetched 520 rows for AAPL in 2023-09.\n",
            "Fetching AAPL data for 2023-10...\n",
            "Successfully fetched 572 rows for AAPL in 2023-10.\n",
            "Fetching AAPL data for 2023-11...\n",
            "Successfully fetched 534 rows for AAPL in 2023-11.\n",
            "Fetching AAPL data for 2023-12...\n",
            "Successfully fetched 520 rows for AAPL in 2023-12.\n",
            "Fetching AAPL data for 2024-01...\n",
            "Successfully fetched 546 rows for AAPL in 2024-01.\n",
            "Fetching AAPL data for 2024-02...\n",
            "Successfully fetched 520 rows for AAPL in 2024-02.\n",
            "Fetching AAPL data for 2024-03...\n",
            "Successfully fetched 520 rows for AAPL in 2024-03.\n",
            "Fetching AAPL data for 2024-04...\n",
            "Successfully fetched 572 rows for AAPL in 2024-04.\n",
            "Fetching AAPL data for 2024-05...\n",
            "Successfully fetched 572 rows for AAPL in 2024-05.\n",
            "Fetching AAPL data for 2024-06...\n",
            "Successfully fetched 494 rows for AAPL in 2024-06.\n",
            "Fetching AAPL data for 2024-07...\n",
            "Successfully fetched 561 rows for AAPL in 2024-07.\n",
            "Fetching AAPL data for 2024-08...\n",
            "Successfully fetched 572 rows for AAPL in 2024-08.\n",
            "Fetching AAPL data for 2024-09...\n",
            "Successfully fetched 520 rows for AAPL in 2024-09.\n",
            "Fetching AAPL data for 2024-10...\n",
            "Successfully fetched 598 rows for AAPL in 2024-10.\n",
            "Fetching AAPL data for 2024-11...\n",
            "Successfully fetched 510 rows for AAPL in 2024-11.\n",
            "Fetching AAPL data for 2024-12...\n",
            "Successfully fetched 536 rows for AAPL in 2024-12.\n",
            "AAPL data saved to Google Drive as AAPL_data.csv\n",
            "Fetching MSFT data for 2021-01...\n",
            "Successfully fetched 494 rows for MSFT in 2021-01.\n",
            "Fetching MSFT data for 2021-02...\n",
            "Successfully fetched 492 rows for MSFT in 2021-02.\n",
            "Fetching MSFT data for 2021-03...\n",
            "Successfully fetched 598 rows for MSFT in 2021-03.\n",
            "Fetching MSFT data for 2021-04...\n",
            "Successfully fetched 546 rows for MSFT in 2021-04.\n",
            "Fetching MSFT data for 2021-05...\n",
            "Successfully fetched 520 rows for MSFT in 2021-05.\n",
            "Fetching MSFT data for 2021-06...\n",
            "Successfully fetched 572 rows for MSFT in 2021-06.\n",
            "Fetching MSFT data for 2021-07...\n",
            "Successfully fetched 546 rows for MSFT in 2021-07.\n",
            "Fetching MSFT data for 2021-08...\n",
            "Successfully fetched 570 rows for MSFT in 2021-08.\n",
            "Fetching MSFT data for 2021-09...\n",
            "Successfully fetched 546 rows for MSFT in 2021-09.\n",
            "Fetching MSFT data for 2021-10...\n",
            "Successfully fetched 546 rows for MSFT in 2021-10.\n",
            "Fetching MSFT data for 2021-11...\n",
            "Successfully fetched 546 rows for MSFT in 2021-11.\n",
            "Fetching MSFT data for 2021-12...\n",
            "Successfully fetched 572 rows for MSFT in 2021-12.\n",
            "Fetching MSFT data for 2022-01...\n",
            "Successfully fetched 520 rows for MSFT in 2022-01.\n",
            "Fetching MSFT data for 2022-02...\n",
            "Successfully fetched 494 rows for MSFT in 2022-02.\n",
            "Fetching MSFT data for 2022-03...\n",
            "Successfully fetched 598 rows for MSFT in 2022-03.\n",
            "Fetching MSFT data for 2022-04...\n",
            "Successfully fetched 520 rows for MSFT in 2022-04.\n",
            "Fetching MSFT data for 2022-05...\n",
            "Successfully fetched 546 rows for MSFT in 2022-05.\n",
            "Fetching MSFT data for 2022-06...\n",
            "Successfully fetched 546 rows for MSFT in 2022-06.\n",
            "Fetching MSFT data for 2022-07...\n",
            "Successfully fetched 520 rows for MSFT in 2022-07.\n",
            "Fetching MSFT data for 2022-08...\n",
            "Successfully fetched 598 rows for MSFT in 2022-08.\n",
            "Fetching MSFT data for 2022-09...\n",
            "Successfully fetched 546 rows for MSFT in 2022-09.\n",
            "Fetching MSFT data for 2022-10...\n",
            "Successfully fetched 546 rows for MSFT in 2022-10.\n",
            "Fetching MSFT data for 2022-11...\n",
            "Successfully fetched 534 rows for MSFT in 2022-11.\n",
            "Fetching MSFT data for 2022-12...\n",
            "Successfully fetched 546 rows for MSFT in 2022-12.\n",
            "Fetching MSFT data for 2023-01...\n",
            "Successfully fetched 520 rows for MSFT in 2023-01.\n",
            "Fetching MSFT data for 2023-02...\n",
            "Successfully fetched 494 rows for MSFT in 2023-02.\n",
            "Fetching MSFT data for 2023-03...\n",
            "Successfully fetched 598 rows for MSFT in 2023-03.\n",
            "Fetching MSFT data for 2023-04...\n",
            "Successfully fetched 494 rows for MSFT in 2023-04.\n",
            "Fetching MSFT data for 2023-05...\n",
            "Successfully fetched 572 rows for MSFT in 2023-05.\n",
            "Fetching MSFT data for 2023-06...\n",
            "Successfully fetched 546 rows for MSFT in 2023-06.\n",
            "Fetching MSFT data for 2023-07...\n",
            "Successfully fetched 520 rows for MSFT in 2023-07.\n",
            "Fetching MSFT data for 2023-08...\n",
            "Successfully fetched 598 rows for MSFT in 2023-08.\n",
            "Fetching MSFT data for 2023-09...\n",
            "Successfully fetched 520 rows for MSFT in 2023-09.\n",
            "Fetching MSFT data for 2023-10...\n",
            "Successfully fetched 572 rows for MSFT in 2023-10.\n",
            "Fetching MSFT data for 2023-11...\n",
            "Successfully fetched 534 rows for MSFT in 2023-11.\n",
            "Fetching MSFT data for 2023-12...\n",
            "Successfully fetched 520 rows for MSFT in 2023-12.\n",
            "Fetching MSFT data for 2024-01...\n",
            "Successfully fetched 546 rows for MSFT in 2024-01.\n",
            "Fetching MSFT data for 2024-02...\n",
            "Successfully fetched 520 rows for MSFT in 2024-02.\n",
            "Fetching MSFT data for 2024-03...\n",
            "Successfully fetched 520 rows for MSFT in 2024-03.\n",
            "Fetching MSFT data for 2024-04...\n",
            "Successfully fetched 572 rows for MSFT in 2024-04.\n",
            "Fetching MSFT data for 2024-05...\n",
            "Successfully fetched 572 rows for MSFT in 2024-05.\n",
            "Fetching MSFT data for 2024-06...\n",
            "Successfully fetched 494 rows for MSFT in 2024-06.\n",
            "Fetching MSFT data for 2024-07...\n",
            "Successfully fetched 561 rows for MSFT in 2024-07.\n",
            "Fetching MSFT data for 2024-08...\n",
            "Successfully fetched 572 rows for MSFT in 2024-08.\n",
            "Fetching MSFT data for 2024-09...\n",
            "Successfully fetched 520 rows for MSFT in 2024-09.\n",
            "Fetching MSFT data for 2024-10...\n",
            "Successfully fetched 598 rows for MSFT in 2024-10.\n",
            "Fetching MSFT data for 2024-11...\n",
            "Successfully fetched 510 rows for MSFT in 2024-11.\n",
            "Fetching MSFT data for 2024-12...\n",
            "Successfully fetched 535 rows for MSFT in 2024-12.\n",
            "MSFT data saved to Google Drive as MSFT_data.csv\n"
          ]
        }
      ],
      "source": [
        "import time as t\n",
        "from twelvedata import TDClient\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta, time\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Initialize the client\n",
        "td = TDClient(apikey=api_key)\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def fetch_stock_data(symbol):\n",
        "    # Define the date range\n",
        "    start_date = datetime(2021, 1, 1)\n",
        "    end_date = datetime(2024, 12, 31)\n",
        "\n",
        "    # Define market hours\n",
        "    market_open_time = time(9, 30)  # 9:30 AM\n",
        "    market_close_time = time(16, 0)  # 4:00 PM\n",
        "\n",
        "    all_data = []\n",
        "\n",
        "    # Iterate through each month in the date range\n",
        "    current_date = start_date\n",
        "    while current_date <= end_date:\n",
        "        # Calculate the last day of the current month\n",
        "        if current_date.month == 12:\n",
        "            next_month = current_date.replace(year=current_date.year + 1, month=1, day=1)\n",
        "        else:\n",
        "            next_month = current_date.replace(month=current_date.month + 1, day=1)\n",
        "        last_day_of_month = next_month - timedelta(days=1)\n",
        "\n",
        "        # Define the start and end times for the entire month\n",
        "        month_start = datetime.combine(current_date, market_open_time)\n",
        "        month_end = datetime.combine(last_day_of_month, market_close_time)\n",
        "\n",
        "        # Fetch data for the entire month\n",
        "        try:\n",
        "            print(f\"Fetching {symbol} data for {current_date.strftime('%Y-%m')}...\")\n",
        "            data = td.time_series(\n",
        "                symbol=symbol,\n",
        "                interval=\"15min\",\n",
        "                start_date=month_start.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                end_date=month_end.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                outputsize=5000\n",
        "            ).as_pandas()\n",
        "\n",
        "            if not data.empty:\n",
        "                all_data.append(data)\n",
        "                print(f\"Successfully fetched {len(data)} rows for {symbol} in {current_date.strftime('%Y-%m')}.\")\n",
        "            else:\n",
        "                print(f\"No data returned for {symbol} in {current_date.strftime('%Y-%m')} (market may have been closed).\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching {symbol} data for {current_date.strftime('%Y-%m')}: {e}\")\n",
        "\n",
        "        # Move to the next month\n",
        "        current_date = next_month\n",
        "\n",
        "        # Add delay between requests to avoid hitting API limits\n",
        "        t.sleep(10)  # 10-second delay (6 requests per minute)\n",
        "\n",
        "    # Combine all chunks into a single DataFrame\n",
        "    if all_data:\n",
        "        full_data = pd.concat(all_data)\n",
        "        # Remove duplicates (if any)\n",
        "        full_data = full_data[~full_data.index.duplicated(keep='first')]\n",
        "        # Save to Google Drive\n",
        "        full_data.to_csv(f'/content/drive/My Drive/{symbol}_data.csv', index=True)\n",
        "        print(f\"{symbol} data saved to Google Drive as {symbol}_data.csv\")\n",
        "    else:\n",
        "        print(f\"No data collected for {symbol}\")\n",
        "\n",
        "# Pull data for AAPL and MSFT\n",
        "fetch_stock_data(\"AAPL\")\n",
        "fetch_stock_data(\"MSFT\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dS_65ns2f4HY",
        "outputId": "87daa9ec-c016-4a74-f0ee-3061a8a250f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Attempt 1 failed: No data is available on the specified dates. Try setting different start/end dates.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Fetching AAPL:   0%|          | 0/1457 [00:00<?, ?it/s]\u001b[A\n",
            "Fetching AAPL:   1%|          | 14/1457 [00:00<01:21, 17.71it/s]\u001b[A\n",
            "Fetching AAPL:   2%|▏         | 28/1457 [00:01<01:20, 17.84it/s]\u001b[A\n",
            "Fetching AAPL:   3%|▎         | 42/1457 [00:02<01:19, 17.90it/s]\u001b[A\n",
            "Fetching AAPL:   3%|▎         | 42/1457 [00:20<01:19, 17.90it/s]\u001b[A\n",
            "Fetching AAPL:   4%|▍         | 56/1457 [01:04<41:28,  1.78s/it]\u001b[A\n",
            "Fetching AAPL:   5%|▍         | 70/1457 [01:04<26:39,  1.15s/it]\u001b[A\n",
            "Fetching AAPL:   6%|▌         | 84/1457 [01:05<17:47,  1.29it/s]\u001b[A\n",
            "Fetching AAPL:   7%|▋         | 98/1457 [01:06<12:16,  1.85it/s]\u001b[A\n",
            "Fetching AAPL:   8%|▊         | 112/1457 [01:07<08:41,  2.58it/s]\u001b[A\n",
            "Fetching AAPL:   9%|▊         | 126/1457 [01:07<06:19,  3.51it/s]\u001b[A\n",
            "Fetching AAPL:  10%|▉         | 140/1457 [01:08<04:42,  4.67it/s]\u001b[A\n",
            "Fetching AAPL:  11%|█         | 154/1457 [01:09<03:32,  6.12it/s]\u001b[A\n",
            "Fetching AAPL:  11%|█         | 154/1457 [01:20<03:32,  6.12it/s]\u001b[A\n",
            "Fetching AAPL:  12%|█▏        | 168/1457 [02:11<31:17,  1.46s/it]\u001b[A\n",
            "Fetching AAPL:  12%|█▏        | 182/1457 [02:11<21:53,  1.03s/it]\u001b[A\n",
            "Fetching AAPL:  13%|█▎        | 196/1457 [02:12<15:27,  1.36it/s]\u001b[A\n",
            "Fetching AAPL:  14%|█▍        | 210/1457 [02:13<11:02,  1.88it/s]\u001b[A\n",
            "Fetching AAPL:  15%|█▌        | 224/1457 [02:14<07:58,  2.57it/s]\u001b[A\n",
            "Fetching AAPL:  16%|█▋        | 238/1457 [02:15<05:51,  3.47it/s]\u001b[A\n",
            "Fetching AAPL:  17%|█▋        | 252/1457 [02:15<04:24,  4.56it/s]\u001b[A\n",
            "Fetching AAPL:  18%|█▊        | 266/1457 [02:16<03:22,  5.88it/s]\u001b[A\n",
            "Fetching AAPL:  18%|█▊        | 266/1457 [02:30<03:22,  5.88it/s]\u001b[A\n",
            "Fetching AAPL:  19%|█▉        | 280/1457 [03:18<28:22,  1.45s/it]\u001b[A\n",
            "Fetching AAPL:  20%|██        | 294/1457 [03:19<19:56,  1.03s/it]\u001b[A\n",
            "Fetching AAPL:  21%|██        | 308/1457 [03:20<14:06,  1.36it/s]\u001b[A\n",
            "Fetching AAPL:  22%|██▏       | 322/1457 [03:20<10:01,  1.89it/s]\u001b[A\n",
            "Fetching AAPL:  23%|██▎       | 336/1457 [03:21<07:15,  2.57it/s]\u001b[A\n",
            "Fetching AAPL:  24%|██▍       | 350/1457 [03:22<05:19,  3.46it/s]\u001b[A\n",
            "Fetching AAPL:  25%|██▍       | 364/1457 [03:23<04:00,  4.55it/s]\u001b[A\n",
            "Fetching AAPL:  26%|██▌       | 378/1457 [03:23<03:01,  5.95it/s]\u001b[A\n",
            "Fetching AAPL:  26%|██▌       | 378/1457 [03:40<03:01,  5.95it/s]\u001b[A\n",
            "Fetching AAPL:  27%|██▋       | 392/1457 [04:25<25:36,  1.44s/it]\u001b[A\n",
            "Fetching AAPL:  28%|██▊       | 406/1457 [04:26<17:59,  1.03s/it]\u001b[A\n",
            "Fetching AAPL:  29%|██▉       | 420/1457 [04:27<12:43,  1.36it/s]\u001b[A\n",
            "Fetching AAPL:  30%|██▉       | 434/1457 [04:27<09:02,  1.88it/s]\u001b[A\n",
            "Fetching AAPL:  31%|███       | 448/1457 [04:28<06:32,  2.57it/s]\u001b[A\n",
            "Fetching AAPL:  32%|███▏      | 462/1457 [04:29<04:47,  3.46it/s]\u001b[A\n",
            "Fetching AAPL:  33%|███▎      | 476/1457 [04:30<03:34,  4.56it/s]\u001b[A\n",
            "Fetching AAPL:  34%|███▎      | 490/1457 [04:31<02:45,  5.86it/s]\u001b[A\n",
            "Fetching AAPL:  34%|███▎      | 490/1457 [04:50<02:45,  5.86it/s]\u001b[A\n",
            "Fetching AAPL:  35%|███▍      | 504/1457 [05:32<22:56,  1.44s/it]\u001b[A\n",
            "Fetching AAPL:  36%|███▌      | 518/1457 [05:33<16:06,  1.03s/it]\u001b[A\n",
            "Fetching AAPL:  37%|███▋      | 532/1457 [05:34<11:22,  1.36it/s]\u001b[A\n",
            "Fetching AAPL:  37%|███▋      | 546/1457 [05:35<08:06,  1.87it/s]\u001b[A\n",
            "Fetching AAPL:  38%|███▊      | 560/1457 [05:36<05:51,  2.55it/s]\u001b[A\n",
            "Fetching AAPL:  39%|███▉      | 574/1457 [05:37<04:17,  3.43it/s]\u001b[A\n",
            "Fetching AAPL:  40%|████      | 588/1457 [05:37<03:10,  4.56it/s]\u001b[A\n",
            "Fetching AAPL:  41%|████▏     | 602/1457 [05:38<02:25,  5.87it/s]\u001b[A\n",
            "Fetching AAPL:  41%|████▏     | 602/1457 [05:50<02:25,  5.87it/s]\u001b[A\n",
            "Fetching AAPL:  42%|████▏     | 616/1457 [06:40<20:14,  1.44s/it]\u001b[A\n",
            "Fetching AAPL:  43%|████▎     | 630/1457 [06:41<14:08,  1.03s/it]\u001b[A\n",
            "Fetching AAPL:  44%|████▍     | 644/1457 [06:41<09:56,  1.36it/s]\u001b[A\n",
            "Fetching AAPL:  45%|████▌     | 658/1457 [06:42<07:03,  1.89it/s]\u001b[A\n",
            "Fetching AAPL:  46%|████▌     | 672/1457 [06:43<05:02,  2.59it/s]\u001b[A\n",
            "Fetching AAPL:  47%|████▋     | 686/1457 [06:44<03:41,  3.47it/s]\u001b[A\n",
            "Fetching AAPL:  48%|████▊     | 700/1457 [06:44<02:45,  4.58it/s]\u001b[A\n",
            "Fetching AAPL:  49%|████▉     | 714/1457 [06:45<02:06,  5.87it/s]\u001b[A\n",
            "Fetching AAPL:  49%|████▉     | 714/1457 [07:00<02:06,  5.87it/s]\u001b[A\n",
            "Fetching AAPL:  50%|████▉     | 728/1457 [07:47<17:32,  1.44s/it]\u001b[A\n",
            "Fetching AAPL:  51%|█████     | 742/1457 [07:48<12:15,  1.03s/it]\u001b[A\n",
            "Fetching AAPL:  52%|█████▏    | 756/1457 [07:49<08:36,  1.36it/s]\u001b[A\n",
            "Fetching AAPL:  53%|█████▎    | 770/1457 [07:49<06:06,  1.87it/s]\u001b[A\n",
            "Fetching AAPL:  54%|█████▍    | 784/1457 [07:50<04:22,  2.56it/s]\u001b[A\n",
            "Fetching AAPL:  55%|█████▍    | 798/1457 [07:51<03:11,  3.44it/s]\u001b[A\n",
            "Fetching AAPL:  56%|█████▌    | 812/1457 [07:52<02:20,  4.59it/s]\u001b[A\n",
            "Fetching AAPL:  57%|█████▋    | 826/1457 [07:53<01:47,  5.88it/s]\u001b[A\n",
            "Fetching AAPL:  57%|█████▋    | 826/1457 [08:10<01:47,  5.88it/s]\u001b[A\n",
            "Fetching AAPL:  58%|█████▊    | 840/1457 [08:54<14:50,  1.44s/it]\u001b[A\n",
            "Fetching AAPL:  59%|█████▊    | 854/1457 [08:55<10:19,  1.03s/it]\u001b[A\n",
            "Fetching AAPL:  60%|█████▉    | 868/1457 [08:56<07:12,  1.36it/s]\u001b[A\n",
            "Fetching AAPL:  61%|██████    | 882/1457 [08:57<05:05,  1.88it/s]\u001b[A\n",
            "Fetching AAPL:  61%|██████▏   | 896/1457 [08:57<03:36,  2.59it/s]\u001b[A\n",
            "Fetching AAPL:  62%|██████▏   | 910/1457 [08:58<02:37,  3.48it/s]\u001b[A\n",
            "Fetching AAPL:  63%|██████▎   | 924/1457 [08:59<01:56,  4.57it/s]\u001b[A\n",
            "Fetching AAPL:  64%|██████▍   | 938/1457 [09:00<01:28,  5.86it/s]\u001b[A\n",
            "Fetching AAPL:  64%|██████▍   | 938/1457 [09:10<01:28,  5.86it/s]\u001b[A\n",
            "Fetching AAPL:  65%|██████▌   | 952/1457 [10:02<12:09,  1.44s/it]\u001b[A\n",
            "Fetching AAPL:  66%|██████▋   | 966/1457 [10:02<08:24,  1.03s/it]\u001b[A\n",
            "Fetching AAPL:  67%|██████▋   | 980/1457 [10:03<05:51,  1.36it/s]\u001b[A\n",
            "Fetching AAPL:  68%|██████▊   | 994/1457 [10:04<04:05,  1.89it/s]\u001b[A\n",
            "Fetching AAPL:  69%|██████▉   | 1008/1457 [10:05<02:53,  2.59it/s]\u001b[A\n",
            "Fetching AAPL:  70%|███████   | 1022/1457 [10:05<02:05,  3.47it/s]\u001b[A\n",
            "Fetching AAPL:  71%|███████   | 1036/1457 [10:06<01:30,  4.63it/s]\u001b[A\n",
            "Fetching AAPL:  72%|███████▏  | 1050/1457 [10:07<01:07,  6.04it/s]\u001b[A\n",
            "Fetching AAPL:  72%|███████▏  | 1050/1457 [10:20<01:07,  6.04it/s]\u001b[A\n",
            "Fetching AAPL:  73%|███████▎  | 1064/1457 [11:09<09:26,  1.44s/it]\u001b[A\n",
            "Fetching AAPL:  74%|███████▍  | 1078/1457 [11:09<06:28,  1.03s/it]\u001b[A\n",
            "Fetching AAPL:  75%|███████▍  | 1092/1457 [11:10<04:27,  1.37it/s]\u001b[A\n",
            "Fetching AAPL:  76%|███████▌  | 1106/1457 [11:11<03:04,  1.90it/s]\u001b[A\n",
            "Fetching AAPL:  77%|███████▋  | 1120/1457 [11:12<02:16,  2.47it/s]\u001b[A\n",
            "Fetching AAPL:  78%|███████▊  | 1134/1457 [11:13<01:37,  3.32it/s]\u001b[A\n",
            "Fetching AAPL:  79%|███████▉  | 1148/1457 [11:14<01:09,  4.43it/s]\u001b[A\n",
            "Fetching AAPL:  80%|███████▉  | 1162/1457 [11:15<00:51,  5.72it/s]\u001b[A\n",
            "Fetching AAPL:  80%|███████▉  | 1162/1457 [11:30<00:51,  5.72it/s]\u001b[A\n",
            "Fetching AAPL:  81%|████████  | 1176/1457 [12:16<06:45,  1.44s/it]\u001b[A\n",
            "Fetching AAPL:  82%|████████▏ | 1190/1457 [12:17<04:34,  1.03s/it]\u001b[A\n",
            "Fetching AAPL:  83%|████████▎ | 1204/1457 [12:18<03:05,  1.36it/s]\u001b[A\n",
            "Fetching AAPL:  84%|████████▎ | 1218/1457 [12:19<02:06,  1.89it/s]\u001b[A\n",
            "Fetching AAPL:  85%|████████▍ | 1232/1457 [12:19<01:26,  2.60it/s]\u001b[A\n",
            "Fetching AAPL:  86%|████████▌ | 1246/1457 [12:20<01:00,  3.50it/s]\u001b[A\n",
            "Fetching AAPL:  86%|████████▋ | 1260/1457 [12:21<00:42,  4.60it/s]\u001b[A\n",
            "Fetching AAPL:  87%|████████▋ | 1274/1457 [12:21<00:30,  6.01it/s]\u001b[A\n",
            "Fetching AAPL:  87%|████████▋ | 1274/1457 [12:40<00:30,  6.01it/s]\u001b[A\n",
            "Fetching AAPL:  88%|████████▊ | 1288/1457 [13:23<04:03,  1.44s/it]\u001b[A\n",
            "Fetching AAPL:  89%|████████▉ | 1302/1457 [13:24<02:38,  1.02s/it]\u001b[A\n",
            "Fetching AAPL:  90%|█████████ | 1316/1457 [13:25<01:43,  1.36it/s]\u001b[A\n",
            "Fetching AAPL:  91%|█████████▏| 1330/1457 [13:26<01:07,  1.88it/s]\u001b[A\n",
            "Fetching AAPL:  92%|█████████▏| 1344/1457 [13:26<00:44,  2.56it/s]\u001b[A\n",
            "Fetching AAPL:  93%|█████████▎| 1358/1457 [13:27<00:28,  3.45it/s]\u001b[A\n",
            "Fetching AAPL:  94%|█████████▍| 1372/1457 [13:28<00:18,  4.58it/s]\u001b[A\n",
            "Fetching AAPL:  95%|█████████▌| 1386/1457 [13:29<00:11,  5.97it/s]\u001b[A\n",
            "Fetching AAPL:  95%|█████████▌| 1386/1457 [13:40<00:11,  5.97it/s]\u001b[A\n",
            "Fetching AAPL:  96%|█████████▌| 1400/1457 [14:30<01:22,  1.44s/it]\u001b[A\n",
            "Fetching AAPL:  97%|█████████▋| 1414/1457 [14:31<00:43,  1.02s/it]\u001b[A\n",
            "Fetching AAPL:  98%|█████████▊| 1428/1457 [14:32<00:21,  1.37it/s]\u001b[A\n",
            "Fetching AAPL:  99%|█████████▉| 1442/1457 [14:33<00:07,  1.89it/s]\u001b[A\n",
            "Fetching AAPL: 100%|█████████▉| 1456/1457 [14:33<00:00,  2.60it/s]\u001b[A\n",
            "Fetching AAPL: 1470it [14:34,  3.49it/s]                          \u001b[A\n",
            "Fetching AAPL: 1484it [14:35,  4.65it/s]\u001b[A\n",
            "Fetching AAPL: 1498it [14:36,  5.97it/s]\u001b[A\n",
            "Fetching AAPL: 1498it [14:50,  5.97it/s]\u001b[A\n",
            "Fetching AAPL: 1512it [15:37,  1.44s/it]\u001b[A"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-d5f9139514f9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;31m# Fetch data for both stocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m \u001b[0mfetch_stock_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AAPL\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-23-d5f9139514f9>\u001b[0m in \u001b[0;36mfetch_stock_data\u001b[0;34m(symbol)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mchunk_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_dt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdays\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_days\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_dt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         chunk_data = fetch_chunk(\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0msymbol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_dt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarket_open\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-d5f9139514f9>\u001b[0m in \u001b[0;36mfetch_chunk\u001b[0;34m(symbol, start_dt, end_dt, retries)\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0mend_date\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_dt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%Y-%m-%d %H:%M:%S\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0moutputsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDATA_LIMIT_PER_REQUEST\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             ).as_pandas()\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mapi_usage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m  \u001b[0;31m# Increment usage counter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/twelvedata/time_series.py\u001b[0m in \u001b[0;36mas_pandas\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprice_endpoint_enabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprice_endpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/twelvedata/mixins.py\u001b[0m in \u001b[0;36mas_pandas\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"as_json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"is_batch\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_batch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_collection_to_pandas_multi_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/twelvedata/mixins.py\u001b[0m in \u001b[0;36mas_json\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mAsJsonMixin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mas_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"JSON\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mjson\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'is_batch'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_batch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/twelvedata/endpoints.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, format, debug)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mbuild_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/twelvedata/http_client.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, relative_url, *args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"params\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelative_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Is_batch'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Is_batch'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'true'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0;34m'Content-Type'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Content-Type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'text/csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"allow_redirects\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GET\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    668\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    788\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;31m# Receive the response from the server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0;31m# Get the response from http.client.HTTPConnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m         \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1393\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1395\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1396\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1312\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1314\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1166\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1167\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import time as t\n",
        "from twelvedata import TDClient\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta, time\n",
        "import os\n",
        "from google.colab import drive\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Initialize TwelveData client\n",
        "td = TDClient(apikey=api_key)\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# API constraints\n",
        "MAX_DAILY_REQUESTS = 800\n",
        "MAX_MINUTE_REQUESTS = 8\n",
        "DATA_LIMIT_PER_REQUEST = 5000\n",
        "\n",
        "# Track API usage\n",
        "api_usage = 452  # Update this dynamically if needed\n",
        "\n",
        "def fetch_chunk(symbol, start_dt, end_dt, retries=3):\n",
        "    \"\"\"Fetch data chunk with retry logic, ensuring API constraints.\"\"\"\n",
        "    global api_usage\n",
        "\n",
        "    if api_usage >= MAX_DAILY_REQUESTS:\n",
        "        print(\"Daily API limit reached. Stopping execution.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            data = td.time_series(\n",
        "                symbol=symbol,\n",
        "                interval=\"15min\",\n",
        "                start_date=start_dt.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                end_date=end_dt.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                outputsize=DATA_LIMIT_PER_REQUEST\n",
        "            ).as_pandas()\n",
        "\n",
        "            api_usage += 1  # Increment usage counter\n",
        "            return data[data.index >= start_dt]  # Trim overlap\n",
        "        except Exception as e:\n",
        "            print(f\"Attempt {attempt+1} failed: {e}\")\n",
        "            t.sleep(min(8 * (attempt + 1), 60))  # Exponential backoff, capped at 60s\n",
        "\n",
        "    return pd.DataFrame()\n",
        "\n",
        "def fetch_stock_data(symbol):\n",
        "    \"\"\"Fetch full stock data while respecting API rate limits.\"\"\"\n",
        "    global api_usage\n",
        "\n",
        "    market_open = time(9, 30)\n",
        "    market_close = time(16, 0)\n",
        "    chunk_days = 14\n",
        "    overlap = timedelta(minutes=30)\n",
        "\n",
        "    all_data = []\n",
        "    current_dt = datetime(2021, 1, 1)\n",
        "    end_dt = datetime(2024, 12, 31)\n",
        "\n",
        "    # Find first available trading day\n",
        "    while current_dt <= end_dt:\n",
        "        if current_dt.weekday() >= 5:\n",
        "            current_dt += timedelta(days=1)\n",
        "            continue\n",
        "\n",
        "        test_data = fetch_chunk(\n",
        "            symbol,\n",
        "            datetime.combine(current_dt, market_open),\n",
        "            datetime.combine(current_dt, market_close),\n",
        "            retries=1\n",
        "        )\n",
        "        if not test_data.empty:\n",
        "            break\n",
        "        current_dt += timedelta(days=1)\n",
        "\n",
        "    pbar = tqdm(total=(end_dt - current_dt).days, desc=f\"Fetching {symbol}\")\n",
        "\n",
        "    while current_dt <= end_dt and api_usage < MAX_DAILY_REQUESTS:\n",
        "        chunk_end = min(current_dt + timedelta(days=chunk_days), end_dt)\n",
        "\n",
        "        chunk_data = fetch_chunk(\n",
        "            symbol,\n",
        "            datetime.combine(current_dt, market_open),\n",
        "            datetime.combine(chunk_end, market_close) + overlap,\n",
        "        )\n",
        "\n",
        "        if not chunk_data.empty:\n",
        "            all_data.append(chunk_data)\n",
        "            pbar.update((chunk_end - current_dt).days)\n",
        "            current_dt = chunk_data.index[-1].to_pydatetime() - overlap\n",
        "        else:\n",
        "            current_dt += timedelta(days=1)\n",
        "\n",
        "        # Skip weekends efficiently\n",
        "        if current_dt.weekday() >= 5:\n",
        "            current_dt += timedelta(days=7 - current_dt.weekday())\n",
        "\n",
        "        # Ensure we don't exceed per-minute limit\n",
        "        if api_usage % MAX_MINUTE_REQUESTS == 0:\n",
        "            t.sleep(61)  # Sleep slightly over 1 min to reset API window\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "    if all_data:\n",
        "        final_data = pd.concat(all_data)\n",
        "        final_data = final_data[~final_data.index.duplicated(keep='last')]\n",
        "        final_data.sort_index(inplace=True)\n",
        "        save_path = f'/content/drive/My Drive/{symbol}_data.csv'\n",
        "        final_data.to_csv(save_path)\n",
        "        print(f\"Saved {len(final_data)} records for {symbol} at {save_path}\")\n",
        "\n",
        "# Fetch data for both stocks\n",
        "fetch_stock_data(\"AAPL\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## [2 points] How many API credits would be required for downloading 4 years worth of data at 15-minute intervals, per stock? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## [3 points] The API applies a rate limit. How long would it take to download 4 years worth of data at 15-minute intervals, per stock?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## [20 points for setting up data collection] We’ll be working with just 2 stocks: AAPL and MSFT. The need for speed in a price API makes it inevitable that a small percentage of data elements has errors in it. To compensate for such errors, a “good enough” strategy is to request data for overlapping periods, say fetching 30 minutes of data every 15 minutes – replacing the most recently arrived data with a corrected version."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below is the setup for using the api to gather the stock data. The full script I ran to gather the data using the API is under the following question. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install twelvedata\n",
        "import twelvedata\n",
        "\n",
        "from google.colab import drive\n",
        "import sys\n",
        "drive.mount(\"/content/drive/\", force_remount=True)\n",
        "sys.path.append(\"/content/drive/My Drive/Colab Notebooks\")\n",
        "from key import twelveDataKey as api_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## [25 points for collecting data] Technically, final testing can only be done while the market is open: 9:30 am to 4:00 pm on weekdays. In a compromise to accommodate everyone’s schedules we will pull the stock data of each stock and gather up at least 4 years of stock price data (January 2021-December 2024) and use it as the basis of our work.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below is the script I ran for getting the data and storing them in csv files. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time as t\n",
        "from twelvedata import TDClient\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta, time\n",
        "import os\n",
        "from google.colab import drive\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Initialize TwelveData client\n",
        "td = TDClient(apikey=api_key)\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# API constraints\n",
        "MAX_DAILY_REQUESTS = 800\n",
        "MAX_MINUTE_REQUESTS = 8\n",
        "DATA_LIMIT_PER_REQUEST = 5000\n",
        "\n",
        "# Track API usage\n",
        "api_usage = 452  # Update this dynamically if needed\n",
        "\n",
        "def fetch_chunk(symbol, start_dt, end_dt, retries=3):\n",
        "    \"\"\"Fetch data chunk with retry logic, ensuring API constraints.\"\"\"\n",
        "    global api_usage\n",
        "\n",
        "    if api_usage >= MAX_DAILY_REQUESTS:\n",
        "        print(\"Daily API limit reached. Stopping execution.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            data = td.time_series(\n",
        "                symbol=symbol,\n",
        "                interval=\"15min\",\n",
        "                start_date=start_dt.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                end_date=end_dt.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                outputsize=DATA_LIMIT_PER_REQUEST\n",
        "            ).as_pandas()\n",
        "\n",
        "            api_usage += 1  # Increment usage counter\n",
        "            return data[data.index >= start_dt]  # Trim overlap\n",
        "        except Exception as e:\n",
        "            print(f\"Attempt {attempt+1} failed: {e}\")\n",
        "            t.sleep(min(8 * (attempt + 1), 60))  # Exponential backoff, capped at 60s\n",
        "\n",
        "    return pd.DataFrame()\n",
        "\n",
        "def fetch_stock_data(symbol):\n",
        "    \"\"\"Fetch full stock data while respecting API rate limits.\"\"\"\n",
        "    global api_usage\n",
        "\n",
        "    market_open = time(9, 30)\n",
        "    market_close = time(16, 0)\n",
        "    chunk_days = 14\n",
        "    overlap = timedelta(minutes=30)\n",
        "\n",
        "    all_data = []\n",
        "    current_dt = datetime(2021, 1, 1)\n",
        "    end_dt = datetime(2024, 12, 31)\n",
        "\n",
        "    # Find first available trading day\n",
        "    while current_dt <= end_dt:\n",
        "        if current_dt.weekday() >= 5:\n",
        "            current_dt += timedelta(days=1)\n",
        "            continue\n",
        "\n",
        "        test_data = fetch_chunk(\n",
        "            symbol,\n",
        "            datetime.combine(current_dt, market_open),\n",
        "            datetime.combine(current_dt, market_close),\n",
        "            retries=1\n",
        "        )\n",
        "        if not test_data.empty:\n",
        "            break\n",
        "        current_dt += timedelta(days=1)\n",
        "\n",
        "    pbar = tqdm(total=(end_dt - current_dt).days, desc=f\"Fetching {symbol}\")\n",
        "\n",
        "    while current_dt <= end_dt and api_usage < MAX_DAILY_REQUESTS:\n",
        "        chunk_end = min(current_dt + timedelta(days=chunk_days), end_dt)\n",
        "\n",
        "        chunk_data = fetch_chunk(\n",
        "            symbol,\n",
        "            datetime.combine(current_dt, market_open),\n",
        "            datetime.combine(chunk_end, market_close) + overlap,\n",
        "        )\n",
        "\n",
        "        if not chunk_data.empty:\n",
        "            all_data.append(chunk_data)\n",
        "            pbar.update((chunk_end - current_dt).days)\n",
        "            current_dt = chunk_data.index[-1].to_pydatetime() - overlap\n",
        "        else:\n",
        "            current_dt += timedelta(days=1)\n",
        "\n",
        "        # Skip weekends efficiently\n",
        "        if current_dt.weekday() >= 5:\n",
        "            current_dt += timedelta(days=7 - current_dt.weekday())\n",
        "\n",
        "        # Ensure we don't exceed per-minute limit\n",
        "        if api_usage % MAX_MINUTE_REQUESTS == 0:\n",
        "            t.sleep(61)  # Sleep slightly over 1 min to reset API window\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "    if all_data:\n",
        "        final_data = pd.concat(all_data)\n",
        "        final_data = final_data[~final_data.index.duplicated(keep='last')]\n",
        "        final_data.sort_index(inplace=True)\n",
        "        save_path = f'/content/drive/My Drive/{symbol}_data.csv'\n",
        "        final_data.to_csv(save_path)\n",
        "        print(f\"Saved {len(final_data)} records for {symbol} at {save_path}\")\n",
        "\n",
        "# Fetch data for both stocks\n",
        "fetch_stock_data(\"AAPL\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## [25 points] Create a program new-stock-price-feeder.py that uses a more modern API (e.g., twelveData) instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, avg, lag, when, lit, input_file_name, round\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"StockTradingStrategy\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Read all CSV files from HDFS directory\n",
        "stock_df = spark.read.csv(\n",
        "    \"hdfs:///user/root/stock_data/\",\n",
        "    header=True,\n",
        "    inferSchema=True\n",
        ")\n",
        "\n",
        "# Add symbol column based on filename\n",
        "stock_df = stock_df.withColumn(\n",
        "    \"filename\", input_file_name()\n",
        ").withColumn(\n",
        "    \"symbol\",\n",
        "    when(\n",
        "        col(\"filename\").contains(\"AAPL\"), \"AAPL\"\n",
        "    ).when(\n",
        "        col(\"filename\").contains(\"MSFT\"), \"MSFT\"\n",
        "    ).otherwise(\"UNKNOWN\")\n",
        ").drop(\"filename\")\n",
        "\n",
        "# Verify we have both symbols\n",
        "print(\"Distinct symbols found:\")\n",
        "stock_df.select(\"symbol\").distinct().show()\n",
        "\n",
        "# Define window specs for moving averages\n",
        "window_10 = Window.partitionBy(\"symbol\").orderBy(\"datetime\").rowsBetween(-9, 0)\n",
        "window_40 = Window.partitionBy(\"symbol\").orderBy(\"datetime\").rowsBetween(-39, 0)\n",
        "\n",
        "# Calculate moving averages\n",
        "stock_with_ma = stock_df.withColumn(\"ma_10\", avg(\"close\").over(window_10)) \\\n",
        "                       .withColumn(\"ma_40\", avg(\"close\").over(window_40))\n",
        "\n",
        "# Function to generate trading signals with share calculations\n",
        "def generate_trades(df):\n",
        "    window_spec = Window.partitionBy(\"symbol\").orderBy(\"datetime\")\n",
        "    return df.withColumn(\"prev_ma_10\", lag(\"ma_10\", 1).over(window_spec)) \\\n",
        "            .withColumn(\"prev_ma_40\", lag(\"ma_40\", 1).over(window_spec)) \\\n",
        "            .withColumn(\"signal\",\n",
        "                when(\n",
        "                    (col(\"prev_ma_10\") <= col(\"prev_ma_40\")) & \n",
        "                    (col(\"ma_10\") > col(\"ma_40\")), \"buy\"\n",
        "                ).when(\n",
        "                    (col(\"prev_ma_10\") >= col(\"prev_ma_40\")) & \n",
        "                    (col(\"ma_10\") < col(\"ma_40\")), \"sell\"\n",
        "                ).otherwise(None)\n",
        "            ) \\\n",
        "            .withColumn(\"shares\", round(lit(100000)/col(\"close\"))) \\\n",
        "            .filter(col(\"signal\").isNotNull()) \\\n",
        "            .select(\n",
        "                \"datetime\",\n",
        "                \"symbol\",\n",
        "                \"close\",\n",
        "                \"signal\",\n",
        "                \"shares\"\n",
        "            )\n",
        "\n",
        "# Generate trades\n",
        "trades_df = generate_trades(stock_with_ma)\n",
        "\n",
        "# Format output as requested\n",
        "formatted_trades = trades_df.rdd.map(\n",
        "    lambda row: f\"({row['datetime']} {row['signal']} {row['symbol']}) - {row['shares']} shares @ ${row['close']:.2f}\"\n",
        ").collect()\n",
        "\n",
        "# Save results to HDFS\n",
        "trades_df.write.mode(\"overwrite\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .csv(\"hdfs:///user/root/stock_trades/\")\n",
        "\n",
        "# Print formatted trades\n",
        "print(\"\\nTrading Recommendations ($100K per trade):\")\n",
        "for trade in formatted_trades:\n",
        "    print(trade)\n",
        "\n",
        "# Sample output:\n",
        "# (2021-01-04 11:30:00 buy AAPL) - 758 shares @ $132.06\n",
        "# (2021-01-05 14:45:00 sell AAPL) - 742 shares @ $134.75\n",
        "# (2021-01-04 10:15:00 buy MSFT) - 459 shares @ $217.89\n",
        "# (2021-01-05 15:30:00 sell MSFT) - 447 shares @ $223.71"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ## [10 pts] Within Spark, filter the incoming date to create aaplPrice and msftPrice streams. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The code is shown above, but the excerpt is pasted below as well. This chunk of code grabs the csv that is stored in hdfs from the earlier steps. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read all CSV files from HDFS directory\n",
        "stock_df = spark.read.csv(\n",
        "    \"hdfs:///user/root/stock_data/\",\n",
        "    header=True,\n",
        "    inferSchema=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## [10 pts] From aaplPrice produce two other streams aapl10Day, aapl40Day. Both of these streams and their comparison to generate buy/sell signals are not shown in the diagram above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The code for this is shown above in the entire code chunk. The excerpt is pasted below as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "window_10 = Window.partitionBy(\"symbol\").orderBy(\"datetime\").rowsBetween(-9, 0)\n",
        "window_40 = Window.partitionBy(\"symbol\").orderBy(\"datetime\").rowsBetween(-39, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## [10 pts] From msftPrice produce two more streams msft10Day and msft40Day ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " The code is generalized as shown above, so the code for this is the same as for AAPL. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## [20 pts]. Compare the two moving averages (10-day MA and the 40-day MA) to indicate buy and sell signals . Your output should be of the form [( <datetime> buy <symbol>), ( <datetime> sell <symbol>), etc]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Example output from AAPL part of downloaded csv file:\n",
        "\n",
        "        datetime\tsymbol\tclose\tsignal\tshares\n",
        "        2021-01-04T12:00:00.000Z\tAAPL\t127.41\tsell\t785\n",
        "        2021-01-05T10:00:00.000Z\tAAPL\t131.4893\tbuy\t761\n",
        "        2021-01-06T10:15:00.000Z\tAAPL\t128.9501\tsell\t775\n",
        "        2021-01-07T11:30:00.000Z\tAAPL\t130.705\tbuy\t765\n",
        "        2021-01-11T10:00:00.000Z\tAAPL\t129.69\tsell\t771\n",
        "        2021-01-13T10:00:00.000Z\tAAPL\t129.82001\tbuy\t770\n",
        "        2021-01-14T11:30:00.000Z\tAAPL\t130.21001\tsell\t768\n",
        "        2021-01-19T14:45:00.000Z\tAAPL\t127.89\tbuy\t782\n",
        "        2021-01-26T12:15:00.000Z\tAAPL\t142.405\tsell\t702\n",
        "        2021-01-26T13:45:00.000Z\tAAPL\t142.965\tbuy\t699\n",
        "        2021-01-27T15:15:00.000Z\tAAPL\t141.64999\tsell\t706"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Example output from MSFT part of csv file:\n",
        "\n",
        "        2021-01-05T14:45:00.000Z\tMSFT\t217.64\tbuy\t459\n",
        "        2021-01-06T09:45:00.000Z\tMSFT\t213.2406\tsell\t469\n",
        "        2021-01-07T11:00:00.000Z\tMSFT\t217.64\tbuy\t459\n",
        "        2021-01-08T13:15:00.000Z\tMSFT\t217.75999\tsell\t459\n",
        "        2021-01-08T15:15:00.000Z\tMSFT\t219.55\tbuy\t455\n",
        "        2021-01-11T11:15:00.000Z\tMSFT\t218.08\tsell\t459\n",
        "        2021-01-13T11:45:00.000Z\tMSFT\t215.95\tbuy\t463\n",
        "        2021-01-14T11:45:00.000Z\tMSFT\t214.6358\tsell\t466\n",
        "        2021-01-19T10:00:00.000Z\tMSFT\t214.50999\tbuy\t466\n",
        "        2021-01-19T10:30:00.000Z\tMSFT\t213.49001\tsell\t468\n",
        "        2021-01-19T10:45:00.000Z\tMSFT\t213.87\tbuy\t468\n",
        "        2021-01-19T11:00:00.000Z\tMSFT\t212.7641\tsell\t470\n",
        "        2021-01-19T11:45:00.000Z\tMSFT\t213.6147\tbuy\t468\n",
        "        2021-01-22T15:45:00.000Z\tMSFT\t225.95\tsell\t443\n",
        "        2021-01-25T10:30:00.000Z\tMSFT\t228.48\tbuy\t438\n",
        "        2021-01-25T10:45:00.000Z\tMSFT\t226.18401\tsell\t442\n",
        "        2021-01-25T14:00:00.000Z\tMSFT\t228.23\tbuy\t438"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see above, the csv file is formatted by datetime, stock symbol, close price, buy/sell signal, and then finally the number of shares, assuming you are buying or selling 100,000 in stocks. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
