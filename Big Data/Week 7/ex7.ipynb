{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xwKPkm7bkxJW",
        "outputId": "4a9199f0-c73a-41b9-bafe-9390e74701f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Fetching AAPL data for 2021-01...\n",
            "Successfully fetched 494 rows for AAPL in 2021-01.\n",
            "Fetching AAPL data for 2021-02...\n",
            "Successfully fetched 494 rows for AAPL in 2021-02.\n",
            "Fetching AAPL data for 2021-03...\n",
            "Successfully fetched 598 rows for AAPL in 2021-03.\n",
            "Fetching AAPL data for 2021-04...\n",
            "Successfully fetched 546 rows for AAPL in 2021-04.\n",
            "Fetching AAPL data for 2021-05...\n",
            "Successfully fetched 520 rows for AAPL in 2021-05.\n",
            "Fetching AAPL data for 2021-06...\n",
            "Successfully fetched 572 rows for AAPL in 2021-06.\n",
            "Fetching AAPL data for 2021-07...\n",
            "Successfully fetched 546 rows for AAPL in 2021-07.\n",
            "Fetching AAPL data for 2021-08...\n",
            "Successfully fetched 572 rows for AAPL in 2021-08.\n",
            "Fetching AAPL data for 2021-09...\n",
            "Successfully fetched 546 rows for AAPL in 2021-09.\n",
            "Fetching AAPL data for 2021-10...\n",
            "Successfully fetched 546 rows for AAPL in 2021-10.\n",
            "Fetching AAPL data for 2021-11...\n",
            "Successfully fetched 546 rows for AAPL in 2021-11.\n",
            "Fetching AAPL data for 2021-12...\n",
            "Successfully fetched 572 rows for AAPL in 2021-12.\n",
            "Fetching AAPL data for 2022-01...\n",
            "Successfully fetched 520 rows for AAPL in 2022-01.\n",
            "Fetching AAPL data for 2022-02...\n",
            "Successfully fetched 494 rows for AAPL in 2022-02.\n",
            "Fetching AAPL data for 2022-03...\n",
            "Successfully fetched 598 rows for AAPL in 2022-03.\n",
            "Fetching AAPL data for 2022-04...\n",
            "Successfully fetched 520 rows for AAPL in 2022-04.\n",
            "Fetching AAPL data for 2022-05...\n",
            "Successfully fetched 546 rows for AAPL in 2022-05.\n",
            "Fetching AAPL data for 2022-06...\n",
            "Successfully fetched 546 rows for AAPL in 2022-06.\n",
            "Fetching AAPL data for 2022-07...\n",
            "Successfully fetched 520 rows for AAPL in 2022-07.\n",
            "Fetching AAPL data for 2022-08...\n",
            "Successfully fetched 598 rows for AAPL in 2022-08.\n",
            "Fetching AAPL data for 2022-09...\n",
            "Successfully fetched 546 rows for AAPL in 2022-09.\n",
            "Fetching AAPL data for 2022-10...\n",
            "Successfully fetched 546 rows for AAPL in 2022-10.\n",
            "Fetching AAPL data for 2022-11...\n",
            "Successfully fetched 534 rows for AAPL in 2022-11.\n",
            "Fetching AAPL data for 2022-12...\n",
            "Successfully fetched 546 rows for AAPL in 2022-12.\n",
            "Fetching AAPL data for 2023-01...\n",
            "Successfully fetched 520 rows for AAPL in 2023-01.\n",
            "Fetching AAPL data for 2023-02...\n",
            "Successfully fetched 494 rows for AAPL in 2023-02.\n",
            "Fetching AAPL data for 2023-03...\n",
            "Successfully fetched 597 rows for AAPL in 2023-03.\n",
            "Fetching AAPL data for 2023-04...\n",
            "Successfully fetched 494 rows for AAPL in 2023-04.\n",
            "Fetching AAPL data for 2023-05...\n",
            "Successfully fetched 572 rows for AAPL in 2023-05.\n",
            "Fetching AAPL data for 2023-06...\n",
            "Successfully fetched 546 rows for AAPL in 2023-06.\n",
            "Fetching AAPL data for 2023-07...\n",
            "Successfully fetched 520 rows for AAPL in 2023-07.\n",
            "Fetching AAPL data for 2023-08...\n",
            "Successfully fetched 598 rows for AAPL in 2023-08.\n",
            "Fetching AAPL data for 2023-09...\n",
            "Successfully fetched 520 rows for AAPL in 2023-09.\n",
            "Fetching AAPL data for 2023-10...\n",
            "Successfully fetched 572 rows for AAPL in 2023-10.\n",
            "Fetching AAPL data for 2023-11...\n",
            "Successfully fetched 534 rows for AAPL in 2023-11.\n",
            "Fetching AAPL data for 2023-12...\n",
            "Successfully fetched 520 rows for AAPL in 2023-12.\n",
            "Fetching AAPL data for 2024-01...\n",
            "Successfully fetched 546 rows for AAPL in 2024-01.\n",
            "Fetching AAPL data for 2024-02...\n",
            "Successfully fetched 520 rows for AAPL in 2024-02.\n",
            "Fetching AAPL data for 2024-03...\n",
            "Successfully fetched 520 rows for AAPL in 2024-03.\n",
            "Fetching AAPL data for 2024-04...\n",
            "Successfully fetched 572 rows for AAPL in 2024-04.\n",
            "Fetching AAPL data for 2024-05...\n",
            "Successfully fetched 572 rows for AAPL in 2024-05.\n",
            "Fetching AAPL data for 2024-06...\n",
            "Successfully fetched 494 rows for AAPL in 2024-06.\n",
            "Fetching AAPL data for 2024-07...\n",
            "Successfully fetched 561 rows for AAPL in 2024-07.\n",
            "Fetching AAPL data for 2024-08...\n",
            "Successfully fetched 572 rows for AAPL in 2024-08.\n",
            "Fetching AAPL data for 2024-09...\n",
            "Successfully fetched 520 rows for AAPL in 2024-09.\n",
            "Fetching AAPL data for 2024-10...\n",
            "Successfully fetched 598 rows for AAPL in 2024-10.\n",
            "Fetching AAPL data for 2024-11...\n",
            "Successfully fetched 510 rows for AAPL in 2024-11.\n",
            "Fetching AAPL data for 2024-12...\n",
            "Successfully fetched 536 rows for AAPL in 2024-12.\n",
            "AAPL data saved to Google Drive as AAPL_data.csv\n",
            "Fetching MSFT data for 2021-01...\n",
            "Successfully fetched 494 rows for MSFT in 2021-01.\n",
            "Fetching MSFT data for 2021-02...\n",
            "Successfully fetched 492 rows for MSFT in 2021-02.\n",
            "Fetching MSFT data for 2021-03...\n",
            "Successfully fetched 598 rows for MSFT in 2021-03.\n",
            "Fetching MSFT data for 2021-04...\n",
            "Successfully fetched 546 rows for MSFT in 2021-04.\n",
            "Fetching MSFT data for 2021-05...\n",
            "Successfully fetched 520 rows for MSFT in 2021-05.\n",
            "Fetching MSFT data for 2021-06...\n",
            "Successfully fetched 572 rows for MSFT in 2021-06.\n",
            "Fetching MSFT data for 2021-07...\n",
            "Successfully fetched 546 rows for MSFT in 2021-07.\n",
            "Fetching MSFT data for 2021-08...\n",
            "Successfully fetched 570 rows for MSFT in 2021-08.\n",
            "Fetching MSFT data for 2021-09...\n",
            "Successfully fetched 546 rows for MSFT in 2021-09.\n",
            "Fetching MSFT data for 2021-10...\n",
            "Successfully fetched 546 rows for MSFT in 2021-10.\n",
            "Fetching MSFT data for 2021-11...\n",
            "Successfully fetched 546 rows for MSFT in 2021-11.\n",
            "Fetching MSFT data for 2021-12...\n",
            "Successfully fetched 572 rows for MSFT in 2021-12.\n",
            "Fetching MSFT data for 2022-01...\n",
            "Successfully fetched 520 rows for MSFT in 2022-01.\n",
            "Fetching MSFT data for 2022-02...\n",
            "Successfully fetched 494 rows for MSFT in 2022-02.\n",
            "Fetching MSFT data for 2022-03...\n",
            "Successfully fetched 598 rows for MSFT in 2022-03.\n",
            "Fetching MSFT data for 2022-04...\n",
            "Successfully fetched 520 rows for MSFT in 2022-04.\n",
            "Fetching MSFT data for 2022-05...\n",
            "Successfully fetched 546 rows for MSFT in 2022-05.\n",
            "Fetching MSFT data for 2022-06...\n",
            "Successfully fetched 546 rows for MSFT in 2022-06.\n",
            "Fetching MSFT data for 2022-07...\n",
            "Successfully fetched 520 rows for MSFT in 2022-07.\n",
            "Fetching MSFT data for 2022-08...\n",
            "Successfully fetched 598 rows for MSFT in 2022-08.\n",
            "Fetching MSFT data for 2022-09...\n",
            "Successfully fetched 546 rows for MSFT in 2022-09.\n",
            "Fetching MSFT data for 2022-10...\n",
            "Successfully fetched 546 rows for MSFT in 2022-10.\n",
            "Fetching MSFT data for 2022-11...\n",
            "Successfully fetched 534 rows for MSFT in 2022-11.\n",
            "Fetching MSFT data for 2022-12...\n",
            "Successfully fetched 546 rows for MSFT in 2022-12.\n",
            "Fetching MSFT data for 2023-01...\n",
            "Successfully fetched 520 rows for MSFT in 2023-01.\n",
            "Fetching MSFT data for 2023-02...\n",
            "Successfully fetched 494 rows for MSFT in 2023-02.\n",
            "Fetching MSFT data for 2023-03...\n",
            "Successfully fetched 598 rows for MSFT in 2023-03.\n",
            "Fetching MSFT data for 2023-04...\n",
            "Successfully fetched 494 rows for MSFT in 2023-04.\n",
            "Fetching MSFT data for 2023-05...\n",
            "Successfully fetched 572 rows for MSFT in 2023-05.\n",
            "Fetching MSFT data for 2023-06...\n",
            "Successfully fetched 546 rows for MSFT in 2023-06.\n",
            "Fetching MSFT data for 2023-07...\n",
            "Successfully fetched 520 rows for MSFT in 2023-07.\n",
            "Fetching MSFT data for 2023-08...\n",
            "Successfully fetched 598 rows for MSFT in 2023-08.\n",
            "Fetching MSFT data for 2023-09...\n",
            "Successfully fetched 520 rows for MSFT in 2023-09.\n",
            "Fetching MSFT data for 2023-10...\n",
            "Successfully fetched 572 rows for MSFT in 2023-10.\n",
            "Fetching MSFT data for 2023-11...\n",
            "Successfully fetched 534 rows for MSFT in 2023-11.\n",
            "Fetching MSFT data for 2023-12...\n",
            "Successfully fetched 520 rows for MSFT in 2023-12.\n",
            "Fetching MSFT data for 2024-01...\n",
            "Successfully fetched 546 rows for MSFT in 2024-01.\n",
            "Fetching MSFT data for 2024-02...\n",
            "Successfully fetched 520 rows for MSFT in 2024-02.\n",
            "Fetching MSFT data for 2024-03...\n",
            "Successfully fetched 520 rows for MSFT in 2024-03.\n",
            "Fetching MSFT data for 2024-04...\n",
            "Successfully fetched 572 rows for MSFT in 2024-04.\n",
            "Fetching MSFT data for 2024-05...\n",
            "Successfully fetched 572 rows for MSFT in 2024-05.\n",
            "Fetching MSFT data for 2024-06...\n",
            "Successfully fetched 494 rows for MSFT in 2024-06.\n",
            "Fetching MSFT data for 2024-07...\n",
            "Successfully fetched 561 rows for MSFT in 2024-07.\n",
            "Fetching MSFT data for 2024-08...\n",
            "Successfully fetched 572 rows for MSFT in 2024-08.\n",
            "Fetching MSFT data for 2024-09...\n",
            "Successfully fetched 520 rows for MSFT in 2024-09.\n",
            "Fetching MSFT data for 2024-10...\n",
            "Successfully fetched 598 rows for MSFT in 2024-10.\n",
            "Fetching MSFT data for 2024-11...\n",
            "Successfully fetched 510 rows for MSFT in 2024-11.\n",
            "Fetching MSFT data for 2024-12...\n",
            "Successfully fetched 535 rows for MSFT in 2024-12.\n",
            "MSFT data saved to Google Drive as MSFT_data.csv\n"
          ]
        }
      ],
      "source": [
        "import time as t\n",
        "from twelvedata import TDClient\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta, time\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Initialize the client\n",
        "td = TDClient(apikey=api_key)\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def fetch_stock_data(symbol):\n",
        "    # Define the date range\n",
        "    start_date = datetime(2021, 1, 1)\n",
        "    end_date = datetime(2024, 12, 31)\n",
        "\n",
        "    # Define market hours\n",
        "    market_open_time = time(9, 30)  # 9:30 AM\n",
        "    market_close_time = time(16, 0)  # 4:00 PM\n",
        "\n",
        "    all_data = []\n",
        "\n",
        "    # Iterate through each month in the date range\n",
        "    current_date = start_date\n",
        "    while current_date <= end_date:\n",
        "        # Calculate the last day of the current month\n",
        "        if current_date.month == 12:\n",
        "            next_month = current_date.replace(year=current_date.year + 1, month=1, day=1)\n",
        "        else:\n",
        "            next_month = current_date.replace(month=current_date.month + 1, day=1)\n",
        "        last_day_of_month = next_month - timedelta(days=1)\n",
        "\n",
        "        # Define the start and end times for the entire month\n",
        "        month_start = datetime.combine(current_date, market_open_time)\n",
        "        month_end = datetime.combine(last_day_of_month, market_close_time)\n",
        "\n",
        "        # Fetch data for the entire month\n",
        "        try:\n",
        "            print(f\"Fetching {symbol} data for {current_date.strftime('%Y-%m')}...\")\n",
        "            data = td.time_series(\n",
        "                symbol=symbol,\n",
        "                interval=\"15min\",\n",
        "                start_date=month_start.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                end_date=month_end.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                outputsize=5000\n",
        "            ).as_pandas()\n",
        "\n",
        "            if not data.empty:\n",
        "                all_data.append(data)\n",
        "                print(f\"Successfully fetched {len(data)} rows for {symbol} in {current_date.strftime('%Y-%m')}.\")\n",
        "            else:\n",
        "                print(f\"No data returned for {symbol} in {current_date.strftime('%Y-%m')} (market may have been closed).\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching {symbol} data for {current_date.strftime('%Y-%m')}: {e}\")\n",
        "\n",
        "        # Move to the next month\n",
        "        current_date = next_month\n",
        "\n",
        "        # Add delay between requests to avoid hitting API limits\n",
        "        t.sleep(10)  # 10-second delay (6 requests per minute)\n",
        "\n",
        "    # Combine all chunks into a single DataFrame\n",
        "    if all_data:\n",
        "        full_data = pd.concat(all_data)\n",
        "        # Remove duplicates (if any)\n",
        "        full_data = full_data[~full_data.index.duplicated(keep='first')]\n",
        "        # Save to Google Drive\n",
        "        full_data.to_csv(f'/content/drive/My Drive/{symbol}_data.csv', index=True)\n",
        "        print(f\"{symbol} data saved to Google Drive as {symbol}_data.csv\")\n",
        "    else:\n",
        "        print(f\"No data collected for {symbol}\")\n",
        "\n",
        "# Pull data for AAPL and MSFT\n",
        "fetch_stock_data(\"AAPL\")\n",
        "fetch_stock_data(\"MSFT\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dS_65ns2f4HY"
      },
      "outputs": [],
      "source": [
        "import time as t\n",
        "from twelvedata import TDClient\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta, time\n",
        "import os\n",
        "from google.colab import drive\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Initialize TwelveData client\n",
        "td = TDClient(apikey=api_key)\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# API constraints\n",
        "MAX_DAILY_REQUESTS = 800\n",
        "MAX_MINUTE_REQUESTS = 8\n",
        "DATA_LIMIT_PER_REQUEST = 5000\n",
        "\n",
        "# Track API usage\n",
        "api_usage = 452  # Update this dynamically if needed\n",
        "\n",
        "def fetch_chunk(symbol, start_dt, end_dt, retries=3):\n",
        "    \"\"\"Fetch data chunk with retry logic, ensuring API constraints.\"\"\"\n",
        "    global api_usage\n",
        "\n",
        "    if api_usage >= MAX_DAILY_REQUESTS:\n",
        "        print(\"Daily API limit reached. Stopping execution.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            data = td.time_series(\n",
        "                symbol=symbol,\n",
        "                interval=\"15min\",\n",
        "                start_date=start_dt.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                end_date=end_dt.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                outputsize=DATA_LIMIT_PER_REQUEST\n",
        "            ).as_pandas()\n",
        "\n",
        "            api_usage += 1  # Increment usage counter\n",
        "            return data[data.index >= start_dt]  # Trim overlap\n",
        "        except Exception as e:\n",
        "            print(f\"Attempt {attempt+1} failed: {e}\")\n",
        "            t.sleep(min(8 * (attempt + 1), 60))  # Exponential backoff, capped at 60s\n",
        "\n",
        "    return pd.DataFrame()\n",
        "\n",
        "def fetch_stock_data(symbol):\n",
        "    \"\"\"Fetch full stock data while respecting API rate limits.\"\"\"\n",
        "    global api_usage\n",
        "\n",
        "    market_open = time(9, 30)\n",
        "    market_close = time(16, 0)\n",
        "    chunk_days = 14\n",
        "    overlap = timedelta(minutes=30)\n",
        "\n",
        "    all_data = []\n",
        "    current_dt = datetime(2021, 1, 1)\n",
        "    end_dt = datetime(2024, 12, 31)\n",
        "\n",
        "    # Find first available trading day\n",
        "    while current_dt <= end_dt:\n",
        "        if current_dt.weekday() >= 5:\n",
        "            current_dt += timedelta(days=1)\n",
        "            continue\n",
        "\n",
        "        test_data = fetch_chunk(\n",
        "            symbol,\n",
        "            datetime.combine(current_dt, market_open),\n",
        "            datetime.combine(current_dt, market_close),\n",
        "            retries=1\n",
        "        )\n",
        "        if not test_data.empty:\n",
        "            break\n",
        "        current_dt += timedelta(days=1)\n",
        "\n",
        "    pbar = tqdm(total=(end_dt - current_dt).days, desc=f\"Fetching {symbol}\")\n",
        "\n",
        "    while current_dt <= end_dt and api_usage < MAX_DAILY_REQUESTS:\n",
        "        chunk_end = min(current_dt + timedelta(days=chunk_days), end_dt)\n",
        "\n",
        "        chunk_data = fetch_chunk(\n",
        "            symbol,\n",
        "            datetime.combine(current_dt, market_open),\n",
        "            datetime.combine(chunk_end, market_close) + overlap,\n",
        "        )\n",
        "\n",
        "        if not chunk_data.empty:\n",
        "            all_data.append(chunk_data)\n",
        "            pbar.update((chunk_end - current_dt).days)\n",
        "            current_dt = chunk_data.index[-1].to_pydatetime() - overlap\n",
        "        else:\n",
        "            current_dt += timedelta(days=1)\n",
        "\n",
        "        # Skip weekends efficiently\n",
        "        if current_dt.weekday() >= 5:\n",
        "            current_dt += timedelta(days=7 - current_dt.weekday())\n",
        "\n",
        "        # Ensure we don't exceed per-minute limit\n",
        "        if api_usage % MAX_MINUTE_REQUESTS == 0:\n",
        "            t.sleep(61)  # Sleep slightly over 1 min to reset API window\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "    if all_data:\n",
        "        final_data = pd.concat(all_data)\n",
        "        final_data = final_data[~final_data.index.duplicated(keep='last')]\n",
        "        final_data.sort_index(inplace=True)\n",
        "        save_path = f'/content/drive/My Drive/{symbol}_data.csv'\n",
        "        final_data.to_csv(save_path)\n",
        "        print(f\"Saved {len(final_data)} records for {symbol} at {save_path}\")\n",
        "\n",
        "# Fetch data for both stocks\n",
        "fetch_stock_data(\"AAPL\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time as t\n",
        "from twelvedata import TDClient\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta, time\n",
        "import os\n",
        "from google.colab import drive\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Initialize TwelveData client\n",
        "td = TDClient(apikey=api_key)\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# API constraints\n",
        "MAX_DAILY_REQUESTS = 800\n",
        "MAX_MINUTE_REQUESTS = 8\n",
        "DATA_LIMIT_PER_REQUEST = 5000\n",
        "\n",
        "# Track API usage\n",
        "api_usage = 452  # Update this dynamically if needed\n",
        "\n",
        "def fetch_chunk(symbol, start_dt, end_dt, retries=3):\n",
        "    \"\"\"Fetch data chunk with retry logic, ensuring API constraints.\"\"\"\n",
        "    global api_usage\n",
        "\n",
        "    if api_usage >= MAX_DAILY_REQUESTS:\n",
        "        print(\"Daily API limit reached. Stopping execution.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            data = td.time_series(\n",
        "                symbol=symbol,\n",
        "                interval=\"15min\",\n",
        "                start_date=start_dt.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                end_date=end_dt.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                outputsize=DATA_LIMIT_PER_REQUEST\n",
        "            ).as_pandas()\n",
        "\n",
        "            api_usage += 1  # Increment usage counter\n",
        "            return data[data.index >= start_dt]  # Trim overlap\n",
        "        except Exception as e:\n",
        "            print(f\"Attempt {attempt+1} failed: {e}\")\n",
        "            t.sleep(min(8 * (attempt + 1), 60))  # Exponential backoff, capped at 60s\n",
        "\n",
        "    return pd.DataFrame()\n",
        "\n",
        "def fetch_stock_data(symbol):\n",
        "    \"\"\"Fetch full stock data while respecting API rate limits.\"\"\"\n",
        "    global api_usage\n",
        "\n",
        "    market_open = time(9, 30)\n",
        "    market_close = time(16, 0)\n",
        "    chunk_days = 14\n",
        "    overlap = timedelta(minutes=30)\n",
        "\n",
        "    all_data = []\n",
        "    current_dt = datetime(2021, 1, 1)\n",
        "    end_dt = datetime(2024, 12, 31)\n",
        "\n",
        "    # Find first available trading day\n",
        "    while current_dt <= end_dt:\n",
        "        if current_dt.weekday() >= 5:  # Skip weekends\n",
        "            current_dt += timedelta(days=1)\n",
        "            continue\n",
        "\n",
        "        test_data = fetch_chunk(\n",
        "            symbol,\n",
        "            datetime.combine(current_dt, market_open),\n",
        "            datetime.combine(current_dt, market_close),\n",
        "            retries=1\n",
        "        )\n",
        "        if not test_data.empty:\n",
        "            break\n",
        "        current_dt += timedelta(days=1)\n",
        "\n",
        "    # Calculate total calendar days (simpler approach)\n",
        "    total_days = (end_dt - current_dt).days\n",
        "    if total_days <= 0:\n",
        "        print(f\"No valid data available for {symbol}. Exiting.\")\n",
        "        return\n",
        "\n",
        "    pbar = tqdm(total=total_days, desc=f\"Fetching {symbol}\")\n",
        "\n",
        "    while current_dt <= end_dt and api_usage < MAX_DAILY_REQUESTS:\n",
        "        # Skip weekends\n",
        "        if current_dt.weekday() >= 5:\n",
        "            current_dt += timedelta(days=1)\n",
        "            pbar.update(1)\n",
        "            continue\n",
        "\n",
        "        chunk_end = min(current_dt + timedelta(days=chunk_days), end_dt)\n",
        "\n",
        "        chunk_data = fetch_chunk(\n",
        "            symbol,\n",
        "            datetime.combine(current_dt, market_open),\n",
        "            datetime.combine(chunk_end, market_close) + overlap,\n",
        "        )\n",
        "\n",
        "        if not chunk_data.empty:\n",
        "            all_data.append(chunk_data)\n",
        "            # Update progress by actual days processed (simple approach)\n",
        "            days_processed = (chunk_end - current_dt).days\n",
        "            pbar.update(days_processed)\n",
        "            current_dt = chunk_end\n",
        "        else:\n",
        "            # If no data, just move forward by chunk_days\n",
        "            pbar.update(chunk_days)\n",
        "            current_dt += timedelta(days=chunk_days)\n",
        "\n",
        "        # API rate limiting\n",
        "        if api_usage % MAX_MINUTE_REQUESTS == 0:\n",
        "            t.sleep(61)\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "    if all_data:\n",
        "        final_data = pd.concat(all_data)\n",
        "        final_data = final_data[~final_data.index.duplicated(keep='last')]\n",
        "        final_data.sort_index(inplace=True)\n",
        "        save_path = f'/content/drive/My Drive/{symbol}_data.csv'\n",
        "        final_data.to_csv(save_path)\n",
        "        print(f\"Saved {len(final_data)} records for {symbol} at {save_path}\")\n",
        "\n",
        "    print(f\"Fetching complete for {symbol} ✅\")\n",
        "\n",
        "# Fetch data for both stocks\n",
        "fetch_stock_data(\"AAPL\")\n"
      ],
      "metadata": {
        "id": "PoLaiz8t8jFR",
        "outputId": "046c3ba3-a24c-48e4-f0a7-2670cc4e4f99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Attempt 1 failed: No data is available on the specified dates. Try setting different start/end dates.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:   0%|          | 0/1457 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:   1%|          | 14/1457 [00:00<01:09, 20.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:   2%|▏         | 28/1457 [00:01<01:08, 20.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:   3%|▎         | 42/1457 [00:01<01:07, 21.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:   3%|▎         | 42/1457 [00:16<01:07, 21.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:   4%|▍         | 56/1457 [01:03<41:20,  1.77s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:   5%|▍         | 70/1457 [01:04<26:38,  1.15s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:   6%|▌         | 84/1457 [01:05<17:51,  1.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:   7%|▋         | 98/1457 [01:06<12:19,  1.84it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:   8%|▊         | 112/1457 [01:07<08:44,  2.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:   9%|▊         | 126/1457 [01:07<06:20,  3.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  10%|▉         | 140/1457 [01:09<05:08,  4.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  11%|█         | 154/1457 [01:10<03:50,  5.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  11%|█         | 154/1457 [01:26<03:50,  5.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  12%|█▏        | 168/1457 [02:11<31:26,  1.46s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  12%|█▏        | 182/1457 [02:12<21:58,  1.03s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  13%|█▎        | 196/1457 [02:13<15:30,  1.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  14%|█▍        | 210/1457 [02:13<11:03,  1.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  15%|█▌        | 224/1457 [02:14<07:59,  2.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  16%|█▋        | 238/1457 [02:15<05:50,  3.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  17%|█▋        | 252/1457 [02:16<04:22,  4.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  18%|█▊        | 266/1457 [02:17<03:21,  5.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  18%|█▊        | 266/1457 [02:36<03:21,  5.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  19%|█▉        | 280/1457 [03:18<28:16,  1.44s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  20%|██        | 294/1457 [03:19<19:51,  1.02s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  21%|██        | 308/1457 [03:20<14:01,  1.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  22%|██▏       | 322/1457 [03:20<10:00,  1.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  23%|██▎       | 336/1457 [03:21<07:10,  2.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  24%|██▍       | 350/1457 [03:22<05:15,  3.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  25%|██▍       | 364/1457 [03:22<03:53,  4.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  26%|██▌       | 378/1457 [03:23<02:58,  6.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  26%|██▌       | 378/1457 [03:36<02:58,  6.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  27%|██▋       | 392/1457 [04:25<25:33,  1.44s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  28%|██▊       | 406/1457 [04:26<17:53,  1.02s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  29%|██▉       | 420/1457 [04:26<12:35,  1.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  30%|██▉       | 434/1457 [04:27<08:58,  1.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  31%|███       | 448/1457 [04:28<06:28,  2.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  32%|███▏      | 462/1457 [04:29<04:43,  3.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  33%|███▎      | 476/1457 [04:29<03:29,  4.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  34%|███▎      | 490/1457 [04:30<02:40,  6.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  34%|███▎      | 490/1457 [04:46<02:40,  6.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  35%|███▍      | 504/1457 [05:32<22:52,  1.44s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  36%|███▌      | 518/1457 [05:32<16:00,  1.02s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  37%|███▋      | 532/1457 [05:33<11:17,  1.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  37%|███▋      | 546/1457 [05:34<07:59,  1.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  38%|███▊      | 560/1457 [05:35<05:45,  2.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  39%|███▉      | 574/1457 [05:35<04:13,  3.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  40%|████      | 588/1457 [05:36<03:07,  4.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  41%|████▏     | 602/1457 [05:37<02:23,  5.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  41%|████▏     | 602/1457 [05:56<02:23,  5.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  42%|████▏     | 616/1457 [06:39<20:11,  1.44s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  43%|████▎     | 630/1457 [06:39<14:05,  1.02s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  44%|████▍     | 644/1457 [06:40<09:53,  1.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  45%|████▌     | 658/1457 [06:41<07:02,  1.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  46%|████▌     | 672/1457 [06:41<05:01,  2.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  47%|████▋     | 686/1457 [06:43<03:53,  3.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  48%|████▊     | 700/1457 [06:44<02:52,  4.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  49%|████▉     | 714/1457 [06:44<02:08,  5.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  49%|████▉     | 714/1457 [06:56<02:08,  5.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  50%|████▉     | 728/1457 [07:46<17:32,  1.44s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  51%|█████     | 742/1457 [07:47<12:14,  1.03s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  52%|█████▏    | 756/1457 [07:48<08:34,  1.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  53%|█████▎    | 770/1457 [07:48<06:02,  1.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  54%|█████▍    | 784/1457 [07:49<04:19,  2.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  55%|█████▍    | 798/1457 [07:50<03:08,  3.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  56%|█████▌    | 812/1457 [07:51<02:20,  4.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  57%|█████▋    | 826/1457 [07:51<01:44,  6.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  57%|█████▋    | 826/1457 [08:06<01:44,  6.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  58%|█████▊    | 840/1457 [08:53<14:47,  1.44s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  59%|█████▊    | 854/1457 [08:54<10:16,  1.02s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  60%|█████▉    | 868/1457 [08:54<07:09,  1.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  61%|██████    | 882/1457 [08:55<05:01,  1.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  61%|██████▏   | 896/1457 [08:56<03:35,  2.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  62%|██████▏   | 910/1457 [08:57<02:36,  3.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  63%|██████▎   | 924/1457 [08:57<01:54,  4.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  64%|██████▍   | 938/1457 [08:58<01:26,  5.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  64%|██████▍   | 938/1457 [09:16<01:26,  5.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  65%|██████▌   | 952/1457 [10:00<12:07,  1.44s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  66%|██████▋   | 966/1457 [10:00<08:21,  1.02s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  67%|██████▋   | 980/1457 [10:01<05:49,  1.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  68%|██████▊   | 994/1457 [10:02<04:04,  1.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  69%|██████▉   | 1008/1457 [10:03<02:53,  2.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  70%|███████   | 1022/1457 [10:03<02:03,  3.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  71%|███████   | 1036/1457 [10:04<01:30,  4.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  72%|███████▏  | 1050/1457 [10:05<01:06,  6.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  72%|███████▏  | 1050/1457 [10:16<01:06,  6.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  73%|███████▎  | 1064/1457 [11:06<09:24,  1.44s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  74%|███████▍  | 1078/1457 [11:07<06:26,  1.02s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  75%|███████▍  | 1092/1457 [11:08<04:25,  1.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  76%|███████▌  | 1106/1457 [11:09<03:04,  1.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  77%|███████▋  | 1120/1457 [11:09<02:09,  2.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  78%|███████▊  | 1134/1457 [11:10<01:32,  3.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  79%|███████▉  | 1148/1457 [11:11<01:06,  4.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  80%|███████▉  | 1162/1457 [11:12<00:49,  5.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  80%|███████▉  | 1162/1457 [11:26<00:49,  5.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  81%|████████  | 1176/1457 [12:13<06:44,  1.44s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  82%|████████▏ | 1190/1457 [12:14<04:33,  1.02s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  83%|████████▎ | 1204/1457 [12:15<03:04,  1.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  84%|████████▎ | 1218/1457 [12:15<02:06,  1.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  85%|████████▍ | 1232/1457 [12:16<01:26,  2.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  86%|████████▌ | 1246/1457 [12:17<01:00,  3.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  86%|████████▋ | 1260/1457 [12:18<00:42,  4.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  87%|████████▋ | 1274/1457 [12:19<00:30,  5.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  87%|████████▋ | 1274/1457 [12:36<00:30,  5.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  88%|████████▊ | 1288/1457 [13:20<04:03,  1.44s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  89%|████████▉ | 1302/1457 [13:21<02:38,  1.02s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  90%|█████████ | 1316/1457 [13:22<01:43,  1.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  91%|█████████▏| 1330/1457 [13:22<01:06,  1.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  92%|█████████▏| 1344/1457 [13:23<00:43,  2.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  93%|█████████▎| 1358/1457 [13:24<00:28,  3.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  94%|█████████▍| 1372/1457 [13:25<00:18,  4.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  95%|█████████▌| 1386/1457 [13:25<00:12,  5.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  95%|█████████▌| 1386/1457 [13:36<00:12,  5.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  96%|█████████▌| 1400/1457 [14:27<01:22,  1.44s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  97%|█████████▋| 1414/1457 [14:28<00:44,  1.03s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  98%|█████████▊| 1428/1457 [14:29<00:21,  1.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL:  99%|█████████▉| 1442/1457 [14:29<00:07,  1.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL: 100%|█████████▉| 1456/1457 [14:30<00:00,  2.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL: 100%|██████████| 1457/1457 [14:46<00:00,  2.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL: 100%|██████████| 1457/1457 [15:34<00:00,  2.26s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempt 1 failed: You have run out of API credits for the day. 801 API credits were used, with the current limit being 800. Wait for the next day or consider switching to a paid plan that will remove daily limits at https://twelvedata.com/pricing\n",
            "Attempt 2 failed: You have run out of API credits for the day. 802 API credits were used, with the current limit being 800. Wait for the next day or consider switching to a paid plan that will remove daily limits at https://twelvedata.com/pricing\n",
            "Attempt 3 failed: You have run out of API credits for the day. 803 API credits were used, with the current limit being 800. Wait for the next day or consider switching to a paid plan that will remove daily limits at https://twelvedata.com/pricing\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "Fetching AAPL: 1471it [42:51,  1.75s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 26074 records for AAPL at /content/drive/My Drive/AAPL_data.csv\n",
            "Fetching complete for AAPL ✅\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run this code tomorrow to see if it works"
      ],
      "metadata": {
        "id": "lWMGP7JaZ7V6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_stock_data(symbol):\n",
        "    \"\"\"Fetch full stock data while respecting API rate limits.\"\"\"\n",
        "    global api_usage\n",
        "\n",
        "    market_open = time(9, 30)\n",
        "    market_close = time(16, 0)\n",
        "    chunk_days = 14\n",
        "    overlap = timedelta(minutes=30)\n",
        "\n",
        "    all_data = []\n",
        "    current_dt = datetime(2021, 1, 1)\n",
        "    end_dt = datetime(2024, 12, 31)\n",
        "\n",
        "    # Find first available trading day\n",
        "    while current_dt <= end_dt:\n",
        "        if current_dt.weekday() >= 5:  # Skip weekends\n",
        "            current_dt += timedelta(days=1)\n",
        "            continue\n",
        "\n",
        "        test_data = fetch_chunk(\n",
        "            symbol,\n",
        "            datetime.combine(current_dt, market_open),\n",
        "            datetime.combine(current_dt, market_close),\n",
        "            retries=1\n",
        "        )\n",
        "        if not test_data.empty:\n",
        "            break\n",
        "        current_dt += timedelta(days=1)\n",
        "\n",
        "    # Calculate total calendar days\n",
        "    total_days = (end_dt - current_dt).days + 1  # +1 to include both start and end dates\n",
        "    if total_days <= 0:\n",
        "        print(f\"No valid data available for {symbol}. Exiting.\")\n",
        "        return\n",
        "\n",
        "    pbar = tqdm(total=total_days, desc=f\"Fetching {symbol}\")\n",
        "    processed_days = 0\n",
        "\n",
        "    while current_dt <= end_dt and api_usage < MAX_DAILY_REQUESTS and processed_days < total_days:\n",
        "        # Skip weekends\n",
        "        if current_dt.weekday() >= 5:\n",
        "            current_dt += timedelta(days=1)\n",
        "            pbar.update(1)\n",
        "            processed_days += 1\n",
        "            continue\n",
        "\n",
        "        chunk_end = min(current_dt + timedelta(days=chunk_days - 1), end_dt)  # -1 because we include current day\n",
        "\n",
        "        chunk_data = fetch_chunk(\n",
        "            symbol,\n",
        "            datetime.combine(current_dt, market_open),\n",
        "            datetime.combine(chunk_end, market_close) + overlap,\n",
        "        )\n",
        "\n",
        "        days_in_chunk = (chunk_end - current_dt).days + 1  # +1 to include both start and end dates\n",
        "\n",
        "        if not chunk_data.empty:\n",
        "            all_data.append(chunk_data)\n",
        "            pbar.update(days_in_chunk)\n",
        "            processed_days += days_in_chunk\n",
        "            current_dt = chunk_end + timedelta(days=1)  # Move to next day after chunk_end\n",
        "        else:\n",
        "            # If no data, move forward by chunk days\n",
        "            pbar.update(days_in_chunk)\n",
        "            processed_days += days_in_chunk\n",
        "            current_dt = chunk_end + timedelta(days=1)\n",
        "\n",
        "        # API rate limiting\n",
        "        if api_usage % MAX_MINUTE_REQUESTS == 0:\n",
        "            t.sleep(61)\n",
        "\n",
        "        # Print status every 100 API calls\n",
        "        if api_usage % 100 == 0:\n",
        "            print(f\"API calls used: {api_usage}, Current date: {current_dt.strftime('%Y-%m-%d')}\")\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "    if all_data:\n",
        "        final_data = pd.concat(all_data)\n",
        "        final_data = final_data[~final_data.index.duplicated(keep='last')]\n",
        "        final_data.sort_index(inplace=True)\n",
        "        save_path = f'/content/drive/My Drive/{symbol}_data.csv'\n",
        "        final_data.to_csv(save_path)\n",
        "        print(f\"Saved {len(final_data)} records for {symbol} at {save_path}\")\n",
        "\n",
        "    if current_dt > end_dt:\n",
        "        print(f\"Successfully processed all dates for {symbol} ✅\")\n",
        "    elif api_usage >= MAX_DAILY_REQUESTS:\n",
        "        print(f\"Stopped due to reaching API limit at {current_dt.strftime('%Y-%m-%d')} ⚠️\")\n",
        "    else:\n",
        "        print(f\"Unexpected exit condition at {current_dt.strftime('%Y-%m-%d')}\")\n",
        "\n",
        "# Fetch data for both stocks\n",
        "fetch_stock_data(\"MSFT\")"
      ],
      "metadata": {
        "id": "EYoypeKfZ5wD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwJWEXvN7GBp"
      },
      "source": [
        "## [2 points] How many API credits would be required for downloading 4 years worth of data at 15-minute intervals, per stock?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GX0Mm1Kp7GBp"
      },
      "source": [
        "## [3 points] The API applies a rate limit. How long would it take to download 4 years worth of data at 15-minute intervals, per stock?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Lg6TTBS7GBq"
      },
      "source": [
        "## [20 points for setting up data collection] We’ll be working with just 2 stocks: AAPL and MSFT. The need for speed in a price API makes it inevitable that a small percentage of data elements has errors in it. To compensate for such errors, a “good enough” strategy is to request data for overlapping periods, say fetching 30 minutes of data every 15 minutes – replacing the most recently arrived data with a corrected version."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOfbo1v07GBq"
      },
      "source": [
        "Below is the setup for using the api to gather the stock data. The full script I ran to gather the data using the API is under the following question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9dkreG7D7GBq",
        "outputId": "4c1d5f96-635e-4cf1-82b7-43ef10ca8902",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting twelvedata\n",
            "  Downloading twelvedata-1.2.24-py2.py3-none-any.whl.metadata (19 kB)\n",
            "Collecting pytimeparse<2,>=1.1 (from twelvedata)\n",
            "  Downloading pytimeparse-1.1.8-py2.py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: requests<3,>=2.22 in /usr/local/lib/python3.11/dist-packages (from twelvedata) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.22->twelvedata) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.22->twelvedata) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.22->twelvedata) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.22->twelvedata) (2025.1.31)\n",
            "Downloading twelvedata-1.2.24-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.4/46.4 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytimeparse-1.1.8-py2.py3-none-any.whl (10.0 kB)\n",
            "Installing collected packages: pytimeparse, twelvedata\n",
            "Successfully installed pytimeparse-1.1.8 twelvedata-1.2.24\n",
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "!pip install twelvedata\n",
        "import twelvedata\n",
        "\n",
        "from google.colab import drive\n",
        "import sys\n",
        "drive.mount(\"/content/drive/\", force_remount=True)\n",
        "sys.path.append(\"/content/drive/My Drive/Colab Notebooks\")\n",
        "from key import twelveDataKey as api_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPAVh9Ji7GBr"
      },
      "source": [
        "## [25 points for collecting data] Technically, final testing can only be done while the market is open: 9:30 am to 4:00 pm on weekdays. In a compromise to accommodate everyone’s schedules we will pull the stock data of each stock and gather up at least 4 years of stock price data (January 2021-December 2024) and use it as the basis of our work.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF9yWH2I7GBr"
      },
      "source": [
        "Below is the script I ran for getting the data and storing them in csv files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AKFd5uF7GBs"
      },
      "outputs": [],
      "source": [
        "import time as t\n",
        "from twelvedata import TDClient\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta, time\n",
        "import os\n",
        "from google.colab import drive\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Initialize TwelveData client\n",
        "td = TDClient(apikey=api_key)\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# API constraints\n",
        "MAX_DAILY_REQUESTS = 800\n",
        "MAX_MINUTE_REQUESTS = 8\n",
        "DATA_LIMIT_PER_REQUEST = 5000\n",
        "\n",
        "# Track API usage\n",
        "api_usage = 452  # Update this dynamically if needed\n",
        "\n",
        "def fetch_chunk(symbol, start_dt, end_dt, retries=3):\n",
        "    \"\"\"Fetch data chunk with retry logic, ensuring API constraints.\"\"\"\n",
        "    global api_usage\n",
        "\n",
        "    if api_usage >= MAX_DAILY_REQUESTS:\n",
        "        print(\"Daily API limit reached. Stopping execution.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            data = td.time_series(\n",
        "                symbol=symbol,\n",
        "                interval=\"15min\",\n",
        "                start_date=start_dt.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                end_date=end_dt.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                outputsize=DATA_LIMIT_PER_REQUEST\n",
        "            ).as_pandas()\n",
        "\n",
        "            api_usage += 1  # Increment usage counter\n",
        "            return data[data.index >= start_dt]  # Trim overlap\n",
        "        except Exception as e:\n",
        "            print(f\"Attempt {attempt+1} failed: {e}\")\n",
        "            t.sleep(min(8 * (attempt + 1), 60))  # Exponential backoff, capped at 60s\n",
        "\n",
        "    return pd.DataFrame()\n",
        "\n",
        "def fetch_stock_data(symbol):\n",
        "    \"\"\"Fetch full stock data while respecting API rate limits.\"\"\"\n",
        "    global api_usage\n",
        "\n",
        "    market_open = time(9, 30)\n",
        "    market_close = time(16, 0)\n",
        "    chunk_days = 14\n",
        "    overlap = timedelta(minutes=30)\n",
        "\n",
        "    all_data = []\n",
        "    current_dt = datetime(2021, 1, 1)\n",
        "    end_dt = datetime(2024, 12, 31)\n",
        "\n",
        "    # Find first available trading day\n",
        "    while current_dt <= end_dt:\n",
        "        if current_dt.weekday() >= 5:\n",
        "            current_dt += timedelta(days=1)\n",
        "            continue\n",
        "\n",
        "        test_data = fetch_chunk(\n",
        "            symbol,\n",
        "            datetime.combine(current_dt, market_open),\n",
        "            datetime.combine(current_dt, market_close),\n",
        "            retries=1\n",
        "        )\n",
        "        if not test_data.empty:\n",
        "            break\n",
        "        current_dt += timedelta(days=1)\n",
        "\n",
        "    pbar = tqdm(total=(end_dt - current_dt).days, desc=f\"Fetching {symbol}\")\n",
        "\n",
        "    while current_dt <= end_dt and api_usage < MAX_DAILY_REQUESTS:\n",
        "        chunk_end = min(current_dt + timedelta(days=chunk_days), end_dt)\n",
        "\n",
        "        chunk_data = fetch_chunk(\n",
        "            symbol,\n",
        "            datetime.combine(current_dt, market_open),\n",
        "            datetime.combine(chunk_end, market_close) + overlap,\n",
        "        )\n",
        "\n",
        "        if not chunk_data.empty:\n",
        "            all_data.append(chunk_data)\n",
        "            pbar.update((chunk_end - current_dt).days)\n",
        "            current_dt = chunk_data.index[-1].to_pydatetime() - overlap\n",
        "        else:\n",
        "            current_dt += timedelta(days=1)\n",
        "\n",
        "        # Skip weekends efficiently\n",
        "        if current_dt.weekday() >= 5:\n",
        "            current_dt += timedelta(days=7 - current_dt.weekday())\n",
        "\n",
        "        # Ensure we don't exceed per-minute limit\n",
        "        if api_usage % MAX_MINUTE_REQUESTS == 0:\n",
        "            t.sleep(61)  # Sleep slightly over 1 min to reset API window\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "    if all_data:\n",
        "        final_data = pd.concat(all_data)\n",
        "        final_data = final_data[~final_data.index.duplicated(keep='last')]\n",
        "        final_data.sort_index(inplace=True)\n",
        "        save_path = f'/content/drive/My Drive/{symbol}_data.csv'\n",
        "        final_data.to_csv(save_path)\n",
        "        print(f\"Saved {len(final_data)} records for {symbol} at {save_path}\")\n",
        "\n",
        "# Fetch data for both stocks\n",
        "fetch_stock_data(\"AAPL\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AetF9ovu7GBs"
      },
      "source": [
        "## [25 points] Create a program new-stock-price-feeder.py that uses a more modern API (e.g., twelveData) instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIWhuEEh7GBs"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, avg, lag, when, lit, input_file_name, round\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"StockTradingStrategy\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Read all CSV files from HDFS directory\n",
        "stock_df = spark.read.csv(\n",
        "    \"hdfs:///user/root/stock_data/\",\n",
        "    header=True,\n",
        "    inferSchema=True\n",
        ")\n",
        "\n",
        "# Add symbol column based on filename\n",
        "stock_df = stock_df.withColumn(\n",
        "    \"filename\", input_file_name()\n",
        ").withColumn(\n",
        "    \"symbol\",\n",
        "    when(\n",
        "        col(\"filename\").contains(\"AAPL\"), \"AAPL\"\n",
        "    ).when(\n",
        "        col(\"filename\").contains(\"MSFT\"), \"MSFT\"\n",
        "    ).otherwise(\"UNKNOWN\")\n",
        ").drop(\"filename\")\n",
        "\n",
        "# Verify we have both symbols\n",
        "print(\"Distinct symbols found:\")\n",
        "stock_df.select(\"symbol\").distinct().show()\n",
        "\n",
        "# Define window specs for moving averages\n",
        "window_10 = Window.partitionBy(\"symbol\").orderBy(\"datetime\").rowsBetween(-9, 0)\n",
        "window_40 = Window.partitionBy(\"symbol\").orderBy(\"datetime\").rowsBetween(-39, 0)\n",
        "\n",
        "# Calculate moving averages\n",
        "stock_with_ma = stock_df.withColumn(\"ma_10\", avg(\"close\").over(window_10)) \\\n",
        "                       .withColumn(\"ma_40\", avg(\"close\").over(window_40))\n",
        "\n",
        "# Function to generate trading signals with share calculations\n",
        "def generate_trades(df):\n",
        "    window_spec = Window.partitionBy(\"symbol\").orderBy(\"datetime\")\n",
        "    return df.withColumn(\"prev_ma_10\", lag(\"ma_10\", 1).over(window_spec)) \\\n",
        "            .withColumn(\"prev_ma_40\", lag(\"ma_40\", 1).over(window_spec)) \\\n",
        "            .withColumn(\"signal\",\n",
        "                when(\n",
        "                    (col(\"prev_ma_10\") <= col(\"prev_ma_40\")) &\n",
        "                    (col(\"ma_10\") > col(\"ma_40\")), \"buy\"\n",
        "                ).when(\n",
        "                    (col(\"prev_ma_10\") >= col(\"prev_ma_40\")) &\n",
        "                    (col(\"ma_10\") < col(\"ma_40\")), \"sell\"\n",
        "                ).otherwise(None)\n",
        "            ) \\\n",
        "            .withColumn(\"shares\", round(lit(100000)/col(\"close\"))) \\\n",
        "            .filter(col(\"signal\").isNotNull()) \\\n",
        "            .select(\n",
        "                \"datetime\",\n",
        "                \"symbol\",\n",
        "                \"close\",\n",
        "                \"signal\",\n",
        "                \"shares\"\n",
        "            )\n",
        "\n",
        "# Generate trades\n",
        "trades_df = generate_trades(stock_with_ma)\n",
        "\n",
        "# Format output as requested\n",
        "formatted_trades = trades_df.rdd.map(\n",
        "    lambda row: f\"({row['datetime']} {row['signal']} {row['symbol']}) - {row['shares']} shares @ ${row['close']:.2f}\"\n",
        ").collect()\n",
        "\n",
        "# Save results to HDFS\n",
        "trades_df.write.mode(\"overwrite\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .csv(\"hdfs:///user/root/stock_trades/\")\n",
        "\n",
        "# Print formatted trades\n",
        "print(\"\\nTrading Recommendations ($100K per trade):\")\n",
        "for trade in formatted_trades:\n",
        "    print(trade)\n",
        "\n",
        "# Sample output:\n",
        "# (2021-01-04 11:30:00 buy AAPL) - 758 shares @ $132.06\n",
        "# (2021-01-05 14:45:00 sell AAPL) - 742 shares @ $134.75\n",
        "# (2021-01-04 10:15:00 buy MSFT) - 459 shares @ $217.89\n",
        "# (2021-01-05 15:30:00 sell MSFT) - 447 shares @ $223.71"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dy-Ik8it7GBt"
      },
      "source": [
        " ## [10 pts] Within Spark, filter the incoming date to create aaplPrice and msftPrice streams."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpxubX0A7GBt"
      },
      "source": [
        "The code is shown above, but the excerpt is pasted below as well. This chunk of code grabs the csv that is stored in hdfs from the earlier steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Q9J3ES07GBt"
      },
      "outputs": [],
      "source": [
        "# Read all CSV files from HDFS directory\n",
        "stock_df = spark.read.csv(\n",
        "    \"hdfs:///user/root/stock_data/\",\n",
        "    header=True,\n",
        "    inferSchema=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-A09JRmV7GBt"
      },
      "source": [
        "## [10 pts] From aaplPrice produce two other streams aapl10Day, aapl40Day. Both of these streams and their comparison to generate buy/sell signals are not shown in the diagram above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zy7hToSk7GBt"
      },
      "source": [
        "The code for this is shown above in the entire code chunk. The excerpt is pasted below as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUt7RLYY7GBt"
      },
      "outputs": [],
      "source": [
        "window_10 = Window.partitionBy(\"symbol\").orderBy(\"datetime\").rowsBetween(-9, 0)\n",
        "window_40 = Window.partitionBy(\"symbol\").orderBy(\"datetime\").rowsBetween(-39, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uNA5a9j7GBu"
      },
      "source": [
        "## [10 pts] From msftPrice produce two more streams msft10Day and msft40Day ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txztCZhz7GBu"
      },
      "source": [
        " The code is generalized as shown above, so the code for this is the same as for AAPL."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OU5KMNQY7GBu"
      },
      "source": [
        "## [20 pts]. Compare the two moving averages (10-day MA and the 40-day MA) to indicate buy and sell signals . Your output should be of the form [( <datetime> buy <symbol>), ( <datetime> sell <symbol>), etc]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgZTJunA7GBu"
      },
      "source": [
        "Example output from AAPL part of downloaded csv file:\n",
        "\n",
        "        datetime\tsymbol\tclose\tsignal\tshares\n",
        "        2021-01-04T12:00:00.000Z\tAAPL\t127.41\tsell\t785\n",
        "        2021-01-05T10:00:00.000Z\tAAPL\t131.4893\tbuy\t761\n",
        "        2021-01-06T10:15:00.000Z\tAAPL\t128.9501\tsell\t775\n",
        "        2021-01-07T11:30:00.000Z\tAAPL\t130.705\tbuy\t765\n",
        "        2021-01-11T10:00:00.000Z\tAAPL\t129.69\tsell\t771\n",
        "        2021-01-13T10:00:00.000Z\tAAPL\t129.82001\tbuy\t770\n",
        "        2021-01-14T11:30:00.000Z\tAAPL\t130.21001\tsell\t768\n",
        "        2021-01-19T14:45:00.000Z\tAAPL\t127.89\tbuy\t782\n",
        "        2021-01-26T12:15:00.000Z\tAAPL\t142.405\tsell\t702\n",
        "        2021-01-26T13:45:00.000Z\tAAPL\t142.965\tbuy\t699\n",
        "        2021-01-27T15:15:00.000Z\tAAPL\t141.64999\tsell\t706"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Xk46Iy87GBu"
      },
      "source": [
        "Example output from MSFT part of csv file:\n",
        "\n",
        "        2021-01-05T14:45:00.000Z\tMSFT\t217.64\tbuy\t459\n",
        "        2021-01-06T09:45:00.000Z\tMSFT\t213.2406\tsell\t469\n",
        "        2021-01-07T11:00:00.000Z\tMSFT\t217.64\tbuy\t459\n",
        "        2021-01-08T13:15:00.000Z\tMSFT\t217.75999\tsell\t459\n",
        "        2021-01-08T15:15:00.000Z\tMSFT\t219.55\tbuy\t455\n",
        "        2021-01-11T11:15:00.000Z\tMSFT\t218.08\tsell\t459\n",
        "        2021-01-13T11:45:00.000Z\tMSFT\t215.95\tbuy\t463\n",
        "        2021-01-14T11:45:00.000Z\tMSFT\t214.6358\tsell\t466\n",
        "        2021-01-19T10:00:00.000Z\tMSFT\t214.50999\tbuy\t466\n",
        "        2021-01-19T10:30:00.000Z\tMSFT\t213.49001\tsell\t468\n",
        "        2021-01-19T10:45:00.000Z\tMSFT\t213.87\tbuy\t468\n",
        "        2021-01-19T11:00:00.000Z\tMSFT\t212.7641\tsell\t470\n",
        "        2021-01-19T11:45:00.000Z\tMSFT\t213.6147\tbuy\t468\n",
        "        2021-01-22T15:45:00.000Z\tMSFT\t225.95\tsell\t443\n",
        "        2021-01-25T10:30:00.000Z\tMSFT\t228.48\tbuy\t438\n",
        "        2021-01-25T10:45:00.000Z\tMSFT\t226.18401\tsell\t442\n",
        "        2021-01-25T14:00:00.000Z\tMSFT\t228.23\tbuy\t438"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvz9IvGe7GBu"
      },
      "source": [
        "As you can see above, the csv file is formatted by datetime, stock symbol, close price, buy/sell signal, and then finally the number of shares, assuming you are buying or selling 100,000 in stocks."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}