{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exercise 7: Data Streams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwJWEXvN7GBp"
      },
      "source": [
        "## [2 points] How many API credits would be required for downloading 4 years worth of data at 15-minute intervals, per stock?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the required setup for this assignment, it took 107 API credits to download each stock. However, this was with the \n",
        "overlapping method required by this assignment. It would take fewer than 107 credits if this were excluded. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GX0Mm1Kp7GBp"
      },
      "source": [
        "## [3 points] The API applies a rate limit. How long would it take to download 4 years worth of data at 15-minute intervals, per stock?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With my setup, it took 14 minutes per stock to download the 4 years worth of data with the overlapping technique. My method for gathering the data involved creating chunks of data for each request to be as large as possible to speed up the process. I also made sure to exclude weekends and holidays from being included in these chunks to make the requests faster and more efficient as well. To avoid issues with the API limits, I have a 61 second sleep that automatically triggers if the minutely request limit is reached. I found this method to be faster than some other attempts where I set chunks of 14 or 30 days per request and not checking for holidays or weekends. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Lg6TTBS7GBq"
      },
      "source": [
        "## [20 points for setting up data collection] We’ll be working with just 2 stocks: AAPL and MSFT. The need for speed in a price API makes it inevitable that a small percentage of data elements has errors in it. To compensate for such errors, a “good enough” strategy is to request data for overlapping periods, say fetching 30 minutes of data every 15 minutes – replacing the most recently arrived data with a corrected version."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOfbo1v07GBq"
      },
      "source": [
        "Below is the setup for using the api to gather the stock data. The full script I ran to gather the data using the API is under the following question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dkreG7D7GBq",
        "outputId": "7c11fb11-0e62-48f4-b8a8-6299cb4548ce"
      },
      "outputs": [],
      "source": [
        "# !pip install twelvedata\n",
        "# import twelvedata\n",
        "\n",
        "from google.colab import drive\n",
        "import sys\n",
        "drive.mount(\"/content/drive/\", force_remount=True)\n",
        "sys.path.append(\"/content/drive/My Drive/Colab Notebooks\")\n",
        "from key import twelveDataKey as api_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPAVh9Ji7GBr"
      },
      "source": [
        "## [25 points for collecting data] Technically, final testing can only be done while the market is open: 9:30 am to 4:00 pm on weekdays. In a compromise to accommodate everyone’s schedules we will pull the stock data of each stock and gather up at least 4 years of stock price data (January 2021-December 2024) and use it as the basis of our work.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF9yWH2I7GBr"
      },
      "source": [
        "Below is the script I ran for getting the data and storing them in csv files. I ran the script once each per stock, to allow myself to answer the previous questions about time and API credits per stock more accurately. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AKFd5uF7GBs"
      },
      "outputs": [],
      "source": [
        "import time as t\n",
        "from twelvedata import TDClient\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta, time\n",
        "import os\n",
        "from google.colab import drive\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Get key from google drive - avoids embedding in code/uploading to github\n",
        "td = TDClient(apikey=api_key)\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# API constraints for use in function\n",
        "MAX_DAILY_REQUESTS = 800\n",
        "MAX_MINUTE_REQUESTS = 8\n",
        "DATA_LIMIT_PER_REQUEST = 5000\n",
        "\n",
        "\n",
        "def fetch_chunk(symbol, start_dt, end_dt, retries=3):\n",
        "    \"\"\"Fetch data chunk with retry logic, ensuring API constraints.\"\"\"\n",
        "    global api_usage\n",
        "\n",
        "    if api_usage >= MAX_DAILY_REQUESTS:\n",
        "        print(\"Daily API limit reached. Stopping execution.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            data = td.time_series(\n",
        "                symbol=symbol,\n",
        "                interval=\"15min\",\n",
        "                start_date=start_dt.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                end_date=end_dt.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                outputsize=DATA_LIMIT_PER_REQUEST\n",
        "            ).as_pandas()\n",
        "\n",
        "            api_usage += 1  # Increment usage counter\n",
        "            return data[data.index >= start_dt]  # Trim overlap\n",
        "        except Exception as e:\n",
        "            print(f\"Attempt {attempt+1} failed: {e}\")\n",
        "            t.sleep(min(8 * (attempt + 1), 60))  # Exponential backoff, capped at 60s\n",
        "\n",
        "    return pd.DataFrame()\n",
        "\n",
        "def fetch_stock_data(symbol):\n",
        "    \"\"\"Fetch full stock data while accounting for API rate limits.\"\"\"\n",
        "    global api_usage\n",
        "\n",
        "    market_open = time(9, 30)\n",
        "    market_close = time(16, 0)\n",
        "    chunk_days = 14\n",
        "    overlap = timedelta(minutes=30)\n",
        "\n",
        "    all_data = []\n",
        "    current_dt = datetime(2021, 1, 1)\n",
        "    end_dt = datetime(2024, 12, 31)\n",
        "\n",
        "    # Find first available trading day\n",
        "    while current_dt <= end_dt:\n",
        "        if current_dt.weekday() >= 5:  # Skip weekends\n",
        "            current_dt += timedelta(days=1)\n",
        "            continue\n",
        "\n",
        "        test_data = fetch_chunk(\n",
        "            symbol,\n",
        "            datetime.combine(current_dt, market_open),\n",
        "            datetime.combine(current_dt, market_close),\n",
        "            retries=1\n",
        "        )\n",
        "        if not test_data.empty:\n",
        "            break\n",
        "        current_dt += timedelta(days=1)\n",
        "\n",
        "    # Calculate total calendar days\n",
        "    total_days = (end_dt - current_dt).days + 1  # +1 to include both start and end dates\n",
        "    if total_days <= 0:\n",
        "        print(f\"No valid data available for {symbol}. Exiting.\")\n",
        "        return\n",
        "\n",
        "    pbar = tqdm(total=total_days, desc=f\"Fetching {symbol}\")\n",
        "    processed_days = 0\n",
        "\n",
        "    while current_dt <= end_dt and api_usage < MAX_DAILY_REQUESTS and processed_days < total_days:\n",
        "        # Skip weekends\n",
        "        if current_dt.weekday() >= 5:\n",
        "            current_dt += timedelta(days=1)\n",
        "            pbar.update(1)\n",
        "            processed_days += 1\n",
        "            continue\n",
        "\n",
        "        chunk_end = min(current_dt + timedelta(days=chunk_days - 1), end_dt)  # -1 because we include current day\n",
        "\n",
        "        chunk_data = fetch_chunk(\n",
        "            symbol,\n",
        "            datetime.combine(current_dt, market_open),\n",
        "            datetime.combine(chunk_end, market_close) + overlap,\n",
        "        )\n",
        "\n",
        "        days_in_chunk = (chunk_end - current_dt).days + 1  # +1 to include both start and end dates\n",
        "\n",
        "        if not chunk_data.empty:\n",
        "            all_data.append(chunk_data)\n",
        "            pbar.update(days_in_chunk)\n",
        "            processed_days += days_in_chunk\n",
        "            current_dt = chunk_end + timedelta(days=1)  # Move to next day after chunk_end\n",
        "        else:\n",
        "            # If no data, move forward by chunk days\n",
        "            pbar.update(days_in_chunk)\n",
        "            processed_days += days_in_chunk\n",
        "            current_dt = chunk_end + timedelta(days=1)\n",
        "\n",
        "        # API rate limiting. 61 second sleep if hit max minute requests - prevent from going over max\n",
        "        if api_usage % MAX_MINUTE_REQUESTS == 0:\n",
        "            t.sleep(61)\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "    if all_data: ##combine data into a csv file to download from cluster\n",
        "        final_data = pd.concat(all_data)\n",
        "        final_data = final_data[~final_data.index.duplicated(keep='last')]\n",
        "        final_data.sort_index(inplace=True)\n",
        "        save_path = f'/content/drive/My Drive/{symbol}_data.csv'\n",
        "        final_data.to_csv(save_path)\n",
        "        print(f\"Saved {len(final_data)} records for {symbol} at {save_path}\")\n",
        "\n",
        "    if current_dt > end_dt:\n",
        "        print(f\"Successfully processed all dates for {symbol} ✅\")\n",
        "    elif api_usage >= MAX_DAILY_REQUESTS:\n",
        "        print(f\"Stopped due to reaching API limit at {current_dt.strftime('%Y-%m-%d')} ⚠️\")\n",
        "    else:\n",
        "        print(f\"Unexpected exit condition at {current_dt.strftime('%Y-%m-%d')}\")\n",
        "\n",
        "# Fetch data for both stocks\n",
        "fetch_stock_data(\"MSFT\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AetF9ovu7GBs"
      },
      "source": [
        "## [25 points] Create a program new-stock-price-feeder.py that uses a more modern API (e.g., twelveData) instead."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The below code was run with the below command. This was the names of one of my clusters/regions/program name. It was changed when run with different clusters. \n",
        "        \n",
        "        gcloud dataproc jobs submit pyspark stocks2.py --cluster cluster-4290 --region us-east1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIWhuEEh7GBs"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, avg, lag, when, lit, input_file_name, round\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"StockTradingStrategy\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Read all CSV files from HDFS put in there before running the script. \n",
        "stock_df = spark.read.csv(\n",
        "    \"hdfs:///user/root/stock_data/\",\n",
        "    header=True,\n",
        "    inferSchema=True\n",
        ")\n",
        "\n",
        "# Add symbol column based on filename\n",
        "stock_df = stock_df.withColumn(\n",
        "    \"filename\", input_file_name()\n",
        ").withColumn(\n",
        "    \"symbol\",\n",
        "    when(\n",
        "        col(\"filename\").contains(\"AAPL\"), \"AAPL\"\n",
        "    ).when(\n",
        "        col(\"filename\").contains(\"MSFT\"), \"MSFT\"\n",
        "    ).otherwise(\"UNKNOWN\")\n",
        ").drop(\"filename\")\n",
        "\n",
        "# Verify we have both symbols\n",
        "print(\"Distinct symbols found:\")\n",
        "stock_df.select(\"symbol\").distinct().show()\n",
        "\n",
        "# Define window specs for moving averages\n",
        "window_10 = Window.partitionBy(\"symbol\").orderBy(\"datetime\").rowsBetween(-9, 0)\n",
        "window_40 = Window.partitionBy(\"symbol\").orderBy(\"datetime\").rowsBetween(-39, 0)\n",
        "\n",
        "# Calculate moving averages\n",
        "stock_with_ma = stock_df.withColumn(\"ma_10\", avg(\"close\").over(window_10)) \\\n",
        "                       .withColumn(\"ma_40\", avg(\"close\").over(window_40))\n",
        "\n",
        "# Function to generate trading signals with share calculations\n",
        "def generate_trades(df):\n",
        "    window_spec = Window.partitionBy(\"symbol\").orderBy(\"datetime\")\n",
        "    return df.withColumn(\"prev_ma_10\", lag(\"ma_10\", 1).over(window_spec)) \\\n",
        "            .withColumn(\"prev_ma_40\", lag(\"ma_40\", 1).over(window_spec)) \\\n",
        "            .withColumn(\"signal\",\n",
        "                when(\n",
        "                    (col(\"prev_ma_10\") <= col(\"prev_ma_40\")) &\n",
        "                    (col(\"ma_10\") > col(\"ma_40\")), \"buy\"\n",
        "                ).when(\n",
        "                    (col(\"prev_ma_10\") >= col(\"prev_ma_40\")) &\n",
        "                    (col(\"ma_10\") < col(\"ma_40\")), \"sell\"\n",
        "                ).otherwise(None)\n",
        "            ) \\\n",
        "            .withColumn(\"shares\", round(lit(100000)/col(\"close\"))) \\\n",
        "            .filter(col(\"signal\").isNotNull()) \\\n",
        "            .select(\n",
        "                \"datetime\",\n",
        "                \"symbol\",\n",
        "                \"close\",\n",
        "                \"signal\",\n",
        "                \"shares\"\n",
        "            )\n",
        "\n",
        "# Generate trades\n",
        "trades_df = generate_trades(stock_with_ma)\n",
        "\n",
        "# Format output following instructions. \n",
        "formatted_trades = trades_df.rdd.map(\n",
        "    lambda row: f\"({row['datetime']} {row['signal']} {row['symbol']}) - {row['shares']} shares @ ${row['close']:.2f}\"\n",
        ").collect()\n",
        "\n",
        "# Save results to HDFS\n",
        "trades_df.write.mode(\"overwrite\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .csv(\"hdfs:///user/root/stock_trades/\")\n",
        "\n",
        "# Print for debugging/testing (not necessary)\n",
        "print(\"\\nTrading Recommendations ($100K per trade):\")\n",
        "for trade in formatted_trades:\n",
        "    print(trade)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dy-Ik8it7GBt"
      },
      "source": [
        " ## [10 pts] Within Spark, filter the incoming date to create aaplPrice and msftPrice streams."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpxubX0A7GBt"
      },
      "source": [
        "The code is shown above, but the excerpt is pasted below as well. This chunk of code grabs the csv that is stored in hdfs from the earlier steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Q9J3ES07GBt"
      },
      "outputs": [],
      "source": [
        "# Read all CSV files from HDFS directory\n",
        "stock_df = spark.read.csv(\n",
        "    \"hdfs:///user/root/stock_data/\",\n",
        "    header=True,\n",
        "    inferSchema=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-A09JRmV7GBt"
      },
      "source": [
        "## [10 pts] From aaplPrice produce two other streams aapl10Day, aapl40Day. Both of these streams and their comparison to generate buy/sell signals are not shown in the diagram above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zy7hToSk7GBt"
      },
      "source": [
        "The code for this is shown above in the entire code chunk. The excerpt is pasted below as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUt7RLYY7GBt"
      },
      "outputs": [],
      "source": [
        "window_10 = Window.partitionBy(\"symbol\").orderBy(\"datetime\").rowsBetween(-9, 0)\n",
        "window_40 = Window.partitionBy(\"symbol\").orderBy(\"datetime\").rowsBetween(-39, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uNA5a9j7GBu"
      },
      "source": [
        "## [10 pts] From msftPrice produce two more streams msft10Day and msft40Day ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txztCZhz7GBu"
      },
      "source": [
        " The code is generalized as shown above, so the code for this is the same as for AAPL."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OU5KMNQY7GBu"
      },
      "source": [
        "## [20 pts]. Compare the two moving averages (10-day MA and the 40-day MA) to indicate buy and sell signals . Your output should be of the form [( <datetime> buy <symbol>), ( <datetime> sell <symbol>), etc]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgZTJunA7GBu"
      },
      "source": [
        "Example output from AAPL part of downloaded csv file:\n",
        "\n",
        "        datetime\t               symbol\tclose\t signal\t    shares\n",
        "        2021-01-04T12:00:00.000Z\tAAPL\t127.41\tsell\t785.0\n",
        "        2021-01-05T10:00:00.000Z\tAAPL\t131.4893\tbuy\t761.0\n",
        "        2021-01-06T10:15:00.000Z\tAAPL\t128.9501\tsell\t775.0\n",
        "        2021-01-07T11:30:00.000Z\tAAPL\t130.705\tbuy\t765.0\n",
        "        2021-01-11T10:00:00.000Z\tAAPL\t129.69\tsell\t771.0\n",
        "        2021-01-13T10:00:00.000Z\tAAPL\t129.82001\tbuy\t770.0\n",
        "        2021-01-14T11:30:00.000Z\tAAPL\t130.21001\tsell\t768.0\n",
        "        2021-01-19T14:45:00.000Z\tAAPL\t127.89\tbuy\t782.0\n",
        "        2021-01-26T12:15:00.000Z\tAAPL\t142.405\tsell\t702.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Xk46Iy87GBu"
      },
      "source": [
        "Example output from MSFT part of csv file:\n",
        "\n",
        "        datetime                        symbol  close   signal  shares\n",
        "\n",
        "        2021-01-04T12:00:00.000Z\tMSFT\t215.09\tsell\t465.0\n",
        "        2021-01-05T14:45:00.000Z\tMSFT\t217.64\tbuy\t459.0\n",
        "        2021-01-06T09:45:00.000Z\tMSFT\t213.2406\tsell\t469.0\n",
        "        2021-01-07T11:00:00.000Z\tMSFT\t217.64\tbuy\t459.0\n",
        "        2021-01-08T13:15:00.000Z\tMSFT\t217.75999\tsell\t459.0\n",
        "        2021-01-08T15:15:00.000Z\tMSFT\t219.55\tbuy\t455.0\n",
        "        2021-01-11T11:15:00.000Z\tMSFT\t218.08\tsell\t459.0\n",
        "        2021-01-13T11:45:00.000Z\tMSFT\t215.95\tbuy\t463.0\n",
        "        2021-01-14T11:45:00.000Z\tMSFT\t214.6358\tsell\t466.0\n",
        "        2021-01-19T10:00:00.000Z\tMSFT\t214.50999\tbuy\t466.0\n",
        "        2021-01-19T10:30:00.000Z\tMSFT\t213.49001\tsell\t468.0\n",
        "        2021-01-19T10:45:00.000Z\tMSFT\t213.87\tbuy\t468.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvz9IvGe7GBu"
      },
      "source": [
        "As you can see above, the csv file is formatted by datetime, stock symbol, close price, buy/sell signal, and then finally the number of shares, assuming you are buying or selling $100,000 in stocks."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
